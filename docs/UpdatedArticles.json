[
  {
    "Number": 109828,
    "Title": "Clarified supported limits for log search alerts.",
    "ClosedAt": "2020-04-01T12:22:59Z",
    "User": "yanivlavi",
    "FileName": "includes/azure-monitor-limits-alerts.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -15,6 +15,6 @@ ms.custom: \"include file\"\n | Metric alerts (classic) |100 active alert rules per subscription. | Call support. |\n | Metric alerts |2,000 active alert rules per subscription in Azure public, Azure China 21Vianet and Azure Government clouds. | Call support. |\n | Activity log alerts | 100 active alert rules per subscription. | Same as default. |\n-| Log alerts | 512 | Call support. |\n+| Log alerts | 512 active alert rules per subscription. 200 active alert rules per resource. | Call support. |\n | Action groups |2,000 action groups per subscription. | Call support. |\n | Autoscale settings |100 per region per subscription. | Same as default. |"
  },
  {
    "Number": 108704,
    "Title": "Automated Metrics",
    "ClosedAt": "2020-04-01T02:23:56Z",
    "User": "rboucher",
    "FileName": "articles/azure-monitor/platform/metrics-supported.md",
    "Addition": 2507,
    "Delections": 2327,
    "Changes": 4834,
    "Patch": null
  },
  {
    "Number": 109790,
    "Title": "Update agent-linux-troubleshoot.md",
    "ClosedAt": "2020-04-01T00:21:11Z",
    "User": "craigcaseyMSFT",
    "FileName": "articles/azure-monitor/platform/agent-linux-troubleshoot.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -72,7 +72,7 @@ If none of these steps work for you, the following support channels are also ava\n | --- | --- |\n | 2 | Invalid option provided to the omsadmin script. Run `sudo sh /opt/microsoft/omsagent/bin/omsadmin.sh -h` for usage. |\n | 3 | Invalid configuration provided to the omsadmin script. Run `sudo sh /opt/microsoft/omsagent/bin/omsadmin.sh -h` for usage. |\n-| 4 | Invalid proxy provided to the omsadmin script. Verify the proxy and see our [documentation for using an HTTP proxy](log-analytics-agent.md#network-firewall-requirements). |\n+| 4 | Invalid proxy provided to the omsadmin script. Verify the proxy and see our [documentation for using an HTTP proxy](log-analytics-agent.md#firewall-requirements). |\n | 5 | 403 HTTP error received from Azure Monitor. See the full output of the omsadmin script for details. |\n | 6 | Non-200 HTTP error received from Azure Monitor. See the full output of the omsadmin script for details. |\n | 7 | Unable to connect to Azure Monitor. See the full output of the omsadmin script for details. |"
  },
  {
    "Number": 109648,
    "Title": "Azure Monitor ARM cost warning",
    "ClosedAt": "2020-03-31T20:55:08Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/platform/template-workspace-configuration.md",
    "Addition": 3,
    "Delections": 0,
    "Changes": 3,
    "Patch": "@@ -44,6 +44,9 @@ The following table lists the API version for the resources used in this example\n \n The following example creates a workspace using a template from your local machine. The JSON template is configured to only require the name and location of the new workspace. It uses values specified for other workspace parameters such as [access control mode](design-logs-deployment.md#access-control-mode), pricing tier, retention, and capacity reservation level.\n \n+> [!WARNING]\n+> The following template creates a Log Analytics workspace and configures data collection. This may change your billing settings. Review [Manage usage and costs with Azure Monitor Logs](manage-cost-storage.md) to understand billing for data collected in a Log Analytics workspace before applying it in your Azure environment.\n+\n For capacity reservation, you define a selected capacity reservation for ingesting data by specifying the SKU `CapacityReservation` and a value in GB for the property `capacityReservationLevel`. The following list details the supported values and behavior when configuring it.\n \n - Once you set the reservation limit, you cannot change to a different SKU within 31 days."
  },
  {
    "Number": 109657,
    "Title": "add info on linux per GitHub issue",
    "ClosedAt": "2020-03-31T14:52:06Z",
    "User": "mrbullwinkle",
    "FileName": "articles/azure-monitor/app/azure-web-apps.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -24,7 +24,7 @@ There are two ways to enable application monitoring for Azure App Services hoste\n \n     * This approach is much more customizable, but it requires [adding a dependency on the Application Insights SDK NuGet packages](https://docs.microsoft.com/azure/azure-monitor/app/asp-net). This method, also means you have to manage the updates to the latest version of the packages yourself.\n \n-    * If you need to make custom API calls to track events/dependencies not captured by default with agent-based monitoring, you would need to use this method. Check out the [API for custom events and metrics article](https://docs.microsoft.com/azure/azure-monitor/app/api-custom-events-metrics) to learn more.\n+    * If you need to make custom API calls to track events/dependencies not captured by default with agent-based monitoring, you would need to use this method. Check out the [API for custom events and metrics article](https://docs.microsoft.com/azure/azure-monitor/app/api-custom-events-metrics) to learn more. This is also currently the only supported option for Linux based workloads.\n \n > [!NOTE]\n > If both agent-based monitoring and manual SDK-based instrumentation is detected, only the manual instrumentation settings will be honored. This is to prevent duplicate data from being sent. To learn more about this, check out the [troubleshooting section](https://docs.microsoft.com/azure/azure-monitor/app/azure-web-apps#troubleshooting) below."
  },
  {
    "Number": 109557,
    "Title": "Azure Monitor Office 365 deprecation date",
    "ClosedAt": "2020-03-31T13:01:56Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/solution-office-365.md",
    "Addition": 19,
    "Delections": 19,
    "Changes": 38,
    "Patch": "@@ -5,7 +5,7 @@ ms.subservice:\n ms.topic: conceptual\n author: bwren\n ms.author: bwren\n-ms.date: 01/08/2019\n+ms.date: 03/30/2020\n \n ---\n \n@@ -16,7 +16,7 @@ ms.date: 01/08/2019\n \n > [!IMPORTANT]\n > ## Solution update\n-> This solution has been replaced by the [Office 365](../../sentinel/connect-office-365.md) General Availability solution in [Azure Sentinel](../../sentinel/overview.md) and the [Azure AD reporting and monitoring solution](../../active-directory/reports-monitoring/plan-monitoring-and-reporting.md). Together they provide an updated version of the previous Azure Monitor Office 365 solution with an improved configuration experience. You can continue to use the existing solution until April 30, 2020.\n+> This solution has been replaced by the [Office 365](../../sentinel/connect-office-365.md) General Availability solution in [Azure Sentinel](../../sentinel/overview.md) and the [Azure AD reporting and monitoring solution](../../active-directory/reports-monitoring/plan-monitoring-and-reporting.md). Together they provide an updated version of the previous Azure Monitor Office 365 solution with an improved configuration experience. You can continue to use the existing solution until July 30, 2020.\n > \n > Azure Sentinel is a cloud native Security Information and Event Management solution that ingests logs and provides additional SIEM functionality including detections, investigations, hunting and machine learning driven insights. Using Azure Sentinel will now provide you with ingestion of Office 365 SharePoint activity and Exchange management logs.\n > \n@@ -49,7 +49,7 @@ ms.date: 01/08/2019\n > | where TimeGenerated >= ago(1d) \n > | where OfficeWorkload == \"AzureActiveDirectory\"                      \n > | where Operation == 'UserLoginFailed'\n-> | summarize count() by UserId\t\n+> | summarize count() by UserId    \n > ```\n > \n > ```Kusto\n@@ -78,10 +78,10 @@ ms.date: 01/08/2019\n > ### Q: Do I need Azure Sentinel to connect the Azure AD logs?\n > You can configure [Azure AD logs integration with Azure Monitor](../../active-directory/reports-monitoring/howto-integrate-activity-logs-with-log-analytics.md), which is not related to the Azure Sentinel solution. Azure Sentinel provides a native connector and out-of-the box content for Azure AD logs. For more information, see the question below on out-of-the-box security-oriented content.\n >\n-> ###\tQ: What are the differences when connecting Azure AD logs from Azure Sentinel and Azure Monitor?\n+> ###    Q: What are the differences when connecting Azure AD logs from Azure Sentinel and Azure Monitor?\n > Azure Sentinel and Azure Monitor connect to Azure AD logs based on the same [Azure AD reporting and monitoring solution](../../active-directory/reports-monitoring/plan-monitoring-and-reporting.md). Azure Sentinel provides a one-click, native connector that connects the same data and provides monitoring information.\n >\n-> ###\tQ: What do I need to change when moving to the new Azure AD reporting and monitoring tables?\n+> ###    Q: What do I need to change when moving to the new Azure AD reporting and monitoring tables?\n > All queries using Azure AD data, including queries in alerts, dashboards, and any content that you created using Office 365 Azure AD data, must be recreated using the new tables.\n >\n > Azure Sentinel and Azure AD provide built-in content that you can use when moving to the Azure AD reporting and monitoring solution. For more information, see the next question on out-of-the-box security-oriented content and [How to use Azure Monitor workbooks for Azure Active Directory reports](../../active-directory/reports-monitoring/howto-use-azure-monitor-workbooks.md). \n@@ -99,11 +99,11 @@ ms.date: 01/08/2019\n > ### Q: Does Azure Sentinel provide additional connectors as part of the solution?\n > Yes, see [Azure Sentinel connect data sources](../../sentinel/connect-data-sources.md).\n > \n-> ###\tQ: What will happen on April 30? Do I need to offboard beforehand?\n+> ###    Q: What will happen on April 30? Do I need to offboard beforehand?\n > \n-> - You won’t be able to receive data from the **Office365** solution. The solution will no longer be available in the Marketplace\n+> - You won't be able to receive data from the **Office365** solution. The solution will no longer be available in the Marketplace\n > - For Azure Sentinel customers, the Log Analytics workspace solution **Office365** will be included in the Azure Sentinel **SecurityInsights** solution.\n-> - If you don’t offboard your solution manually, your data will be disconnected automatically on April 30.\n+> - If you don't offboard your solution manually, your data will be disconnected automatically on April 30.\n > \n > ### Q: Will my data transfer to the new solution?\n > Yes. When you remove the **Office 365** solution from your workspace, its data will become temporarily unavailable because the schema is removed. When you enable the new **Office 365** connector in Sentinel, the schema is restored to the workspace and any data already collected will become available. \n@@ -244,7 +244,7 @@ The dashboard includes the columns in the following table. Each column lists the\n |:--|:--|\n | Operations | Provides information about the active users from your all monitored Office 365 subscriptions. You will also be able to see the number of activities that happen over time.\n | Exchange | Shows the breakdown of Exchange Server activities such as Add-Mailbox Permission, or Set-Mailbox. |\n-| SharePoint | Shows the top activities that users perform on SharePoint documents. When you drill down from this tile, the search page shows the details of these activities, such as the target document and the location of this activity. For example, for a File Accessed event, you will be able to see the document that’s being accessed, its associated account name, and IP address. |\n+| SharePoint | Shows the top activities that users perform on SharePoint documents. When you drill down from this tile, the search page shows the details of these activities, such as the target document and the location of this activity. For example, for a File Accessed event, you will be able to see the document that's being accessed, its associated account name, and IP address. |\n | Azure Active Directory | Includes top user activities, such as Reset User Password and Login Attempts. When you drill down, you will be able to see the details of these activities like the Result Status. This is mostly helpful if you want to monitor suspicious activities on your Azure Active Directory. |\n \n \n@@ -311,7 +311,7 @@ These records are created when change or additions are made to Azure Active Dire\n | ActorContextId | The GUID of the organization that the actor belongs to. |\n | ActorIpAddress | The actor's IP address in IPV4 or IPV6 address format. |\n | InterSystemsId | The GUID that track the actions across components within the Office 365 service. |\n-| IntraSystemId | \tThe GUID that's generated by Azure Active Directory to track the action. |\n+| IntraSystemId |     The GUID that's generated by Azure Active Directory to track the action. |\n | SupportTicketId | The customer support ticket ID for the action in \"act-on-behalf-of\" situations. |\n | TargetContextId | The GUID of the organization that the targeted user belongs to. |\n \n@@ -326,7 +326,7 @@ These records are created from Data Center Security audit data.\n | ElevationApprovedTime | The timestamp for when the elevation was approved. |\n | ElevationApprover | The name of a Microsoft manager. |\n | ElevationDuration | The duration for which the elevation was active. |\n-| ElevationRequestId | \tA unique identifier for the elevation request. |\n+| ElevationRequestId |     A unique identifier for the elevation request. |\n | ElevationRole | The role the elevation was requested for. |\n | ElevationTime | The start time of the elevation. |\n | Start_Time | The start time of the cmdlet execution. |\n@@ -340,8 +340,8 @@ These records are created when changes are made to Exchange configuration.\n |:--- |:--- |\n | OfficeWorkload | Exchange |\n | RecordType     | ExchangeAdmin |\n-| ExternalAccess | \tSpecifies whether the cmdlet was run by a user in your organization, by Microsoft datacenter personnel or a datacenter service account, or by a delegated administrator. The value False indicates that the cmdlet was run by someone in your organization. The value True indicates that the cmdlet was run by datacenter personnel, a datacenter service account, or a delegated administrator. |\n-| ModifiedObjectResolvedName | \tThis is the user friendly name of the object that was modified by the cmdlet. This is logged only if the cmdlet modifies the object. |\n+| ExternalAccess |     Specifies whether the cmdlet was run by a user in your organization, by Microsoft datacenter personnel or a datacenter service account, or by a delegated administrator. The value False indicates that the cmdlet was run by someone in your organization. The value True indicates that the cmdlet was run by datacenter personnel, a datacenter service account, or a delegated administrator. |\n+| ModifiedObjectResolvedName |     This is the user friendly name of the object that was modified by the cmdlet. This is logged only if the cmdlet modifies the object. |\n | OrganizationName | The name of the tenant. |\n | OriginatingServer | The name of the server from which the cmdlet was executed. |\n | Parameters | The name and value for all parameters that were used with the cmdlet that is identified in the Operations property. |\n@@ -362,7 +362,7 @@ These records are created when changes or additions are made to Exchange mailbox\n | ClientVersion | The version of the email client . |\n | InternalLogonType | Reserved for internal use. |\n | Logon_Type | Indicates the type of user who accessed the mailbox and performed the operation that was logged. |\n-| LogonUserDisplayName | \tThe user-friendly name of the user who performed the operation. |\n+| LogonUserDisplayName |     The user-friendly name of the user who performed the operation. |\n | LogonUserSid | The SID of the user who performed the operation. |\n | MailboxGuid | The Exchange GUID of the mailbox that was accessed. |\n | MailboxOwnerMasterAccountSid | Mailbox owner account's master account SID. |\n@@ -401,7 +401,7 @@ These records are created when changes or additions are made to Exchange groups.\n | DestMailboxOwnerUPN | Set only if the CrossMailboxOperations parameter is True. Specifies the UPN of the owner of the target mailbox. |\n | DestFolder | The destination folder, for operations such as Move. |\n | Folder | The folder where a group of items is located. |\n-| Folders | \tInformation about the source folders involved in an operation; for example, if folders are selected and then deleted. |\n+| Folders |     Information about the source folders involved in an operation; for example, if folders are selected and then deleted. |\n \n \n ### SharePoint Base\n@@ -415,7 +415,7 @@ These properties are common to all SharePoint records.\n | EventSource | Identifies that an event occurred in SharePoint. Possible values are SharePoint or ObjectModel. |\n | ItemType | The type of object that was accessed or modified. See the ItemType table for details on the types of objects. |\n | MachineDomainInfo | Information about device sync operations. This information is reported only if it's present in the request. |\n-| MachineId | \tInformation about device sync operations. This information is reported only if it's present in the request. |\n+| MachineId |     Information about device sync operations. This information is reported only if it's present in the request. |\n | Site_ | The GUID of the site where the file or folder accessed by the user is located. |\n | Source_Name | The entity that triggered the audited operation. Possible values are SharePoint or ObjectModel. |\n | UserAgent | Information about the user's client or browser. This information is provided by the client or browser. |\n@@ -430,7 +430,7 @@ These records are created when configuration changes are made to SharePoint.\n | OfficeWorkload | SharePoint |\n | OfficeWorkload | SharePoint |\n | CustomEvent | Optional string for custom events. |\n-| Event_Data | \tOptional payload for custom events. |\n+| Event_Data |     Optional payload for custom events. |\n | ModifiedProperties | The property is included for admin events, such as adding a user as a member of a site or a site collection admin group. The property includes the name of the property that was modified (for example, the Site Admin group), the new value of the modified property (such the user who was added as a site admin), and the previous value of the modified object. |\n \n \n@@ -448,9 +448,9 @@ These records are created in response to file operations in SharePoint.\n | SharingType | The type of sharing permissions that were assigned to the user that the resource was shared with. This user is identified by the UserSharedWith parameter. |\n | Site_Url | The URL of the site where the file or folder accessed by the user is located. |\n | SourceFileExtension | The file extension of the file that was accessed by the user. This property is blank if the object that was accessed is a folder. |\n-| SourceFileName | \tThe name of the file or folder accessed by the user. |\n+| SourceFileName |     The name of the file or folder accessed by the user. |\n | SourceRelativeUrl | The URL of the folder that contains the file accessed by the user. The combination of the values for the SiteURL, SourceRelativeURL, and SourceFileName parameters is the same as the value for the ObjectID property, which is the full path name for the file accessed by the user. |\n-| UserSharedWith | \tThe user that a resource was shared with. |\n+| UserSharedWith |     The user that a resource was shared with. |\n \n \n "
  },
  {
    "Number": 109576,
    "Title": "Azure Monitor solution automation account",
    "ClosedAt": "2020-03-31T13:00:09Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/solutions.md",
    "Addition": 2,
    "Delections": 6,
    "Changes": 8,
    "Patch": "@@ -5,7 +5,7 @@ ms.subservice: logs\n ms.topic: conceptual\n author: bwren\n ms.author: bwren\n-ms.date: 08/13/2019\n+ms.date: 03/30/2020\n \n ---\n \n@@ -79,11 +79,7 @@ All monitoring solutions require a [Log Analytics workspace](../platform/manage-\n * If a solution requires an Automation account, then the Log Analytics workspace and Automation account must be linked to one another. A Log Analytics workspace may only be linked to one Automation account, and an Automation account may only be linked to one Log Analytics workspace.\n * To be linked, the Log Analytics workspace and Automation account must be in the same resource group and region. The exception is a workspace in East US region and Automation account in East US 2.\n \n-### Create a link between a Log Analytics workspace and Automation account\n-How you specify the Log Analytics workspace and Automation account depends on the installation method for your solution.\n-\n-* When you install a solution through the Azure Marketplace, you're prompted for a workspace and Automation account. The link between them is created if they aren't already linked.\n-* For solutions outside of the Azure Marketplace, you must link the Log Analytics workspace and Automation account before installing the solution. You can do this by selecting any solution in the Azure Marketplace and selecting the Log Analytics workspace and Automation account. You don't have to actually install the solution because the link is created as soon as the Log Analytics workspace and Automation account are selected. Once the link is created, then you can use that Log Analytics workspace and Automation account for any solution.\n+When you install a solution through the Azure Marketplace, you're prompted for a workspace and Automation account. The link between them is created if they aren't already linked.\n \n ### Verify the link between a Log Analytics workspace and Automation account\n You can verify the link between a Log Analytics workspace and an Automation account using the following procedure."
  },
  {
    "Number": 109635,
    "Title": "Editing",
    "ClosedAt": "2020-03-31T09:50:35Z",
    "User": "yossi-y",
    "FileName": "articles/azure-monitor/platform/customer-managed-keys.md",
    "Addition": 23,
    "Delections": 27,
    "Changes": 50,
    "Patch": "@@ -40,7 +40,7 @@ options to closely manage encryption or encryption keys.\n The Azure Monitor data-store ensures that all data encrypted at\n rest using Azure-managed keys while stored in Azure Storage. Azure Monitor also\n provides an option for data encryption using your own key that is stored\n-in [Azure Key Vault](https://docs.microsoft.com/azure/key-vault/key-vault-overview),\n+in your [Azure Key Vault](https://docs.microsoft.com/azure/key-vault/key-vault-overview),\n which is accessed using system-assigned [managed identity](https://docs.microsoft.com/azure/active-directory/managed-identities-azure-resources/overview) authentication. This key can be either [software or hardware-HSM\n protected](https://docs.microsoft.com/azure/key-vault/key-vault-overview).\n The Azure Monitor use of encryption is identical to the way \n@@ -50,7 +50,7 @@ operates.\n The frequency that Azure Monitor Storage accesses Key Vault for wrap and\n unwrap operations is between 6 to 60 seconds. Azure Monitor Storage always respects changes in key permissions within an hour.\n \n-Ingested data in last 14 days is also kept in hot-cache (SSD-backed) for efficient query engine operation. This data remains encrypted with Microsoft keys regardless CMK configuration, but we are working to have the SSD encrypted with CMK early 2020.\n+Ingested data in last 14 days is also kept in hot-cache (SSD-backed) for efficient query engine operation. This data remains encrypted with Microsoft keys regardless CMK configuration, but we are working to have the SSD encrypted with CMK in the first half of 2020.\n \n ## How CMK works in Azure Monitor\n \n@@ -73,19 +73,19 @@ authenticate and access your Azure Key Vault via Azure Active Directory.\n 1.\tCustomer’s Key Vault.\n 2.\tCustomer’s Log Analytics *Cluster* resource having managed identity with permissions to Key Vault – The identity is supported at the data-store (ADX cluster) level.\n 3.\tAzure Monitor dedicated ADX cluster.\n-4.\tCustomer’s workspaces associated to Cluster resource for CMK encryption.\n+4.\tCustomer’s workspaces associated to *Cluster* resource for CMK encryption.\n \n ## Encryption keys management\n \n There are 3 types of keys involved in Storage data encryption:\n \n-- **KEK** - Key Encryption Key in Key Vault (CMK)\n+- **KEK** - Key Encryption Key (CMK)\n - **AEK** - Account Encryption Key\n - **DEK** - Data Encryption Key\n \n The following rules apply:\n \n-- The ADX storage account generates a unique encryption key for every storage account, which is known as the AEK.\n+- The ADX storage accounts generate unique encryption key for every Storage account, which is known as the AEK.\n \n - The AEK is used to derive DEKs, which are the keys that are used to\n     encrypt each block of data written to disk.\n@@ -129,12 +129,10 @@ Where *eyJ0eXAiO....* represents the full Authorization token.\n You can acquire the token using one of these methods:\n \n 1. Use [App registrations](https://docs.microsoft.com/graph/auth/auth-concepts#access-tokens) method.\n-\n 2. In the Azure portal\n     1. Navigate to Azure portal in \"developer tool (F12)\n     1. Look for authorization string under \"Request Headers\" in one of the \"batch?api-version\" instances. It looks like: \"authorization: Bearer \\<token\\>\". \n     1. Copy and add it to your API call per the examples below.\n-\n 3. Navigate to Azure REST documentation site. Press \"Try it\" on any API and copy the Bearer token.\n \n ### Subscription whitelisting\n@@ -146,19 +144,19 @@ CMK capability is an early access feature. The subscriptions where you plan to c\n \n ### Storing encryption key (KEK)\n \n-Create or use an Azure Key Vault that you already have to generate, or import a key to be used for data encryption. The Azure Key Vault must be configured as recoverable to protect your key and the access to your data in Azure Monitor. You can verify this configuration under properties in your in your Key Vault, both *Soft delete* and *Purge protection* should be enabled.\n+Create or use an Azure Key Vault that you already have to generate, or import a key to be used for data encryption. The Azure Key Vault must be configured as recoverable to protect your key and the access to your data in Azure Monitor. You can verify this configuration under properties in your Key Vault, both *Soft delete* and *Purge protection* should be enabled.\n \n These settings are available via CLI and PowerShell:\n - [Soft Delete](https://docs.microsoft.com/azure/key-vault/key-vault-ovw-soft-delete)\n - [Purge protection](https://docs.microsoft.com/azure/key-vault/key-vault-ovw-soft-delete#purge-protection) guards against force deletion of the secret / vault even after soft delete\n \n ### Create *Cluster* resource\n \n-This resource is used as an intermediate identity connection between your Key Vault and your workspaces. After you receive confirmation that your subscriptions were whitelisted, create a Log Analytics *Cluster* resource at the region where your workspaces are located. Application Insights and Log Analytics require separate *Cluster* resources types. The type of the *Cluster* resource is defined at creation time by setting the \"clusterType\" property to either \"LogAnalytics\", or \"ApplicationInsights\". The Cluster resource type can’t be altered after.\n+This resource is used as an intermediate identity connection between your Key Vault and your Log Analytics workspaces. After you receive confirmation that your subscriptions were whitelisted, create a Log Analytics *Cluster* resource at the region where your workspaces are located. Application Insights and Log Analytics require separate *Cluster* resources types. The type of the *Cluster* resource is defined at creation time by setting the *clusterType* property to either *LogAnalytics*, or *ApplicationInsights*. The Cluster resource type can’t be altered after.\n \n For Application Insights CMK configuration, follow the Appendix content.\n \n-You must specify the capacity reservation level (sku) for the *Cluster* resource when creating a *Cluster* resource. The capacity reservation level can be in the range of 1000 to 2000 and you can update it in steps of 100 later. If you need capacity reservation level higher than 2000, reach your Microsoft contact to enable it. This property doesn’t affect billing currently -- once pricing model for dedicated cluster is introduced, billing will apply to any existing CMK deployments.\n+You must specify the capacity reservation level (sku) when creating a *Cluster* resource. The capacity reservation level can be in the range of 1,000 to 2,000 GB per day and you can update it in steps of 100 later. If you need capacity reservation level higher than 2,000 GB per day, reach your Microsoft contact to enable it. This property doesn’t affect billing currently -- once pricing model for dedicated cluster is introduced, billing will apply to any existing CMK deployments.\n \n **Create**\n \n@@ -188,7 +186,7 @@ The identity is assigned to the *Cluster* resource at creation time.\n 202 Accepted. This is a standard Resource Manager response for asynchronous operations.\n \n >[!Important]\n-> It takes the provisioning of the underly ADX cluster a few minutes to complete. You can verify the provisioning state when performing GET REST API call on the *Cluster* resource and looking at the *provisioningState* value. It is *ProvisioningAccount* while provisioning and \"Succeeded\" when completed.\n+> It takes the provisioning of the underly ADX cluster a while to complete. You can verify the provisioning state when performing GET REST API call on the *Cluster* resource and looking at the *provisioningState* value. It is *ProvisioningAccount* while provisioning and *Succeeded* when completed.\n \n ### Azure Monitor data-store (ADX cluster) provisioning\n \n@@ -200,7 +198,7 @@ Authorization: Bearer <token>\n ```\n \n > [!IMPORTANT]\n-> Copy and save the response since you will need these details in later steps\n+> Copy and save the response since you will need its details in later steps\n \n **Response**\n ```json\n@@ -242,7 +240,7 @@ The *Get* permission is required to verify that your Key Vault is configured as\n \n ### Update Cluster resource with Key identifier details\n \n-This step applies per initial and future key version updates in your Key Vault. It informs Azure Monitor Storage about the new key version.\n+This step applies per initial and future key version updates in your Key Vault. It informs Azure Monitor Storage about the key version to be used for data encryption. When updated, your new key is being used to wrap and unwrap to Storage key (AEK).\n \n To update the *Cluster* resource with your Key Vault *Key identifier* details, select the current version of your key in Azure Key Vault to get the Key identifier details.\n \n@@ -351,10 +349,10 @@ Content-type: application/json\n }\n ```\n \n-After the workspaces association, data ingested to your workspaces is stored encrypted with your managed key.\n+The workspace association is performed via Resource Manager asynchronous operations, which can take up to 90 minutes to complete. The next step shows you how workspace association state can be checked. After the workspaces association, data ingested to your workspaces is stored encrypted with your managed key.\n \n ### Workspace association verification\n-You can verify if a workspace is associated to a *Custer* resource by looking at the [Workspaces – Get](https://docs.microsoft.com/rest/api/loganalytics/workspaces/get) response. Associated workspace will have a 'clusterResourceId' property with the *Cluster* resource ID.\n+You can verify if a workspace is associated to a *Cluster* resource by looking at the [Workspaces – Get](https://docs.microsoft.com/rest/api/loganalytics/workspaces/get) response. Associated workspaces will have a 'clusterResourceId' property with a *Cluster* resource ID.\n \n ```rest\n GET https://management.azure.com/subscriptions/<subscription-id>/resourcegroups/<resource-group-name>/providers/microsoft.operationalInsights/workspaces/<workspace-name>?api-version=2015-11-01-preview\n@@ -394,7 +392,7 @@ GET https://management.azure.com/subscriptions/<subscription-id>/resourcegroups/\n \n ## CMK (KEK) revocation\n \n-You can revoke your access to your data by disabling your key or deleting the *Cluster* resource access policy in your Key Vault. Azure Monitor Storage will always respect changes in key permissions within an hour, normally sooner, and Storage will become unavailable. Any data ingested to workspaces associated with your *Cluster* resource is dropped and queries will fail. Previously ingested data remains inaccessible in Azure Monitor Storage as long as your key is revoked, and your workspaces aren't deleted. Inaccessible data is governed by the data-retention policy and will be purged when retention is reached.\n+You can revoke your access to your data by disabling your key or deleting the *Cluster* resource access policy in your Key Vault. Azure Monitor Storage will always respect changes in key permissions within an hour, normally sooner, and Storage will become unavailable. Any data ingested to workspaces associated with your *Cluster* resource is dropped and queries will fail. Previously ingested data remains inaccessible in Azure Monitor Storage as long as your your *Cluster* resource and your workspaces aren't deleted. Inaccessible data is governed by the data-retention policy and will be purged when retention is reached.\n \n Storage will periodically poll your Key Vault to attempt to unwrap the\n encryption key and once accessed, data ingestion and query resume within\n@@ -403,24 +401,21 @@ encryption key and once accessed, data ingestion and query resume within\n ## CMK (KEK) rotation\n \n Rotation of CMK requires explicit update of the *Cluster* resource with the new key version in Azure Key Vault. To update Azure Monitor with your new key version, follow the instructions in \"Update *Cluster* resource with Key identifier details\" step. If you update your key version in Key Vault and don't update the new Key identifier details in the *Cluster* resource, Azure Monitor Storage will keep using your previous key.\n-All your data is accessible after the key rotation operation including data ingested before the rotation and after it, since all data remains encrypted by the Account Encryption Key (AEK) while it’s now being encrypted by your new Key Encryption Key (KEK) version.\n+All your data is accessible after the key rotation operation including data ingested before the rotation and after it, since all data remains encrypted by the Account Encryption Key (AEK) while AEK is now being encrypted by your new Key Encryption Key (KEK) version.\n \n ## Limitations and constraints\n \n - The CMK feature is supported at ADX cluster level and requires a\n-    dedicated Azure Monitor ADX cluster\n+    dedicated Azure Monitor ADX cluster with requirement of sending 1TB per day or more.\n \n - The max number of *Cluster* resources per subscription is limited to 2\n \n-- *Cluster* resource association to workspace should be carried ONLY\n-    after you received a confirmation from the product group that the\n-    ADX cluster provisioning was fulfilled. Data that is sent prior to\n-    this provisioning will be dropped and won't be recoverable.\n+- *Cluster* resource association to workspace should be carried ONLY after you have verified that the ADX cluster provisioning was fulfilled. Data that is sent prior to this provisioning will be dropped and won't be recoverable.\n \n - CMK encryption applies to newly ingested data after the CMK\n     configuration. Data that was ingested prior to the CMK\n     configuration, remains encrypted with Microsoft key. You can query\n-    data before and after the CMK configuration seamlessly.\n+    data ingested before and after the CMK configuration seamlessly.\n \n - Once workspace is associated to a *Cluster* resource, it cannot be\n     de-associated from the *Cluster* resource, since data is encrypted\n@@ -447,11 +442,11 @@ All your data is accessible after the key rotation operation including data inge\n ## Troubleshooting and management\n \n - Key Vault availability\n-    - In normal operation, Storage caches AEK for short periods of time periodically goes back to Key Vault to unwrap.\n+    - In normal operation -- Storage caches AEK for short periods of time and goes back to Key Vault to unwrap periodically.\n     \n-    - Transient connection errors. Storage handles transient errors (timeouts, connection failures, DNS issues) by allowing keys to stay in cache for a short while longer and this overcomes any small blips in availability. The query and ingestion capabilities continue without interruption.\n+    - Transient connection errors -- Storage handles transient errors (timeouts, connection failures, DNS issues) by allowing keys to stay in cache for a short while longer and this overcomes any small blips in availability. The query and ingestion capabilities continue without interruption.\n     \n-    - Live site, unavailability of about 30 minutes will cause the Storage account to become unavailable. The query capability is unavailable and ingested data is cached for several hours using Microsoft key to avoid data loss. When access to Key Vault is restored, query becomes available and the temporary cached data is ingested to the data-store and encrypted with CMK.\n+    - Live site -- unavailability of about 30 minutes will cause the Storage account to become unavailable. The query capability is unavailable and ingested data is cached for several hours using Microsoft key to avoid data loss. When access to Key Vault is restored, query becomes available and the temporary cached data is ingested to the data-store and encrypted with CMK.\n \n - If you create a *Cluster* resource and specify the KeyVaultProperties immediately, the operation may fail since the\n     access policy can't be defined until system identity is assigned to the *Cluster* resource.\n@@ -508,7 +503,8 @@ All your data is accessible after the key rotation operation including data inge\n     \n   The same response as for '*Cluster* resources for a resource group', but in subscription scope.\n     \n-- Delete a *Cluster* resource -- a soft-delete operation is performed to allow the recovery of your *Cluster* resource, your data and associated workspaces within 14 days, whether the deletion was accidental or intentional. After the soft-delete period, your *Cluster* resource and data are non-recoverable. The *Cluster* resource name remains reserved during the soft-delete period and you can’t create a new cluster with that name.\n+- Delete your *Cluster* resource -- a soft-delete operation is performed to allow the recovery of your Cluster resource, your data and associated workspaces within 14 days, whether the deletion was accidental or intentional. The *Cluster* resource name remains reserved during the soft-delete period and you can’t create a new cluster with that name. \n+After the soft-delete period, your *Cluster* resource and data are non-recoverable. Associated workspaces are de-associated from the *Cluster* resource and new data is ingested to shared Storage and encrypted with Microsoft key.\n \n   ```rst\n   DELETE"
  },
  {
    "Number": 109547,
    "Title": "Clarify column title on NULLs",
    "ClosedAt": "2020-03-31T04:38:08Z",
    "User": "rboucher",
    "FileName": "articles/azure-monitor/platform/metrics-supported-export-diagnostic-settings.md",
    "Addition": 5,
    "Delections": 4,
    "Changes": 9,
    "Patch": "@@ -2,9 +2,8 @@\n title:  Azure Monitor platform metrics exportable via Diagnostic Settings\n description: List of metrics available for each resource type with Azure Monitor.\n services: azure-monitor\n-\n ms.topic: reference\n-ms.date: 02/10/2020\n+ms.date: 03/30/2020\n ms.subservice: metrics\n ---\n # Azure Monitor platform metrics exportable via Diagnostic Settings\n@@ -19,7 +18,9 @@ Because of intricacies in the Azure Monitor backend, not all metrics are exporta\n \n ## Change to behavior for NULLs and Zero values \n  \n-For the platform metrics that can be exported via diagnostic settings, there are a few metrics for which Azure Monitor interprets '0s' as 'Nulls'. This has caused some confusion between real '0s' (emitted by resource) and interpreted '0s' (Nulls). Starting **April 1, 2020** platform metrics exported via diagnostic settings will no longer export '0s' unless they have truly been emitted by the underlying resource. Please note:\n+For the platform metrics that can be exported via diagnostic settings, there are a few metrics for which Azure Monitor interprets '0s' as 'Nulls'. This has caused some confusion between real '0s' (emitted by resource) and interpreted '0s' (Nulls). Soon a change will occur and platform metrics exported via diagnostic settings will no longer export '0s' unless they have truly been emitted by the underlying resource. The change was scheduled for April 1, 2020, but has been delayed due to priority shifts due to COVID-19. \n+\n+Please note:\n \n 1.\tIf you delete a resource group or a specific resource, metric data from the effected resources will no longer be sent to diagnostic setting export destinations. That is, it will no longer appear in Event Hubs, Storage Accounts and Log Analytics Workspaces.\n 2.\tThis improvement will be available in all public and private clouds.\n@@ -44,7 +45,7 @@ The table contains the following columns.\n > The table below may have a horizontal scroll bar at the bottom. If you believe you are missing information, check to see that the scroll bar is all the way to the left.  \n \n \n-| Exportable via Diagnostic Settings?  | Emits NULLs |  ResourceType  |  Metric  |  MetricDisplayName  |  Unit  |  AggregationType | \n+| Exportable via Diagnostic Settings?  | Already emit NULLs |  ResourceType  |  Metric  |  MetricDisplayName  |  Unit  |  AggregationType | \n |---|---| ---- | ----- | ------ | ---- | ---- | \n | ****Yes****  | No |  Microsoft.AnalysisServices/servers  |  CleanerCurrentPrice  |  Memory: Cleaner Current Price  |  Count  |  Average | \n | ****Yes****  | No |  Microsoft.AnalysisServices/servers  |  CleanerMemoryNonshrinkable  |  Memory: Cleaner Memory nonshrinkable  |  Bytes  |  Average | "
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/active-directory-b2c/custom-policy-developer-notes.md",
    "Addition": 64,
    "Delections": 47,
    "Changes": 111,
    "Patch": "@@ -9,7 +9,7 @@ manager: celestedg\n ms.service: active-directory\n ms.workload: identity\n ms.topic: conceptual\n-ms.date: 02/12/2020\n+ms.date: 03/30/2020\n ms.author: mimart\n ms.subservice: B2C\n ---\n@@ -55,85 +55,102 @@ Developers consuming the custom policy feature set should adhere to the followin\n \n Custom policy/Identity Experience Framework capabilities are under constant and rapid development. The following table is an index of features and component availability.\n \n-### Identity Providers, Tokens, Protocols\n+\n+### Protocols and authorization flows\n \n | Feature | Development | Preview | GA | Notes |\n |-------- | :-----------: | :-------: | :--: | ----- |\n-| IDP-OpenIDConnect |  |  | X | For example, Google+.  |\n-| IDP-OAUTH2 |  |  | X | For example, Facebook.  |\n-| IDP-OAUTH1 (twitter) |  | X |  | For example, Twitter. |\n-| IDP-OAUTH1 (ex-twitter) |  |  |  | Not supported |\n-| IDP-SAML |  |   | X | For example, Salesforce, ADFS. |\n-| IDP-WSFED | X |  |  |  |\n-| Relying Party OAUTH1 |  |  |  | Not supported. |\n-| Relying Party OAUTH2 |  |  | X |  |\n-| Relying Party OIDC |  |  | X |  |\n-| Relying Party SAML |  |X  |  |  |\n-| Relying Party WSFED | X |  |  |  |\n-| REST API with basic and certificate auth |  |  | X | For example, Azure Logic Apps. |\n-\n-### Component Support\n+| [OAuth2 authorization code](authorization-code-flow.md) |  |  | X |  |\n+| OAuth2 authorization code with PKCE |  |  | X | Mobile applications only  |\n+| [OAuth2 implicit flow](implicit-flow-single-page-application.md) |  |  | X |  |\n+| [OAuth2 resource owner password credentials](ropc-custom.md) |  | X |  |  |\n+| [OIDC Connect](openid-connect.md) |  |  | X |  |\n+| [SAML2](connect-with-saml-service-providers.md)  |  |X  |  | POST and Redirect bindings. |\n+| OAuth1 |  |  |  | Not supported. |\n+| WSFED | X |  |  |  |\n+\n+### Identify providers federation \n+\n+| Feature | Development | Preview | GA | Notes |\n+|-------- | :-----------: | :-------: | :--: | ----- |\n+| [OpenID Connect](openid-connect-technical-profile.md) |  |  | X | For example, Google+.  |\n+| [OAuth2](oauth2-technical-profile.md) |  |  | X | For example, Facebook.  |\n+| [OAuth1](oauth1-technical-profile.md) |  | X |  | For example, Twitter. |\n+| [SAML2](saml-technical-profile.md) |  |   | X | For example, Salesforce, ADFS. |\n+| WSFED| X |  |  |  |\n+\n+\n+### REST API integration\n+\n+| Feature | Development | Preview | GA | Notes |\n+|-------- | :-----------: | :-------: | :--: | ----- |\n+| [REST API with basic auth](secure-rest-api.md#http-basic-authentication) |  |  | X |  |\n+| [REST API with client certificate auth](secure-rest-api.md#https-client-certificate-authentication) |  |  | X |  |\n+| [REST API with OAuth2 bearer auth](secure-rest-api.md#oauth2-bearer-authentication) |  | X |  |  |\n+\n+### Component support\n \n | Feature | Development | Preview | GA | Notes |\n | ------- | :-----------: | :-------: | :--: | ----- |\n-| Azure Multi Factor Authentication |  |  | X |  |\n-| Azure Active Directory as local directory |  |  | X |  |\n-| Azure Email subsystem for email verification |  |  | X |  |\n-| Multi-language support|  |  | X |  |\n-| Predicate Validations |  |  | X | For example, password complexity. |\n-| Using third party email service providers |  |X  |  |  |\n+| [Phone factor authentication](phone-factor-technical-profile.md) |  |  | X |  |\n+| [Azure MFA authentication](multi-factor-auth-technical-profile.md) |  | X |  |  |\n+| [One-time password](one-time-password-technical-profile.md) |  | X |  |  |\n+| [Azure Active Directory](active-directory-technical-profile.md) as local directory |  |  | X |  |\n+| Azure email subsystem for email verification |  |  | X |  |\n+| [Third party email service providers](custom-email.md) |  |X  |  |  |\n+| [Multi-language support](localization.md)|  |  | X |  |\n+| [Predicate validations](predicates.md) |  |  | X | For example, password complexity. |\n+| [Display controls](display-controls.md) |  |X  |  |  |\n \n-### Content Definition\n+\n+### Page layout versions\n \n | Feature | Development | Preview | GA | Notes |\n | ------- | :-----------: | :-------: | :--: | ----- |\n-| Error page, api.error |  |  | X |  |\n-| IDP selection page, api.idpselections |  |  | X |  |\n-| IDP selection for signup, api.idpselections.signup |  |  | X |  |\n-| Forgot Password, api.localaccountpasswordreset |  |  | X |  |\n-| Local Account Sign-in, api.localaccountsignin |  |  | X |  |\n-| Local Account Sign-up, api.localaccountsignup |  |  | X |  |\n-| MFA page, api.phonefactor |  |  | X |  |\n-| Self-asserted social account sign-up, api.selfasserted |  |  | X |  |\n-| Self-asserted profile update, api.selfasserted.profileupdate |  |  | X |  |\n-| Unified signup or sign-in page, api.signuporsignin, with parameter \"disableSignup\" |  |  | X |  |\n-| JavaScript / Page layout |  | X |  |  |\n+| [2.0.0](page-layout.md#200) |  | X |  |  |\n+| [1.2.0](page-layout.md#120) |  | X |  |  |\n+| [1.1.0](page-layout.md#110) |  |  | X |  |\n+| [1.0.0](page-layout.md#100) |  |  | X |  |\n+| [JavaScript support](javascript-samples.md) |  | X |  |  |\n \n ### App-IEF integration\n \n | Feature | Development | Preview | GA | Notes |\n | ------- | :-----------: | :-------: | :--: | ----- |\n-| Query string parameter domain_hint |  |  | X | Available as claim, can be passed to IDP. |\n-| Query string parameter login_hint |  |  | X | Available as claim, can be passed to IDP. |\n-| Insert JSON into UserJourney via client_assertion | X |  |  | Will be deprecated. |\n-| Insert JSON into UserJourney as id_token_hint |  | X |  | Go-forward approach to pass JSON. |\n-| Pass IDP TOKEN to the application |  | X |  | For example, from Facebook to app. |\n+| Query string parameter `domain_hint` |  |  | X | Available as claim, can be passed to IDP. |\n+| Query string parameter `login_hint` |  |  | X | Available as claim, can be passed to IDP. |\n+| Insert JSON into user journey via `client_assertion` | X |  |  | Will be deprecated. |\n+| Insert JSON into user journey as `id_token_hint` |  | X |  | Go-forward approach to pass JSON. |\n+| [Pass identity provider token to the application](idp-pass-through-custom.md) |  | X |  | For example, from Facebook to app. |\n \n ### Session Management\n \n | Feature | Development | Preview | GA | Notes |\n | ------- | :-----------: | :-------: | :--: | ----- |\n-| SSO Session Provider |  |  | X |  |\n-| External Login Session Provider |  |  | X |  |\n-| SAML SSO  Session Provider |  |  | X |  |\n-| Default SSO Session Provider |  |  | X |  |\n+| [Default SSO session provider](custom-policy-reference-sso.md#defaultssosessionprovider) |  |  | X |  |\n+| [External login session provider](custom-policy-reference-sso.md#externalloginssosessionprovider) |  |  | X |  |\n+| [SAML SSO session provider](custom-policy-reference-sso.md#samlssosessionprovider) |  |  | X |  |\n+\n \n ### Security\n \n | Feature | Development | Preview | GA | Notes |\n |-------- | :-----------: | :-------: | :--: | ----- |\n | Policy Keys- Generate, Manual, Upload |  |  | X |  |\n | Policy Keys- RSA/Cert, Secrets |  |  | X |  |\n-| Policy upload |  |  | X |  |\n+\n \n ### Developer interface\n \n | Feature | Development | Preview | GA | Notes |\n | ------- | :-----------: | :-------: | :--: | ----- |\n | Azure Portal-IEF UX |  |  | X |  |\n-| Application Insights UserJourney Logs |  | X |  | Used for troubleshooting during development.  |\n-| Application Insights Event Logs (via orchestration steps) |  | X |  | Used to monitor user flows in production. |\n+| Policy upload |  |  | X |  |\n+| [Application Insights user journey logs](troubleshoot-with-application-insights.md) |  | X |  | Used for troubleshooting during development.  |\n+| [Application Insights event logs](application-insights-technical-profile.md) |  | X |  | Used to monitor user flows in production. |\n+\n \n ## Next steps\n \n-Learn more about [custom policies and the differences with user flows](custom-policy-overview.md).\n+- Check the [Microsoft Graph operations available for Azure AD B2C](microsoft-graph-operations.md)\n+- Learn more about [custom policies and the differences with user flows](custom-policy-overview.md)."
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/active-directory-b2c/saml-technical-profile.md",
    "Addition": 27,
    "Delections": 4,
    "Changes": 31,
    "Patch": "@@ -9,7 +9,7 @@ manager: celestedg\n ms.service: active-directory\n ms.workload: identity\n ms.topic: reference\n-ms.date: 02/13/2020\n+ms.date: 03/30/2020\n ms.author: mimart\n ms.subservice: B2C\n ---\n@@ -86,11 +86,32 @@ The **Name** attribute of the Protocol element needs to be set to `SAML2`.\n \n The **OutputClaims** element contains a list of claims returned by the SAML identity provider under the `AttributeStatement` section. You may need to map the name of the claim defined in your policy to the name defined in the identity provider. You can also include claims that aren't returned by the identity provider as long as you set the `DefaultValue` attribute.\n \n-To read the SAML assertion **NamedId** in **Subject** as a normalized claim, set the claim **PartnerClaimType** to `assertionSubjectName`. Make sure the **NameId** is the first value in assertion XML. When you define more than one assertion, Azure AD B2C picks the subject value from the last assertion.\n+### Subject name output claim\n+\n+To read the SAML assertion **NameId** in the **Subject** as a normalized claim, set the claim **PartnerClaimType** to value of the `SPNameQualifier` attribute. If the `SPNameQualifier`attribute is not presented, set the claim **PartnerClaimType** to value of the `NameQualifier` attribute. \n \n-The **OutputClaimsTransformations** element may contain a collection of **OutputClaimsTransformation** elements that are used to modify the output claims or generate new ones.\n \n-The following example shows the claims returned by the Facebook identity provider:\n+SAML assertion: \n+\n+```XML\n+<saml:Subject>\n+  <saml:NameID SPNameQualifier=\"http://your-idp.com/unique-identifier\" Format=\"urn:oasis:names:tc:SAML:2.0:nameid-format:transient\">david@contoso.com</saml:NameID>\n+\t<SubjectConfirmation Method=\"urn:oasis:names:tc:SAML:2.0:cm:bearer\">\n+\t  <SubjectConfirmationData InResponseTo=\"_cd37c3f2-6875-4308-a9db-ce2cf187f4d1\" NotOnOrAfter=\"2020-02-15T16:23:23.137Z\" Recipient=\"https://your-tenant.b2clogin.com/your-tenant.onmicrosoft.com/B2C_1A_TrustFrameworkBase/samlp/sso/assertionconsumer\" />\n+    </SubjectConfirmation>\n+  </saml:SubjectConfirmation>\n+</saml:Subject>\n+```\n+\n+Output claim:\n+\n+```XML\n+<OutputClaim ClaimTypeReferenceId=\"issuerUserId\" PartnerClaimType=\"http://your-idp.com/unique-identifier\" />\n+```\n+\n+If both `SPNameQualifier` or `NameQualifier` attributes are not presented in the SAML assertion, set the claim **PartnerClaimType** to `assertionSubjectName`. Make sure the **NameId** is the first value in assertion XML. When you define more than one assertion, Azure AD B2C picks the subject value from the last assertion.\n+\n+The following example shows the claims returned by a SAML identity provider:\n \n - The **issuerUserId** claim is mapped to the **assertionSubjectName** claim.\n - The **first_name** claim is mapped to the **givenName** claim.\n@@ -115,6 +136,8 @@ The technical profile also returns claims that aren't returned by the identity p\n </OutputClaims>\n ```\n \n+The **OutputClaimsTransformations** element may contain a collection of **OutputClaimsTransformation** elements that are used to modify the output claims or generate new ones.\n+\n ## Metadata\n \n | Attribute | Required | Description |"
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/active-directory-domain-services/network-considerations.md",
    "Addition": 4,
    "Delections": 5,
    "Changes": 9,
    "Patch": "@@ -5,12 +5,11 @@ services: active-directory-ds\n author: iainfoulds\n manager: daveba\n \n-ms.assetid: 23a857a5-2720-400a-ab9b-1ba61e7b145a\n ms.service: active-directory\n ms.subservice: domain-services\n ms.workload: identity\n ms.topic: conceptual\n-ms.date: 01/21/2020\n+ms.date: 03/30/2020\n ms.author: iainfou\n \n ---\n@@ -72,7 +71,7 @@ You can connect a virtual network to another virtual network (VNet-to-VNet) in t\n \n ![Virtual network connectivity using a VPN Gateway](./media/active-directory-domain-services-design-guide/vnet-connection-vpn-gateway.jpg)\n \n-For more information on using virtual private networking, read [Configure a VNet-to-VNet VPN gateway connection by using the Azure portal](https://docs.microsoft.com/azure/vpn-gateway/vpn-gateway-howto-vnet-vnet-resource-manager-portal).\n+For more information on using virtual private networking, read [Configure a VNet-to-VNet VPN gateway connection by using the Azure portal](../vpn-gateway/vpn-gateway-howto-vnet-vnet-resource-manager-portal.md).\n \n ## Name resolution when connecting virtual networks\n \n@@ -93,11 +92,11 @@ An Azure AD DS managed domain creates some networking resources during deploymen\n | Load balancer rules                     | When an Azure AD DS managed domain is configured for secure LDAP on TCP port 636, three rules are created and used on a load balancer to distribute the traffic. |\n \n > [!WARNING]\n-> Don't delete any of the network resource created by Azure AD DS. If you delete any of the network resources, an Azure AD DS service outage occurs.\n+> Don't delete or modify any of the network resource created by Azure AD DS, such as manually configuring the load balancer or rules. If you delete or modify any of the network resources, an Azure AD DS service outage may occur.\n \n ## Network security groups and required ports\n \n-A [network security group (NSG)](https://docs.microsoft.com/azure/virtual-network/virtual-networks-nsg) contains a list of rules that allow or deny network traffic to traffic in an Azure virtual network. A network security group is created when you deploy Azure AD DS that contains a set of rules that let the service provide authentication and management functions. This default network security group is associated with the virtual network subnet your Azure AD DS managed domain is deployed into.\n+A [network security group (NSG)](../virtual-network/virtual-networks-nsg.md) contains a list of rules that allow or deny network traffic to traffic in an Azure virtual network. A network security group is created when you deploy Azure AD DS that contains a set of rules that let the service provide authentication and management functions. This default network security group is associated with the virtual network subnet your Azure AD DS managed domain is deployed into.\n \n The following network security group rules are required for Azure AD DS to provide authentication and management services. Don't edit or delete these network security group rules for the virtual network subnet your Azure AD DS managed domain is deployed into.\n "
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/active-directory/governance/entitlement-management-access-package-first.md",
    "Addition": 29,
    "Delections": 25,
    "Changes": 54,
    "Patch": "@@ -12,7 +12,7 @@ ms.tgt_pltfrm: na\n ms.devlang: na\n ms.topic: tutorial\n ms.subservice: compliance\n-ms.date: 10/22/2019\n+ms.date: 03/30/2020\n ms.author: ajburnle\n ms.reviewer: markwahl-msft\n ms.collection: M365-identity-device-management\n@@ -82,79 +82,83 @@ An *access package* is a bundle of resources that a team or project needs and is\n \n 1. In the Azure portal, in the left navigation, click **Azure Active Directory**.\n \n-1. In the left menu, click **Identity Governance**\n+2. In the left menu, click **Identity Governance**\n \n-1. In the left menu, click **Access packages**.  If you see **Access denied**, ensure that an Azure AD Premium P2 license is present in your directory.\n+3. In the left menu, click **Access packages**.  If you see **Access denied**, ensure that an Azure AD Premium P2 license is present in your directory.\n \n-1. Click **New access package**.\n+4. Click **New access package**.\n \n     ![Entitlement management in the Azure portal](./media/entitlement-management-shared/access-packages-list.png)\n \n-1. On the **Basics** tab, type the name **Marketing Campaign** access package and description **Access to resources for the campaign**.\n+5. On the **Basics** tab, type the name **Marketing Campaign** access package and description **Access to resources for the campaign**.\n \n-1. Leave the **Catalog** drop-down list set to **General**.\n+6. Leave the **Catalog** drop-down list set to **General**.\n \n     ![New access package - Basics tab](./media/entitlement-management-access-package-first/basics.png)\n \n-1. Click **Next** to open the **Resource roles** tab.\n+7. Click **Next** to open the **Resource roles** tab.\n \n     On this tab, you select the resources and the resource role to include in the access package.\n \n-1. Click **Groups and Teams**.\n+8. Click **Groups and Teams**.\n \n-1. In the Select groups pane, find and select the **Marketing resources** group you created earlier.\n+9. In the Select groups pane, find and select the **Marketing resources** group you created earlier.\n \n     By default, you see groups inside and outside the **General** catalog. When you select a group outside of the **General** catalog, it will be added to the **General** catalog.\n \n     ![New access package - Resource roles tab](./media/entitlement-management-access-package-first/resource-roles-select-groups.png)\n \n-1. Click **Select** to add the group to the list.\n+10. Click **Select** to add the group to the list.\n \n-1. In the **Role** drop-down list, select **Member**.\n+11. In the **Role** drop-down list, select **Member**.\n \n     ![New access package - Resource roles tab](./media/entitlement-management-access-package-first/resource-roles.png)\n \n-1. Click **Next** to open the **Requests** tab.\n+    >[!NOTE]\n+    > When using [dynamic groups](../users-groups-roles/groups-create-rule.md) you will not see any other roles available besides owner. This is by design.\n+    > ![Scenario overview](./media/entitlement-management-access-package-first/dynamic-group-warning.png)\n+\n+12. Click **Next** to open the **Requests** tab.\n \n     On this tab, you create a request policy. A *policy* defines the rules or guardrails to access an access package. You create a policy that allows a specific user in the resource directory to request this access package.\n \n-1. In the **Users who can request access** section, click **For users in your directory** and then click **Specific users and groups**.\n+13. In the **Users who can request access** section, click **For users in your directory** and then click **Specific users and groups**.\n \n     ![New access package - Requests tab](./media/entitlement-management-access-package-first/requests.png)\n \n-1. Click **Add users and groups**.\n+14. Click **Add users and groups**.\n \n-1. In the Select users and groups pane, select the **Requestor1** user you created earlier.\n+15. In the Select users and groups pane, select the **Requestor1** user you created earlier.\n \n     ![New access package - Requests tab - Select users and groups](./media/entitlement-management-access-package-first/requests-select-users-groups.png)\n \n-1. Click **Select**.\n+16. Click **Select**.\n \n-1. Scroll down to the **Approval** and **Enable requests** sections.\n+17. Scroll down to the **Approval** and **Enable requests** sections.\n \n-1. Leave **Require approval** set to **No**.\n+18. Leave **Require approval** set to **No**.\n \n-1. For **Enable requests**, click **Yes** to enable this access package to be requested as soon as it is created.\n+19. For **Enable requests**, click **Yes** to enable this access package to be requested as soon as it is created.\n \n     ![New access package - Requests tab - Approval and Enable requests](./media/entitlement-management-access-package-first/requests-approval-enable.png)\n \n-1. Click **Next** to open the **Lifecycle** tab.\n+20. Click **Next** to open the **Lifecycle** tab.\n \n-1. In the **Expiration** section, set **Access package assignments expire** to **Number of days**.\n+21. In the **Expiration** section, set **Access package assignments expire** to **Number of days**.\n \n-1. Set **Assignments expire after** to **30** days.\n+22. Set **Assignments expire after** to **30** days.\n \n     ![New access package - Lifecycle tab](./media/entitlement-management-access-package-first/lifecycle.png)\n \n-1. Click **Next** to open the **Review + Create** tab.\n+23. Click **Next** to open the **Review + Create** tab.\n \n     ![New access package - Review + Create tab](./media/entitlement-management-access-package-first/review-create.png)\n \n     After a few moments, you should see a notification that the access package was successfully created.\n \n-1. In left menu of the Marketing Campaign access package, click **Overview**.\n+24. In left menu of the Marketing Campaign access package, click **Overview**.\n \n-1. Copy the **My Access portal link**.\n+25. Copy the **My Access portal link**.\n \n     You'll use this link for the next step.\n "
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/active-directory/manage-apps/application-proxy-faq.md",
    "Addition": 7,
    "Delections": 3,
    "Changes": 10,
    "Patch": "@@ -109,15 +109,15 @@ No, this scenario isn't supported because Application Proxy will terminate TLS t\n \n Refer to [Publish Remote Desktop with Azure AD Application Proxy](application-proxy-integrate-with-remote-desktop-services.md).\n \n-### Can I use Kerberos Constrained Delegation in the Remote Desktop Gateway publishing scenario?\n+### Can I use Kerberos Constrained Delegation (Single Sign-On - Windows Integrated Authentication) in the Remote Desktop Gateway publishing scenario?\n \n No, this scenario isn't supported.  \n \n ### My users don't use Internet Explorer 11 and the pre-authentication scenario doesn’t work for them. Is this expected?\n \n Yes, it’s expected. The pre-authentication scenario requires an ActiveX control, which isn't supported in third-party browsers.\n \n-### Is the Remote Desktop Web Client supported?\n+### Is the Remote Desktop Web Client (HTML5) supported?\n \n No, this scenario isn't currently supported. Follow our [UserVoice](https://aka.ms/aadapuservoice) feedback forum for updates on this feature.\n \n@@ -131,6 +131,10 @@ Yes, it's expected. If the user’s computer is Azure AD joined, the user signs\n \n Refer to [Enable remote access to SharePoint with Azure AD Application Proxy](application-proxy-integrate-with-sharepoint-server.md).\n \n+### Can I use the SharePoint mobile app (iOS/ Android) to access a published SharePoint server?\n+\n+The [SharePoint mobile app](https://docs.microsoft.com/sharepoint/administration/supporting-the-sharepoint-mobile-apps-online-and-on-premises) does not support Azure Active Directory pre-authentication currently.\n+\n ## Active Directory Federation Services (AD FS) publishing \n \n ### Can I use Azure AD Application Proxy as AD FS proxy (like Web Application Proxy)?\n@@ -143,7 +147,7 @@ No. Azure AD Application Proxy is designed to work with Azure AD and doesn’t f\n \n Currently, WebSocket protocol support is still in public preview and it may not work for other applications. Some customers have had mixed success using WebSocket protocol with other applications. If you test such scenarios, we would love to hear your results. Please send us your feedback at aadapfeedback@microsoft.com.\n \n-Features (Eventlogs, PowerShell and Remote Desktop Services) in Windows Admin Center (WAC) or Remote Desktop Web Client do not work through Azure AD Application Proxy presently.\n+Features (Eventlogs, PowerShell and Remote Desktop Services) in Windows Admin Center (WAC) or Remote Desktop Web Client (HTML5) do not work through Azure AD Application Proxy presently.\n \n ## Link translation\n "
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/analysis-services/analysis-services-addservprinc-admins.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -4,7 +4,7 @@ description: Learn how to add an automation service principal to the Azure Analy\n author: minewiskan\n ms.service: azure-analysis-services\n ms.topic: conceptual\n-ms.date: 10/29/2019\n+ms.date: 03/30/2020\n ms.author: owend\n ms.reviewer: minewiskan\n ms.custom: fasttrack-edit"
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/analysis-services/analysis-services-backup.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -4,7 +4,7 @@ description: This article describes how to backup and restore model metadata and\n author: minewiskan\n ms.service: azure-analysis-services\n ms.topic: conceptual\n-ms.date: 10/30/2019\n+ms.date: 03/30/2020\n ms.author: owend\n ms.reviewer: minewiskan\n "
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/analysis-services/analysis-services-bcdr.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -4,7 +4,7 @@ description: This article describes how Azure Analysis Services provides high av\n author: minewiskan\n ms.service: azure-analysis-services\n ms.topic: conceptual\n-ms.date: 10/30/2019\n+ms.date: 03/30/2020\n ms.author: owend\n ms.reviewer: minewiskan\n "
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/analysis-services/analysis-services-connect-excel.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -4,7 +4,7 @@ description: Learn how to connect to an Azure Analysis Services server by using\n author: minewiskan\n ms.service: azure-analysis-services\n ms.topic: conceptual\n-ms.date: 10/30/2019\n+ms.date: 03/30/2020\n ms.author: owend\n ms.reviewer: minewiskan\n "
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/analysis-services/analysis-services-connect-pbi.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -4,7 +4,7 @@ description: Learn how to connect to an Azure Analysis Services server by using\n author: minewiskan\n ms.service: azure-analysis-services\n ms.topic: conceptual\n-ms.date: 10/30/2019\n+ms.date: 03/30/2020\n ms.author: owend\n ms.reviewer: minewiskan\n "
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/analysis-services/analysis-services-connect.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -4,7 +4,7 @@ description: Learn how to connect to and get data from an Analysis Services serv\n author: minewiskan\n ms.service: azure-analysis-services\n ms.topic: conceptual\n-ms.date: 10/29/2019\n+ms.date: 03/30/2020\n ms.author: owend\n ms.reviewer: minewiskan\n "
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/analysis-services/analysis-services-create-powershell.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -4,7 +4,7 @@ description: Learn how to create an Azure Analysis Services server by using Powe\n author: minewiskan\n ms.service: azure-analysis-services\n ms.topic: quickstart\n-ms.date: 07/29/2019\n+ms.date: 03/30/2020\n ms.author: owend\n ms.reviewer: minewiskan\n #Customer intent: As a BI developer, I want to create an Azure Analysis Services server by using PowerShell."
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/analysis-services/analysis-services-overview.md",
    "Addition": 10,
    "Delections": 10,
    "Changes": 20,
    "Patch": "@@ -4,7 +4,7 @@ description: Learn about Azure Analysis Services, a fully managed platform as a\n author: minewiskan\n ms.service: azure-analysis-services\n ms.topic: overview\n-ms.date: 02/20/2020\n+ms.date: 03/30/2020\n ms.author: owend\n ms.reviewer: minewiskan\n #Customer intent: As a BI developer, I want to determine if Azure Analysis Services is the best data modeling platform for our organization.\n@@ -14,7 +14,7 @@ ms.reviewer: minewiskan\n \n ![Azure Analysis Services](./media/analysis-services-overview/aas-overview-aas-icon.png)\n \n-Azure Analysis Services is a fully managed platform as a service (PaaS) that provides enterprise-grade data models in the cloud. Use advanced mashup and modeling features to combine data from multiple data sources, define metrics, and secure your data in a single, trusted tabular semantic data model. The data model provides an easier and faster way for users to browse massive amounts of data for ad hoc data analysis.\n+Azure Analysis Services is a fully managed platform as a service (PaaS) that provides enterprise-grade data models in the cloud. Use advanced mashup and modeling features to combine data from multiple data sources, define metrics, and secure your data in a single, trusted tabular semantic data model. The data model provides an easier and faster way for users to perform ad hoc data analysis using tools like Power BI and Excel.\n \n ![Data sources](./media/analysis-services-overview/aas-overview-overall.png)\n \n@@ -34,7 +34,7 @@ Azure Analysis Services is available in **Developer**, **Basic**, and **Standard\n \n ### Developer tier\n \n-This tier is recommended for evaluation, development, and test scenarios. A single plan includes the same functionality of the standard tier, but is limited in processing power, QPUs, and memory size. Query replica scale out *is not available* for this tier. This tier does not offer an SLA.\n+This tier is recommended for evaluation, development, and test scenarios. A single plan includes the same functionality of the standard tier, but is limited in processing power, QPUs, and memory size. Query replica scale-out *is not available* for this tier. This tier does not offer an SLA.\n \n |Plan  |QPUs  |Memory (GB)  |\n |---------|---------|---------|\n@@ -204,6 +204,10 @@ Microsoft Analysis Services Projects is available as a free installable VSIX pac\n \n Manage your servers and model databases by using [SQL Server Management Studio (SSMS)](https://docs.microsoft.com/sql/ssms/download-sql-server-management-studio-ssms). Connect to your servers in the cloud. Run TMSL scripts right from the XMLA query window, and automate tasks by using TMSL scripts and PowerShell. New features and functionality happen fast - SSMS is updated monthly.\n \n+### Open-source tools\n+\n+Analysis Services has a vibrant community of developers who create tools. Be sure to check out [Tabular Editor](https://tabulareditor.github.io/), an open-source tool for creating, maintaining, and managing tabular models using an intuitive, lightweight editor. [DAX Studio](https://daxstudio.org/), is a great open-source tool for DAX authoring, diagnosis, performance tuning, and analysis.\n+\n ### PowerShell\n \n Server resource management tasks like creating server resources, suspending or resuming server operations, or changing the service level (tier) use Azure PowerShell cmdlets. Other tasks for managing databases such as adding or removing role members, processing, or running TMSL scripts use cmdlets in the SqlServer module. To learn more, see [Manage Azure Analysis Services with PowerShell](analysis-services-powershell.md).\n@@ -229,19 +233,15 @@ Azure Analysis Services also supports using [Dynamic Management Views (DMVs)](ht\n \n Documentation specific to Azure Analysis Services is included here. Use the table of contents on the left side of your browser screen to find articles. \n \n-Because Azure Analysis Services tabular models are much the same as tabular models in SQL Server Analysis Services, there's an extensive library of shared data modeling tutorials, conceptual, procedural, developer, and reference articles in [SQL Server Analysis Services documentation](https://docs.microsoft.com/analysis-services/analysis-services-overview). Articles in the SQL Server Analysis Services documentation show if they also apply to Azure Analysis Services by an APPLIES TO banner beneath the title.\n+Because tabular models in Azure Analysis Services are much the same as tabular models in SQL Server Analysis Services and Power BI Premium datasets, there's an extensive library of shared data modeling tutorials, conceptual, procedural, developer, and reference articles in [Analysis Services documentation](https://docs.microsoft.com/analysis-services/?view=azure-analysis-services-current). Articles in the shared Analysis Services documentation show if they also apply to Azure Analysis Services by an APPLIES TO banner beneath the title. You can also use the Version selector above the table of contents to see only those articles that apply to the platform you're using.\n \n ![Shared documentation](./media/analysis-services-overview/aas-overview-applies-to.png)\n \n ### Contribute!\n \n-Analysis Services documentation, like this article, are open source. If you have a GitHub account, you can edit an article by clicking Edit (pencil) in the upper right corner of your browser screen. Use the in-browser editor  and then click Propose file change. \n-\n-![Shared documentation](./media/analysis-services-overview/aas-overview-edit.png)\n-\n-Your contribution will be reviewed by the documentation team and if approved, your GitHub account name will be shown as a contributor. To learn more, see the [Docs contributor guide](https://docs.microsoft.com/contribute/).\n+Analysis Services documentation, like this article, are open source. To learn more about how you can contribute, see the [Docs contributor guide](https://docs.microsoft.com/contribute/). \n \n-Azure Analysis Services documentation also uses [GitHub Issues](https://docs.microsoft.com/teamblog/a-new-feedback-system-is-coming-to-docs). You can provide feedback about the product or documentation. Use **Feedback** at the bottom of an article. GitHub Issues aren't yet enabled for SQL Server Analysis Services documentation. \n+Azure Analysis Services documentation also uses [GitHub Issues](https://docs.microsoft.com/teamblog/a-new-feedback-system-is-coming-to-docs). You can provide feedback about the product or documentation. Use **Feedback** at the bottom of an article. GitHub Issues are not enabled for the shared Analysis Services documentation. \n \n ## Blogs\n "
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/application-gateway/application-gateway-diagnostics.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -91,9 +91,9 @@ The following snippet shows an example of the response:\n You can use different types of logs in Azure to manage and troubleshoot application gateways. You can access some of these logs through the portal. All logs can be extracted from Azure Blob storage and viewed in different tools, such as [Azure Monitor logs](../azure-monitor/insights/azure-networking-analytics.md), Excel, and Power BI. You can learn more about the different types of logs from the following list:\n \n * **Activity log**: You can use [Azure activity logs](../monitoring-and-diagnostics/insights-debugging-with-events.md) (formerly known as operational logs and audit logs) to view all operations that are submitted to your Azure subscription, and their status. Activity log entries are collected by default, and you can view them in the Azure portal.\n-* **Access log**: You can use this log to view Application Gateway access patterns and analyze important information. This includes the caller's IP, requested URL, response latency, return code, and bytes in and out. An access log is collected every 300 seconds. This log contains one record per instance of Application Gateway. The Application Gateway instance is identified by the instanceId property.\n+* **Access log**: You can use this log to view Application Gateway access patterns and analyze important information. This includes the caller's IP, requested URL, response latency, return code, and bytes in and out. An access log is collected every 60 seconds. This log contains one record per instance of Application Gateway. The Application Gateway instance is identified by the instanceId property.\n * **Performance log**: You can use this log to view how Application Gateway instances are performing. This log captures performance information for each instance, including total requests served, throughput in bytes, total requests served, failed request count, and healthy and unhealthy back-end instance count. A performance log is collected every 60 seconds. The Performance log is available only for the v1 SKU. For the v2 SKU, use [Metrics](application-gateway-metrics.md) for performance data.\n-* **Firewall log**: You can use this log to view the requests that are logged through either detection or prevention mode of an application gateway that is configured with the web application firewall.\n+* **Firewall log**: You can use this log to view the requests that are logged through either detection or prevention mode of an application gateway that is configured with the web application firewall. Firewall logs are collected every 60 seconds. \n \n > [!NOTE]\n > Logs are available only for resources deployed in the Azure Resource Manager deployment model. You cannot use logs for resources in the classic deployment model. For a better understanding of the two models, see the [Understanding Resource Manager deployment and classic deployment](../azure-resource-manager/management/deployment-models.md) article."
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/application-gateway/application-gateway-faq.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -348,11 +348,11 @@ Currently, one instance of Ingress Controller can only be associated to one Appl\n \n Application Gateway provides three logs: \n \n-* **ApplicationGatewayAccessLog**: The access log contains each request submitted to the application gateway frontend. The data includes the caller's IP, URL requested, response latency, return code, and bytes in and out. The access log is collected every 300 seconds. It contains one record per application gateway.\n+* **ApplicationGatewayAccessLog**: The access log contains each request submitted to the application gateway frontend. The data includes the caller's IP, URL requested, response latency, return code, and bytes in and out. It contains one record per application gateway.\n * **ApplicationGatewayPerformanceLog**: The performance log captures performance information for each application gateway. Information includes the throughput in bytes, total requests served, failed request count, and healthy and unhealthy backend instance count.\n * **ApplicationGatewayFirewallLog**: For application gateways that you configure with WAF, the firewall log contains requests that are logged through either detection mode or prevention mode.\n \n-For more information, see [Backend health, diagnostics logs, and metrics for Application Gateway](application-gateway-diagnostics.md).\n+All logs are collected every 60 seconds. For more information, see [Backend health, diagnostics logs, and metrics for Application Gateway](application-gateway-diagnostics.md).\n \n ### How do I know if my backend pool members are healthy?\n "
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/automation/automation-dsc-onboarding.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -104,7 +104,7 @@ You can onboard Linux servers running on-premises or in other cloud environments\n \n      `/opt/microsoft/dsc/Scripts/Register.py <Automation account registration key> <Automation account registration URL>`\n \n-   - To find the registration key and registration URL for your Automation account, see the [Onboarding securely using registration](#onboarding-securely-using-registration) section if this article.\n+   - To find the registration key and registration URL for your Automation account, see the [Onboarding securely using registration](#onboarding-securely-using-registration) section of this article.\n \n 3. If the PowerShell DSC Local Configuration Manager (LCM) defaults don't match your use case, or you want to onboard machines that only report to Azure Automation State Configuration, follow steps 4-7. Otherwise, proceed directly to step 7.\n "
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/automation/automation-tutorial-update-management.md",
    "Addition": 44,
    "Delections": 46,
    "Changes": 90,
    "Patch": "@@ -34,13 +34,13 @@ Sign in to the Azure portal at https://portal.azure.com.\n \n ## View update assessment\n \n-After Update Management is enabled, the **Update management** pane opens. If any updates are identified as missing, a list of missing updates is shown on the **Missing updates** tab.\n+After you enable Update Management, the **Update management** page opens. If any updates are identified as missing, a list of missing updates is shown on the **Missing updates** tab.\n \n Under **Information link**, select the update link to open the support article for the update. You can learn important information about the update.\n \n ![View update status](./media/automation-tutorial-update-management/manageupdates-view-status-win.png)\n \n-Click anywhere else on the update to open the **Log search** pane for the selected update. The query for the log search is predefined for that specific update. You can modify this query or create your own query to view detailed information about the updates that are deployed or missing in your environment.\n+Click anywhere else on the update to open the **Log search** pane for the selected update. The query for the log search is predefined for that specific update. You can modify this query or create your own query to view detailed information about updates deployed or missing in your environment.\n \n ![View update status](./media/automation-tutorial-update-management/logsearch.png)\n \n@@ -50,18 +50,18 @@ In this step, you learn to set up an alert to let you know the status of an upda\n \n ### Alert conditions\n \n-In your Automation Account, under **Monitoring** go to **Alerts**, and then click **+ New alert rule**.\n+In your Automation account, go to **Alerts** under **Monitoring**, then click **New alert rule**.\n \n-Your Automation Account is already selected as the resource. If you want to change it you can click **Select** and on the **Select a resource** page, select **Automation Accounts** in the **Filter by resource type** dropdown. Select your Automation Account, and then select **Done**.\n+Your Automation account is already selected as the resource. If you want to change it, click **Select**. On the **Select a resource** page, choose **Automation Accounts** from the **Filter by resource type** dropdown menu. Select your Automation account, and then click **Done**.\n \n-Click **Add condition** to select the signal that is appropriate for your update deployment. The following table shows the details of the two available signals for update deployments:\n+Click **Add condition** to select the signal that is appropriate for your update deployment. The following table shows the details of the two available signals.\n \n |Signal Name|Dimensions|Description|\n |---|---|---|\n-|**Total Update Deployment Runs**|- Update Deployment Name</br>- Status|This signal is used to alert on the overall status of an update deployment.|\n-|**Total Update Deployment Machine Runs**|- Update Deployment Name</br>- Status</br>- Target Computer</br>- Update Deployment Run ID|This signal is used to alert on the status of an update deployment targeted at specific machines|\n+|`Total Update Deployment Runs`|- Update Deployment Name<br>- Status|This signal alerts on the overall status of an update deployment.|\n+|`Total Update Deployment Machine Runs`|- Update Deployment Name</br>- Status</br>- Target Computer</br>- Update Deployment Run ID|This signal alerts on the status of an update deployment targeted at specific machines.|\n \n-For the dimension values, select a valid value from the list. If the value you are looking for is not in the list, click the **\\+** sign next to the dimension and type in the custom name. You can then select the value you want to look for. If you want to select all values from a dimension, click the **Select \\*** button. If you do not choose a value for a dimension, that dimension will be ignored during evaluation.\n+For a dimension, select a valid value from the list. If the value you want isn't in the list, click the **\\+** sign next to the dimension and type in the custom name. Then select the value to look for. If you want to select all values for a dimension, click the **Select \\*** button. If you do not choose a value for a dimension, Update Management ignores that dimension.\n \n ![Configure signal logic](./media/automation-tutorial-update-management/signal-logic.png)\n \n@@ -73,27 +73,26 @@ Under **2. Define alert details**, enter a name and description for the alert. S\n \n ![Configure signal logic](./media/automation-tutorial-update-management/define-alert-details.png)\n \n-Under **Action groups**, select **Create New**. An action group is a group of actions that you can use across multiple alerts. The actions can include but are not limited to email notifications, runbooks, webhooks, and many more. To learn more about action groups, see [Create and manage action groups](../azure-monitor/platform/action-groups.md).\n+Under **Action groups**, select **Create New**. An action group is a group of actions that you can use across multiple alerts. The actions can include email notifications, runbooks, webhooks, and many more. To learn more about action groups, see [Create and manage action groups](../azure-monitor/platform/action-groups.md).\n \n-In the **Action group name** box, enter a name for the alert and a short name. The short name is used in place of a full action group name when notifications are sent by using this group.\n+In the **Action group name** field, enter a name for the alert and a short name. Update Management uses the short name in place of a full action group name when sending notifications using the specified group.\n \n-Under **Actions**, enter a name for the action, like **Email notifications**. Under **Action type**, select **Email/SMS/Push/Voice**. Under **Details**, select **Edit details**.\n+Under **Actions**, enter a name for the action, like **Email Notification**. For **Action Type**, select **Email/SMS/Push/Voice**. For **Details**, select **Edit details**.\n \n In the **Email/SMS/Push/Voice** pane, enter a name. Select the **Email** check box, and then enter a valid email address.\n \n ![Configure an email action group](./media/automation-tutorial-update-management/configure-email-action-group.png)\n \n-In the **Email/SMS/Push/Voice** pane, select **Ok**. In the **Add action group** pane, select **Ok**.\n+In the **Email/SMS/Push/Voice** pane, click **OK**. In the **Add action group** pane, click **OK**.\n \n-To customize the subject of the alert email,  under **Create rule**, under **Customize actions**, select **Email subject**. When you're finished, select **Create alert rule**. The alert tells you when an update deployment succeeds, and which machines were part of that update deployment run.\n+To customize the subject of the alert email, under **Create rule**, under **Customize actions**, select **Email subject**. When you're finished, select **Create alert rule**. The alert tells you when an update deployment succeeds, and which machines were part of the update deployment run.\n \n ## Schedule an update deployment\n \n-Next, schedule a deployment that follows your release schedule and service window to install updates. You can choose which update types to include in the deployment. For example, you can include critical or security updates and exclude update rollups.\n+Next, schedule a deployment that follows your release schedule and service window to install updates. You can choose the update types to include in the deployment. For example, you can include critical or security updates and exclude update rollups.\n \n >[!NOTE]\n->When you schedule an update deployment, it creates a [schedule](shared-resources/schedules.md) resource linked to the **Patch-MicrosoftOMSComputers** runbook that handles the update deployment on the target machines. If you delete the schedule resource from the Azure portal or using PowerShell after creating the deployment, it breaks the scheduled update deployment and presents an error when you attempt to reconfigure it from the portal. You can only delete the schedule resource by deleting the corresponding deployment schedule.  \n->\n+>Scheduling an update deployment creates a [schedule](shared-resources/schedules.md) resource linked to the **Patch-MicrosoftOMSComputers** runbook that handles the update deployment on the target machines. If you delete the schedule resource from the Azure portal or using PowerShell after creating the deployment, the deletion breaks the scheduled update deployment and presents an error when you attempt to reconfigure the schedule resource from the portal. You can only delete the schedule resource by deleting the corresponding deployment schedule.  \n \n To schedule a new update deployment for the VM, go to **Update management**, and then select **Schedule update deployment**.\n \n@@ -103,89 +102,89 @@ Under **New update deployment**, specify the following information:\n \n * **Operating system**: Select the OS to target for the update deployment.\n \n-* **Groups to update (preview)**: Define a query based on a combination of subscription, resource groups, locations, and tags to build a dynamic group of Azure VMs to include in your deployment. To learn more, see [Dynamic Groups](automation-update-management-groups.md)\n+* **Groups to update (preview)**: Define a query that combines subscription, resource groups, locations, and tags to build a dynamic group of Azure VMs to include in your deployment. To learn more, see [Dynamic Groups](automation-update-management-groups.md).\n \n-* **Machines to update**: Select a Saved search, Imported group, or pick Machine from the drop-down and select individual machines. If you choose **Machines**, the readiness of the machine is shown in the **Update agent readiness** column. To learn about the different methods of creating computer groups in Azure Monitor logs, see [Computer groups in Azure Monitor logs](../azure-monitor/platform/computer-groups.md)\n+* **Machines to update**: Select a Saved search, Imported group, or pick **Machines** from the dropdown menu and select individual machines. If you choose **Machines**, the readiness of each machine is shown in the **Update agent readiness** column. To learn about the different methods of creating computer groups in Azure Monitor logs, see [Computer groups in Azure Monitor logs](../azure-monitor/platform/computer-groups.md).\n \n-* **Update classification**: Select the supported update classifications available for each product that can be included in the update deployment. For this tutorial, leave all types selected.\n+* **Update classification**: For each product, deselect all supported update classifications but the ones to include in your update deployment. For this tutorial, leave all types selected for all products.\n \n   The classification types are:\n \n    |OS  |Type  |\n    |---------|---------|\n-   |Windows     | Critical updates</br>Security updates</br>Update rollups</br>Feature packs</br>Service packs</br>Definition updates</br>Tools</br>Updates        |\n+   |Windows     | Critical updates</br>Security updates</br>Update rollups</br>Feature packs</br>Service packs</br>Definition updates</br>Tools</br>Updates<br>Driver        |\n    |Linux     | Critical and security updates</br>Other updates       |\n \n-   For a description of the classification types, see [update classifications](automation-view-update-assessments.md#update-classifications).\n+   For descriptions of the classification types, see [Update classifications](automation-view-update-assessments.md#update-classifications).\n \n-* **Updates to include/exclude** - This opens the **Include/Exclude** page. Updates to be included or excluded are on separate tabs.\n+* **Updates to include/exclude** - Opens the Include/Exclude page. Updates to be included or excluded are on separate tabs.\n \n > [!NOTE]\n-> It is important to know that exclusions override inclusions. For instance, if you define an exclusion rule of `*`, then no patches or packages are installed as they are all excluded. Excluded patches still show as missing from the machine. For Linux machines if a package is included but has a dependent package that was excluded, the package is not installed.\n+> It's important to know that exclusions override inclusions. For instance, if you define an exclusion rule of `*`, Update Management installs no patches or packages, as they're all excluded. Excluded patches still show as missing from the machine. For Linux machines, if you include a package that has a dependent package that has been excluded, Update Management doesn't install the main package.\n \n > [!NOTE]\n-> You cannot specify updates that have been superseded for inclusion with the update deployment.\n+> You can't specify updates that have been superseded for inclusion with the update deployment.\n >\n \n * **Schedule settings**: The **Schedule Settings** pane opens. The default start time is 30 minutes after the current time. You can set the start time to any time from 10 minutes in the future.\n \n-   You can also specify whether the deployment occurs once, or set up a recurring schedule. Under **Recurrence**, select **Once**. Leave the default as 1 day and select **Ok**. This sets up a recurring schedule.\n+   You can also specify whether the deployment occurs once, or set up a recurring schedule. Under **Recurrence**, select **Once**. Leave the default as 1 day and click **OK**. These entries set up a recurring schedule.\n \n * **Pre-scripts + Post-scripts**: Select the scripts to run before and after your deployment. To learn more, see [Manage Pre and Post scripts](pre-post-scripts.md).\n \n-* **Maintenance window (minutes)**: Leave the default value. Maintenance windows control the amount of time allowed for updates to install. Consider the following details when specifying a maintenance window.\n+* **Maintenance window (minutes)**: Leave the default value. Maintenance windows control the amount of time allowed for updates to install. Consider the following details when specifying a maintenance window:\n \n-  * Maintenance windows control how many updates are attempted to be installed.\n-  * Update Management does not stop installing new updates if the end of a maintenance window is approaching.\n-  * Update Management does not terminate in-progress updates if when the maintenance window is exceeded.\n-  * If the maintenance window is exceeded on Windows, it is often because of a service pack update taking a long time to install.\n+  * Maintenance windows control how many updates are installed.\n+  * Update Management doesn't stop installing new updates if the end of a maintenance window is approaching.\n+  * Update Management doesn't terminate in-progress updates if the maintenance window is exceeded.\n+  * If the maintenance window is exceeded on Windows, it's often because a service pack update is taking a long time to install.\n \n   > [!NOTE]\n   > To avoid updates being applied outside of a maintenance window on Ubuntu, reconfigure the Unattended-Upgrade package to disable automatic updates. For information about how to configure the package, see [Automatic Updates topic in the Ubuntu Server Guide](https://help.ubuntu.com/lts/serverguide/automatic-updates.html).\n \n-* **Reboot options**: This setting determines how reboots should be handled. Available options are:\n-  * Reboot if required (Default)\n+* **Reboot options**: Use to specify options for handling reboots. The following options are available:\n+  * Reboot if necessary (default)\n   * Always reboot\n   * Never reboot\n-  * Only reboot - will not install updates\n+  * Only reboot - doesn't install updates\n \n > [!NOTE]\n-> The Registry keys listed under [Registry keys used to manage restart](/windows/deployment/update/waas-restart#registry-keys-used-to-manage-restart) can cause a reboot event if **Reboot Control** is set to **Never Reboot**.\n+> The registry keys listed under [Registry keys used to manage restart](/windows/deployment/update/waas-restart#registry-keys-used-to-manage-restart) can cause a reboot event if **Reboot options** is set to **Never reboot**.\n \n-When you're finished configuring the schedule, select **Create**.\n+When you're finished configuring the schedule, click **Create**.\n \n ![Update Schedule Settings pane](./media/automation-tutorial-update-management/manageupdates-schedule-win.png)\n \n-You're returned to the status dashboard. Select **Scheduled Update deployments** to show the deployment schedule you created.\n+You're returned to the status dashboard. Select **Scheduled Update deployments** to show the deployment schedule just created.\n \n > [!NOTE]\n-> Update Management supports deploying first party updates and pre-downloading patches. This requires changes on the systems being patched, see [first party and pre-download support](automation-configure-windows-update.md) to learn how to configure these settings on your systems.\n+> Update Management supports deploying first-party updates and pre-downloading patches. This support requires changes on the systems being patched. See [First party and pre-download support](automation-configure-windows-update.md) to learn how to configure these settings on your systems.\n \n-**Update Deployments** can also be created programmatically. To learn how to create an **Update Deployment** with the REST API, see [Software Update Configurations - Create](/rest/api/automation/softwareupdateconfigurations/create). There is also a sample runbook that can be used to create a weekly **Update Deployment**. To learn more about this runbook, see [Create a weekly update deployment for one or more VMs in a resource group](https://gallery.technet.microsoft.com/scriptcenter/Create-a-weekly-update-2ad359a1).\n+You can also create update deployments programmatically. To learn how to create an update deployment with the REST API, see [Software Update Configurations - Create](/rest/api/automation/softwareupdateconfigurations/create). There is also a sample runbook that you can use to create a weekly update deployment. To learn more about this runbook, see [Create a weekly update deployment for one or more VMs in a resource group](https://gallery.technet.microsoft.com/scriptcenter/Create-a-weekly-update-2ad359a1).\n \n ## View results of an update deployment\n \n-After the scheduled deployment starts, you can see the status for that deployment on the **Update deployments** tab under **Update management**. The status is **In progress** when the deployment is currently running. When the deployment finishes, if it's successful, the status changes to **Succeeded**. When there are failures with one or more updates in the deployment, the status is **Partially failed**.\n+After the scheduled deployment starts, you can see the status for that deployment on the **Update deployments** tab under **Update management**. The status is **In progress** when the deployment is currently running. When the deployment finishes successfully, the status changes to **Succeeded**. If there are failures with one or more updates in the deployment, the status is **Partially failed**.\n \n-Select the completed update deployment to see the dashboard for that update deployment.\n+Select the completed update deployment to see its dashboard.\n \n ![Update deployment status dashboard for a specific deployment](./media/automation-tutorial-update-management/manageupdates-view-results.png)\n \n Under **Update results**, a summary provides the total number of updates and deployment results on the VM. The table on the right shows a detailed breakdown of each update and the installation results.\n \n-The following list shows the available values:\n+The available values are:\n \n-* **Not attempted**: The update wasn't installed because there was insufficient time available based on the maintenance window duration defined.\n+* **Not attempted**: The update wasn't installed because there was insufficient time available, based on the maintenance window duration defined.\n * **Succeeded**: The update succeeded.\n * **Failed**: The update failed.\n \n-Select **All logs** to see all log entries that the deployment created.\n+Select **All logs** to see all log entries that the deployment has created.\n \n Select **Output** to see the job stream of the runbook responsible for managing the update deployment on the target VM.\n \n Select **Errors** to see detailed information about any errors from the deployment.\n \n-When your update deployment is successful, an email that's similar to the following example is sent to show success of the deployment:\n+When your update deployment succeeds, you receive a confirming email similar to the following:\n \n ![Configure email action group](./media/automation-tutorial-update-management/email-notification.png)\n \n@@ -203,5 +202,4 @@ In this tutorial, you learned how to:\n Continue to the overview for the Update Management solution.\n \n > [!div class=\"nextstepaction\"]\n-> [Update Management solution](automation-update-management.md)\n-\n+> [Update Management solution](automation-update-management.md)\n\\ No newline at end of file"
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/automation/automation-update-management-deploy-template.md",
    "Addition": 83,
    "Delections": 81,
    "Changes": 164,
    "Patch": "@@ -6,7 +6,7 @@ ms.subservice: update-management\n ms.topic: conceptual\n author: mgoedtel\n ms.author: magoedte\n-ms.date: 02/27/2020\n+ms.date: 03/30/2020\n \n ---\n \n@@ -52,6 +52,7 @@ The following parameters in the template are set with a default value for the Lo\n \n * sku - defaults to the new Per-GB pricing tier released in the April 2018 pricing model\n * data retention - defaults to thirty days\n+* capacity reservation - defaults to 100 GB\n \n >[!WARNING]\n >If creating or configuring a Log Analytics workspace in a subscription that has opted into the new April 2018 pricing model, the only valid Log Analytics pricing tier is **PerGB2018**.\n@@ -67,89 +68,89 @@ The following parameters in the template are set with a default value for the Lo\n     ```json\n     {\n     \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n-\t\"contentVersion\": \"1.0.0.0\",\n-\t\"parameters\": {\n-\t\t\"workspaceName\": {\n-\t\t\t\"type\": \"string\",\n-\t\t\t\"metadata\": {\n-\t\t\t\t\"description\": \"Workspace name\"\n-\t\t\t}\n-\t\t},\n-\t\t\"pricingTier\": {\n-\t\t\t\"type\": \"string\",\n-\t\t\t\"allowedValues\": [\n-\t\t\t\t\"pergb2018\",\n-\t\t\t\t\"Free\",\n-\t\t\t\t\"Standalone\",\n-\t\t\t\t\"PerNode\",\n-\t\t\t\t\"Standard\",\n-\t\t\t\t\"Premium\"\n-\t\t\t],\n-\t\t\t\"defaultValue\": \"pergb2018\",\n-\t\t\t\"metadata\": {\n-\t\t\t\t\"description\": \"Pricing tier: perGB2018 or legacy tiers (Free, Standalone, PerNode, Standard or Premium) which are not available to all customers.\"\n-\t\t\t}\n-\t\t},\n-\t\t\"dataRetention\": {\n-\t\t\t\"type\": \"int\",\n-\t\t\t\"defaultValue\": 30,\n-\t\t\t\"minValue\": 7,\n-\t\t\t\"maxValue\": 730,\n-\t\t\t\"metadata\": {\n-\t\t\t\t\"description\": \"Number of days of retention. Workspaces in the legacy Free pricing tier can only have 7 days.\"\n-\t\t\t}\n-\t\t},\n-\t\t\"immediatePurgeDataOn30Days\": {\n-\t\t\t\"type\": \"bool\",\n-\t\t\t\"defaultValue\": \"[bool('false')]\",\n-\t\t\t\"metadata\": {\n-\t\t\t\t\"description\": \"If set to true when changing retention to 30 days, older data will be immediately deleted. Use this with extreme caution. This only applies when retention is being set to 30 days.\"\n-\t\t\t}\n-\t\t},\n-\t\t\"location\": {\n-\t\t\t\"type\": \"string\",\n-\t\t\t\"allowedValues\": [\n-\t\t\t\t\"australiacentral\",\n-\t\t\t\t\"australiaeast\",\n-\t\t\t\t\"australiasoutheast\",\n-\t\t\t\t\"brazilsouth\",\n-\t\t\t\t\"canadacentral\",\n-\t\t\t\t\"centralindia\",\n-\t\t\t\t\"centralus\",\n-\t\t\t\t\"eastasia\",\n-\t\t\t\t\"eastus\",\n-\t\t\t\t\"eastus2\",\n-\t\t\t\t\"francecentral\",\n-\t\t\t\t\"japaneast\",\n-\t\t\t\t\"koreacentral\",\n-\t\t\t\t\"northcentralus\",\n-\t\t\t\t\"northeurope\",\n-\t\t\t\t\"southafricanorth\",\n-\t\t\t\t\"southcentralus\",\n-\t\t\t\t\"southeastasia\",\n-\t\t\t\t\"uksouth\",\n-\t\t\t\t\"ukwest\",\n-\t\t\t\t\"westcentralus\",\n-\t\t\t\t\"westeurope\",\n-\t\t\t\t\"westus\",\n-\t\t\t\t\"westus2\"\n-\t\t\t],\n-\t\t\t\"metadata\": {\n-\t\t\t\t\"description\": \"Specifies the location in which to create the workspace.\"\n-\t\t\t}\n-\t\t},\n+    \"contentVersion\": \"1.0.0.0\",\n+    \"parameters\": {\n+        \"workspaceName\": {\n+            \"type\": \"string\",\n+            \"metadata\": {\n+                \"description\": \"Workspace name\"\n+            }\n+        },\n+        \"sku\": {\n+            \"type\": \"string\",\n+            \"allowedValues\": [\n+                \"pergb2018\",\n+                \"Free\",\n+                \"Standalone\",\n+                \"PerNode\",\n+                \"Standard\",\n+                \"Premium\"\n+            ],\n+            \"defaultValue\": \"pergb2018\",\n+            \"metadata\": {\n+                \"description\": \"Pricing tier: perGB2018 or legacy tiers (Free, Standalone, PerNode, Standard or Premium) which are not available to all customers.\"\n+            }\n+        },\n+        \"dataRetention\": {\n+            \"type\": \"int\",\n+            \"defaultValue\": 30,\n+            \"minValue\": 7,\n+            \"maxValue\": 730,\n+            \"metadata\": {\n+                \"description\": \"Number of days of retention. Workspaces in the legacy Free pricing tier can only have 7 days.\"\n+            }\n+        },\n+        \"immediatePurgeDataOn30Days\": {\n+            \"type\": \"bool\",\n+            \"defaultValue\": \"[bool('false')]\",\n+            \"metadata\": {\n+                \"description\": \"If set to true when changing retention to 30 days, older data will be immediately deleted. Use this with extreme caution. This only applies when retention is being set to 30 days.\"\n+            }\n+        },\n+        \"location\": {\n+            \"type\": \"string\",\n+            \"allowedValues\": [\n+                \"australiacentral\",\n+                \"australiaeast\",\n+                \"australiasoutheast\",\n+                \"brazilsouth\",\n+                \"canadacentral\",\n+                \"centralindia\",\n+                \"centralus\",\n+                \"eastasia\",\n+                \"eastus\",\n+                \"eastus2\",\n+                \"francecentral\",\n+                \"japaneast\",\n+                \"koreacentral\",\n+                \"northcentralus\",\n+                \"northeurope\",\n+                \"southafricanorth\",\n+                \"southcentralus\",\n+                \"southeastasia\",\n+                \"uksouth\",\n+                \"ukwest\",\n+                \"westcentralus\",\n+                \"westeurope\",\n+                \"westus\",\n+                \"westus2\"\n+            ],\n+            \"metadata\": {\n+                \"description\": \"Specifies the location in which to create the workspace.\"\n+            }\n+        },\n         \"automationAccountName\": {\n             \"type\": \"string\",\n             \"metadata\": {\n                 \"description\": \"Automation account name\"\n             }\n         },\n-\t\t\"automationAccountLocation\": {\n-\t\t    \"type\": \"string\",\n-\t\t\t\"metadata\": {\n-\t\t\t    \"description\": \"Specify the location in which to create the Automation account.\"\n-\t\t\t}\n-\t\t}\n+        \"automationAccountLocation\": {\n+            \"type\": \"string\",\n+            \"metadata\": {\n+                \"description\": \"Specify the location in which to create the Automation account.\"\n+            }\n+        }\n     },\n     \"variables\": {\n         \"Updates\": {\n@@ -164,7 +165,8 @@ The following parameters in the template are set with a default value for the Lo\n             \"apiVersion\": \"2017-03-15-preview\",\n             \"location\": \"[parameters('location')]\",\n             \"properties\": {\n-                \"sku\": { \n+                \"sku\": {\n+                    \"Name\": \"[parameters('sku')]\",\n                     \"name\": \"CapacityReservation\",\n                     \"capacityReservationLevel\": 100\n                 },\n@@ -209,7 +211,7 @@ The following parameters in the template are set with a default value for the Lo\n                     \"name\": \"Basic\"\n                 }\n             },\n-\t\t},\n+        },\n         {\n             \"apiVersion\": \"2015-11-01-preview\",\n             \"type\": \"Microsoft.OperationalInsights/workspaces/linkedServices\",\n@@ -227,7 +229,7 @@ The following parameters in the template are set with a default value for the Lo\n     }\n     ```\n \n-2. Edit the template to meet your requirements.\n+2. Edit the template to meet your requirements. Consider creating a [Resource Manager parameters file](../azure-resource-manager/templates/parameter-files.md) instead of passing parameters as inline values.\n \n 3. Save this file as deployUMSolutiontemplate.json to a local folder.\n "
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/azure-arc/servers/overview.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -142,7 +142,7 @@ The Azure Connected Machine agent for Windows and Linux can be upgraded to the l\n \n ### Agent status\n \n-The Connected Machine agent sends a regular heartbeat message to the service every 5 minutes. If one is not received for 15 minutes, the machine is considered offline and the status will automatically be changed to **Disconnected** in the portal. Upon receiving a subsequent heartbeat message from the Connected Machine agent, its status will automatically be changed to **Connected**.\n+The Connected Machine agent sends a regular heartbeat message to the service every 5 minutes. If the service stops receiving these heartbeat messages from a machine, that machine is considered offline and the status will automatically be changed to **Disconnected** in the portal within 15 to 30 minutes. Upon receiving a subsequent heartbeat message from the Connected Machine agent, its status will automatically be changed to **Connected**.\n \n ## Install and configure agent\n \n@@ -156,4 +156,4 @@ Connecting machines in your hybrid environment directly with Azure can be accomp\n \n ## Next steps\n \n-- To begin evaluating Azure Arc for servers (preview), follow the article [Connect hybrid machines to Azure from the Azure portal](onboard-portal.md). \n\\ No newline at end of file\n+- To begin evaluating Azure Arc for servers (preview), follow the article [Connect hybrid machines to Azure from the Azure portal](onboard-portal.md). "
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/azure-government/documentation-government-ase-disa-cap.md",
    "Addition": 24,
    "Delections": 16,
    "Changes": 40,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Use DISA CAP to connect to Azure Government\n+title: ASE deployment with DISA CAP\n description: This document provides a comparison of features and guidance on developing applications for Azure Government\n services: azure-government\n cloud: gov\n@@ -18,53 +18,61 @@ ms.author: joscot\n \n ---\n \n-# App Service Environment reference for DoD customers using a DISA CAP connection\n+# App Service Environment reference for DoD customers connected to the DISA CAP\n \n This article explains the baseline configuration of an App Service Environment (ASE) with an internal load balancer (ILB) for customers who use the DISA CAP to connect to Azure Government.\n \n ## Environment configuration\n \n ### Assumptions\n \n-The customer has deployed an ASE with an ILB and has implemented an ExpressRoute connection via the DISA Cloud Access Point (CAP) process.\n+The customer has deployed an ASE with an ILB and has implemented an ExpressRoute connection to the DISA Cloud Access Point (CAP).\n \n ### Route table\n \n-When creating the ASE via the portal, a route table with a default route of 0.0.0.0/0 and next hop “Internet” is created.  However, the DISA BGP routes will advertise for 0.0.0.0/0 and this route table should be removed from the ASE subnet.\n+When creating the ASE via the portal, a route table with a default route of 0.0.0.0/0 and next hop “Internet” is created. \n+However, since DISA advertises a default route out the ExpressRoute circuit, the User Defined Route (UDR) should either be deleted, or remove the default route to internet.  \n \n-### Network security group (NSG)\n-\n-The ASE will be created with inbound and outbound security rules as shown below.  The inbound security rules MUST allow ports 454-455 with an ephemeral source port range (*).  Source IPs must include the following Azure Government ranges see [App Service Environment management addresses](https://docs.microsoft.com/azure/app-service/environment/management-addresses\n+You will need to create new routes in the UDR for the management addresses in order to keep the ASE healthy. For Azure Government ranges see [App Service Environment management addresses](https://docs.microsoft.com/azure/app-service/environment/management-addresses\n )\n \n-* 23.97.29.209\n-* 23.97.0.17\n-* 23.97.16.184\n-* 13.72.180.105\n-* 13.72.53.37\n+Rule 1: 23.97.29.209 --> Internet\n+Rule 2: 23.97.0.17 --> Internet \n+Rule 3: 23.97.16.184 --> Internet \n+Rule 4: 13.72.180.105 --> Internet\n+Rule 5: 13.72.53.37 --> Internet\n+\n+Make sure the UDR is applied to the subnet your ASE is deployed to. \n+\n+### Network security group (NSG)\n \n-#### Default NSG security rules\n+The ASE will be created with inbound and outbound security rules as shown below.  The inbound security rules MUST allow ports 454-455 with an ephemeral source port range (*).\n \n The images below describe the default NSG rules created during the ASE creation.  For more information, see [Networking considerations for an App Service Environment](https://docs.microsoft.com/azure/app-service/environment/network-info#network-security-groups)\n \n ![Default inbound NSG security rules for an ILB ASE](media/documentation-government-ase-disacap-inbound-route-table.png)\n \n ![Default outbound NSG security rules for an ILB ASE](media/documentation-government-ase-disacap-outbound-route-table.png)\n \n+### Service Endpoints \n+\n+Depending what storage you are using you will be required to enable Service Endpoints for SQL and Azure Storage to access them without going back down to the DISA BCAP. You also need to enable EventHub Service Endpoint for ASE logs. \n+\n ## FAQs\n \n-* Some configuration changes may take some time to take effect.  Allow for several hours for changes to routing, NSGs, ASE Health, etc. to propagate and take effect.\n+* Some configuration changes may take some time to take effect.  Allow for several hours for changes to routing, NSGs, ASE Health, etc. to propagate and take effect, or optionally you can reboot the ASE. \n \n ## Resource manager template sample\n \n > [!NOTE]\n-   > The Azure Portal will not allow the ASE to be configured with non-RFC 1918 IP addresses.  If your solution requires non-RFC 1918 IP addresses, you must use a Resource Manager Template to deploy the ASE.\n+   >In order to deploy non-RFC 1918 IP addresses in the portal you must pre-stage the VNet and Subnet for the ASE. You can use a Resource Manager Template to deploy the ASE with non-RFC1918 IPs as well.\n    \n <a href=\"https://portal.azure.us/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2FAzure%2Fazure-quickstart-templates%2Fmaster%2FApp-Service-Environment-AzFirewall%2Fazuredeploy.json\" target=\"_blank\">\n+\n <img src=\"https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/1-CONTRIBUTION-GUIDE/images/deploytoazuregov.png\"/>\n </a>\n \n-This template deploys an **ILB ASE** into the Azure Government DoD regions.\n+This template deploys an **ILB ASE** into the Azure Government or Azure DoD regions.\n \n ## Next steps\n [Azure Government overview](documentation-government-welcome.md)"
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/azure-monitor/app/nodejs.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -12,7 +12,7 @@ ms.date: 03/14/2019\n \n To receive, store, and explore your monitoring data, include the SDK in your code, and then set up a corresponding Application Insights resource in Azure. The SDK sends data to that resource for further analysis and exploration.\n \n-The Node.js SDK can automatically monitor incoming and outgoing HTTP requests, exceptions, and some system metrics. Beginning in version 0.20, the SDK also can monitor some common third-party packages, like MongoDB, MySQL, and Redis. All events related to an incoming HTTP request are correlated for faster troubleshooting.\n+The Node.js SDK can automatically monitor incoming and outgoing HTTP requests, exceptions, and some system metrics. Beginning in version 0.20, the SDK also can monitor some common [third-party packages](https://github.com/microsoft/node-diagnostic-channel/tree/master/src/diagnostic-channel-publishers#currently-supported-modules), like MongoDB, MySQL, and Redis. All events related to an incoming HTTP request are correlated for faster troubleshooting.\n \n You can use the TelemetryClient API to manually instrument and monitor additional aspects of your app and system. We describe the TelemetryClient API in more detail later in this article.\n "
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/azure-monitor/app/sdk-connection-string.md",
    "Addition": 7,
    "Delections": 7,
    "Changes": 14,
    "Patch": "@@ -147,7 +147,7 @@ In this example, this connection string specifies explicit overrides for every s\n \n Connection Strings are supported in the following SDK versions:\n - .NET and .NET Core v2.12.0\n-- Java v2.5.1\n+- Java v2.5.1 and Java 3.0\n - Javascript v2.3.0\n - NodeJS v1.5.0\n - Python v1.0.0\n@@ -160,7 +160,7 @@ A connection string can be set by either in code, environment variable, or confi\n \n - Connection String: `APPLICATIONINSIGHTS_CONNECTION_STRING`\n \n-### .Net SDK example\n+# [.NET/.NetCore](#tab/net)\n \n TelemetryConfiguration.ConnectionString: https://github.com/microsoft/ApplicationInsights-dotnet/blob/add45ceed35a817dc7202ec07d3df1672d1f610d/BASE/src/Microsoft.ApplicationInsights/Extensibility/TelemetryConfiguration.cs#L271-L274\n \n@@ -193,10 +193,10 @@ NetCore config.json:\n ```\n \n \n-### Java SDK example\n+# [Java](#tab/java)\n \n \n-Java Explicitly Set:\n+Java (v2.5.x) Explicitly Set:\n ```java\n TelemetryConfiguration.getActive().setConnectionString(\"InstrumentationKey=00000000-0000-0000-0000-000000000000\");\n ```\n@@ -209,7 +209,7 @@ ApplicationInsights.xml\n </ApplicationInsights>\n ```\n \n-### Javascript SDK example\n+# [JavaScript](#tab/js)\n \n Important: Javascript doesn't support the use of Environment Variables.\n \n@@ -238,15 +238,15 @@ appInsights.loadAppInsights();\n appInsights.trackPageView();\n ```\n \n-### Node SDK example\n+# [Node.js](#tab/nodejs)\n \n ```javascript\n const appInsights = require(\"applicationinsights\");\n appInsights.setup(\"InstrumentationKey=00000000-0000-0000-0000-000000000000;\");\n appInsights.start();\n ```\n \n-### Python SDK example\n+# [Python](#tab/python)\n \n We recommend users set the environment variable.\n "
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/azure-monitor/log-query/query-optimization.md",
    "Addition": 16,
    "Delections": 1,
    "Changes": 17,
    "Patch": "@@ -5,7 +5,7 @@ ms.subservice: logs\n ms.topic: conceptual\n author: bwren\n ms.author: bwren\n-ms.date: 02/28/2019\n+ms.date: 03/30/2019\n \n ---\n \n@@ -152,6 +152,21 @@ Heartbeat\n > [!NOTE]\n > This indicator presents only CPU from the immediate cluster. In multi-region query, it would represent only one of the regions. In multi-workspace query, it might not include all workspaces.\n \n+### Avoid full XML and JSON parsing when string parsing works\n+Full parsing of an XML or JSON object may consume high CPU and memory resources. In many cases, when only one or two parameters are needed and the XML or JSON objects are simple, it is easier to parse them as strings using the [parse operator](/azure/kusto/query/parseoperator) or other [text parsing techniques](/azure/azure-monitor/log-query/parse-text). The performance boost will be more significant as the number of records in the XML or JSON object increases. It is essential when the number of records reaches tens of millions.\n+\n+For example, the following query will return exactly the same results as the queries above without performing full XML parsing. Note that it makes some assumptions on the XML file structure such as that FilePath element comes after FileHash and none of them has attributes. \n+\n+```Kusto\n+//even more efficient\n+SecurityEvent\n+| where EventID == 8002 //Only this event have FileHash\n+| where EventData !has \"%SYSTEM32\" //Early removal of unwanted records\n+| parse EventData with * \"<FilePath>\" FilePath \"</FilePath>\" * \"<FileHash>\" FileHash \"</FileHash>\" *\n+| summarize count() by FileHash, FilePath\n+| where FileHash != \"\" // No need to filter out %SYSTEM32 here as it was removed before\n+```\n+\n \n ## Data used for processed query\n "
  },
  {
    "Number": 109563,
    "Title": "3/30 PM Publish",
    "ClosedAt": "2020-03-30T22:01:22Z",
    "User": "huypub",
    "FileName": "articles/azure-monitor/monitor-reference.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -194,7 +194,7 @@ Other solutions are available for monitoring different applications and services\n |:---|:---|\n | [Active Directory health check](insights/ad-assessment.md) | Assess the risk and health of your Active Directory environments. |\n | [Active Directory replication status](insights/ad-replication-status.md) | Regularly monitors your Active Directory environment for any replication failures. |\n-| [Activity log analytics](platform/activity-log-view.md#activity-logs-analytics-monitoring-solution) | Analyze Activity log entries using predefined log queries and views. |\n+| [Activity log analytics](platform/activity-log-view.md#azure-portal) | View Activity Log entries. |\n | [DNS Analytics (preview)](insights/dns-analytics.md) | Collects, analyzes, and correlates Windows DNS analytic and audit logs and other related data from your DNS servers. |\n | [Cloud Foundry](../cloudfoundry/cloudfoundry-oms-nozzle.md) | Collect, view, and analyze your Cloud Foundry system health and performance metrics, across multiple deployments. |\n | [Containers](insights/containers.md) | View and manage Docker and Windows container hosts. |"
  },
  {
    "Number": 109545,
    "Title": "Azure Monitor update cost storage",
    "ClosedAt": "2020-03-30T21:24:02Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/platform/manage-cost-storage.md",
    "Addition": 14,
    "Delections": 16,
    "Changes": 30,
    "Patch": "@@ -11,18 +11,15 @@ ms.service: azure-monitor\n ms.workload: na\n ms.tgt_pltfrm: na\n ms.topic: conceptual\n-ms.date: 03/16/2020\n+ms.date: 03/30/2020\n ms.author: bwren\n ms.subservice: \n ---\n  \n # Manage usage and costs with Azure Monitor Logs\n \n > [!NOTE]\n-> This article describes how to understand and control your costs for Azure Monitor Logs. A related article, [Monitoring usage and estimated costs](https://docs.microsoft.com/azure/azure-monitor/platform/usage-estimated-costs) describes how to view usage and estimated costs across multiple Azure monitoring features for different pricing models.\n-\n-> [!NOTE]\n-> All prices and costs shown in this article are for example purposes only. \n+> This article describes how to understand and control your costs for Azure Monitor Logs. A related article, [Monitoring usage and estimated costs](https://docs.microsoft.com/azure/azure-monitor/platform/usage-estimated-costs) describes how to view usage and estimated costs across multiple Azure monitoring features for different pricing models. All prices and costs shown in this article are for example purposes only. \n \n Azure Monitor Logs is designed to scale and support collecting, indexing, and storing massive amounts of data per day from any source in your enterprise or deployed in Azure.  While this may be a primary driver for your organization, cost-efficiency is ultimately the underlying driver. To that end, it's important to understand that the cost of a Log Analytics workspace isn't based only on the volume of data collected, it is also dependent on the plan selected, and how long you chose to store data generated from your connected sources.  \n \n@@ -85,7 +82,9 @@ You can also [set the pricing tier via Azure Resource Manager](https://docs.micr\n \n ## Legacy pricing tiers\n \n-Subscriptions who had a Log Analytics workspace or Application Insights resource in it before April 2, 2018, or are linked to an Enterprise Agreement that started prior to February 1, 2019, will continue to have access to use the legacy pricing tiers: **Free**, **Standalone (Per GB)** and **Per Node (OMS)**.  Workspaces in the Free pricing tier will have daily data ingestion limited to 500 MB (except for security data types collected by Azure Security Center) and the data retention is limited to 7 days. The Free pricing tier is intended only for evaluation purposes. Workspaces in the Standalone or Per Node pricing tiers have user-configurable retention of up to 2 years. \n+Subscriptions who had a Log Analytics workspace or Application Insights resource in it before April 2, 2018, or are linked to an Enterprise Agreement that started prior to February 1, 2019, will continue to have access to use the legacy pricing tiers: **Free**, **Standalone (Per GB)** and **Per Node (OMS)**.  Workspaces in the Free pricing tier will have daily data ingestion limited to 500 MB (except for security data types collected by Azure Security Center) and the data retention is limited to 7 days. The Free pricing tier is intended only for evaluation purposes. Workspaces in the Standalone or Per Node pricing tiers have user-configurable retention from 30 to 730 days.\n+\n+The Per Node pricing tier charges per monitored VM (node) on an hour granularity. For each monitored node, the workspace is allocated 500 MB of data per day that is not billed. This allocation is aggregated at the workspace level. Data ingested above the aggregate daily data allocation is billed per GB as data overage. Note that on your bill, the service will be **Insight and Analytics** for Log Analytics usage if the workspace is in the Per Node pricing tier. \n \n Workspaces created prior to April 2016 can also access the original **Standard** and **Premium** pricing tiers which have fixed data retention of 30 and 365 days respectively. New workspaces cannot be created in the **Standard** or **Premium** pricing tiers, and if a workspace is moved out of these tiers, it cannot be moved back. \n \n@@ -96,7 +95,7 @@ More details of pricing tier limitations are available [here](https://docs.micro\n \n ## Change the data retention period\n \n-The following steps describe how to configure how long log data is kept by in your workspace.\n+The following steps describe how to configure how long log data is kept by in your workspace. Data retention can be configured from 30 to 730 days (2 years) for all workspaces unless they are using the legacy Free pricing tier. \n \n ### Default retention\n \n@@ -114,7 +113,7 @@ Two data types -- `Usage` and `AzureActivity` -- are retained for 90 days by def\n \n ### Retention by data type\n \n-It is also possible to specify different retention settings for individual data types. Each data type is a sub-resource of the workspace. For instance the SecurityEvent table can be addressed in [Azure Resource Manager](https://docs.microsoft.com/azure/azure-resource-manager/resource-group-overview) as:\n+It is also possible to specify different retention settings for individual data types from 30 to 730 days (except for workspaces in the legacy Free pricing tier). Each data type is a sub-resource of the workspace. For instance the SecurityEvent table can be addressed in [Azure Resource Manager](https://docs.microsoft.com/azure/azure-resource-manager/resource-group-overview) as:\n \n ```\n /subscriptions/00000000-0000-0000-0000-00000000000/resourceGroups/MyResourceGroupName/providers/Microsoft.OperationalInsights/workspaces/MyWorkspaceName/Tables/SecurityEvent\n@@ -144,6 +143,8 @@ To set the retention of a particular data type (in this example SecurityEvent) t\n     }\n ```\n \n+Valid values for `retentionInDays` are from 30 through 730.\n+\n The `Usage` and `AzureActivity` data types cannot be set with custom retention. They will take on the maximum of the default workspace retention or 90 days. \n \n A great tool to connect directly to Azure Resource Manager to set retention by data type is the OSS tool [ARMclient](https://github.com/projectkudu/ARMClient).  Learn more about ARMclient from articles by [David Ebbo](http://blog.davidebbo.com/2015/01/azure-resource-manager-client.html) and [Daniel Bowbyes](https://blog.bowbyes.co.nz/2016/11/02/using-armclient-to-directly-access-azure-arm-rest-apis-and-list-arm-policy-details/).  Here's an example using ARMClient, setting SecurityEvent data to a 730 day retention:\n@@ -152,21 +153,18 @@ A great tool to connect directly to Azure Resource Manager to set retention by d\n armclient PUT /subscriptions/00000000-0000-0000-0000-00000000000/resourceGroups/MyResourceGroupName/providers/Microsoft.OperationalInsights/workspaces/MyWorkspaceName/Tables/SecurityEvent?api-version=2017-04-26-preview \"{properties: {retentionInDays: 730}}\"\n ```\n \n-> [!NOTE]\n+> [!TIP]\n > Setting retention on individual data types can be used to reduce your costs for data retention.  For data collected starting in October 2019 (when this feature was released), reducing the retention for some data types can  reduce your  retention cost over time.  For data collected earlier, setting a lower retention for an individual type will not affect your retention costs.  \n \n ## Manage your maximum daily data volume\n \n You can configure a daily cap and limit the daily ingestion for your workspace, but use care as your goal should not be to hit the daily limit.  Otherwise, you lose data for the remainder of the day, which can impact other Azure services and solutions whose functionality may depend on up-to-date data being available in the workspace.  As a result, your ability to observe and receive alerts when the health conditions of resources supporting IT services are impacted.  The daily cap is intended to be used as a way to manage the unexpected increase in data volume from your managed resources and stay within your limit, or when you want to limit unplanned charges for your workspace.  \n \n-When the daily limit is reached, the collection of billable data types stops for the rest of the day. A warning banner appears across the top of the page for the selected Log Analytics workspace and an operation event is sent to the *Operation* table under **LogManagement** category. Data collection resumes after the reset time defined under *Daily limit will be set at*. We recommend defining an alert rule based on this operation event, configured to notify when the daily data limit has been reached. \n+Soon after the daily limit is reached, the collection of billable data types stops for the rest of the day. (Latency inherent in applying the daily cap can mean that the cap is not applied as precisely the specified daily cap level.) A warning banner appears across the top of the page for the selected Log Analytics workspace and an operation event is sent to the *Operation* table under **LogManagement** category. Data collection resumes after the reset time defined under *Daily limit will be set at*. We recommend defining an alert rule based on this operation event, configured to notify when the daily data limit has been reached. \n \n-> [!NOTE]\n+> [!WARNING]\n > The daily cap does not stop the collection of data from Azure Security Center, except for workspaces in which Azure Security Center was installed before June 19, 2017. \n \n-> [!NOTE]\n-> Latency inherent in applying the daily cap can mean that the cap is not applied as precisely the specified daily cap level. \n-\n ### Identify what daily data limit to define\n \n Review [Log Analytics Usage and estimated costs](usage-estimated-costs.md) to understand the data ingestion trend and what is the daily volume cap to define. It should be considered with care, since you won�t be able to monitor your resources after the limit is reached. \n@@ -237,7 +235,7 @@ union withsource = tt *\n | summarize TotalVolumeBytes=sum(_BilledSize) by computerName\n ```\n \n-> [!NOTE]\n+> [!TIP]\n > Use these `union withsource = tt *` queries sparingly as scans across data types are expensive to execute. This query replaces the old way of querying per-computer information with the Usage data type.  \n \n ## Understanding ingested data volume\n@@ -343,7 +341,7 @@ union withsource = tt *\n \n Changing `subscriptionId` to `resourceGroup` will show the billable ingested data volume by Azure resource group. \n \n-> [!NOTE]\n+> [!WARNING]\n > Some of the fields of the Usage data type, while still in the schema, have been deprecated and will their values are no longer populated. \n > These are **Computer** as well as fields related to ingestion (**TotalBatches**, **BatchesWithinSla**, **BatchesOutsideSla**, **BatchesCapped** and **AverageProcessingTimeMs**.\n "
  },
  {
    "Number": 109517,
    "Title": "Azure Monitor query optimization update",
    "ClosedAt": "2020-03-30T19:54:10Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/log-query/query-optimization.md",
    "Addition": 16,
    "Delections": 1,
    "Changes": 17,
    "Patch": "@@ -5,7 +5,7 @@ ms.subservice: logs\n ms.topic: conceptual\n author: bwren\n ms.author: bwren\n-ms.date: 02/28/2019\n+ms.date: 03/30/2019\n \n ---\n \n@@ -152,6 +152,21 @@ Heartbeat\n > [!NOTE]\n > This indicator presents only CPU from the immediate cluster. In multi-region query, it would represent only one of the regions. In multi-workspace query, it might not include all workspaces.\n \n+### Avoid full XML and JSON parsing when string parsing works\n+Full parsing of an XML or JSON object may consume high CPU and memory resources. In many cases, when only one or two parameters are needed and the XML or JSON objects are simple, it is easier to parse them as strings using the [parse operator](/azure/kusto/query/parseoperator) or other [text parsing techniques](/azure/azure-monitor/log-query/parse-text). The performance boost will be more significant as the number of records in the XML or JSON object increases. It is essential when the number of records reaches tens of millions.\n+\n+For example, the following query will return exactly the same results as the queries above without performing full XML parsing. Note that it makes some assumptions on the XML file structure such as that FilePath element comes after FileHash and none of them has attributes. \n+\n+```Kusto\n+//even more efficient\n+SecurityEvent\n+| where EventID == 8002 //Only this event have FileHash\n+| where EventData !has \"%SYSTEM32\" //Early removal of unwanted records\n+| parse EventData with * \"<FilePath>\" FilePath \"</FilePath>\" * \"<FileHash>\" FileHash \"</FileHash>\" *\n+| summarize count() by FileHash, FilePath\n+| where FileHash != \"\" // No need to filter out %SYSTEM32 here as it was removed before\n+```\n+\n \n ## Data used for processed query\n "
  },
  {
    "Number": 108473,
    "Title": "Azure Monitor activity log transition",
    "ClosedAt": "2020-03-30T16:14:49Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/platform/activity-log-collect.md",
    "Addition": 169,
    "Delections": 29,
    "Changes": 198,
    "Patch": "@@ -1,54 +1,110 @@\n ---\n-title: Collect Azure activity log in Log Analytics workspace\n+title: Collect and analyze Azure activity log in Azure Monitor\n description: Collect the Azure Activity Log in Azure Monitor Logs and use the monitoring solution to analyze and search the Azure activity log across all your Azure subscriptions.\n ms.subservice: logs\n ms.topic: conceptual\n author: bwren\n ms.author: bwren\n-ms.date: 09/30/2019\n+ms.date: 03/24/2020\n \n ---\n \n-# Collect and analyze Azure activity logs in Log Analytics workspace in Azure Monitor\n+# Collect and analyze Azure Activity log in Azure Monitor\n+The [Azure Activity log](platform-logs-overview.md) is a [platform log](platform-logs-overview.md) that provides insight into subscription-level events that have occurred in Azure. While you can view the Activity log in the Azure portal, you should configure it to send to a Log Analytics workspace to enable additional features of Azure Monitor. This article describes how to perform this configuration and how to send the Activity log to Azure storage and event hubs.\n \n-> [!WARNING]\n-> You can now collect the Activity log into a Log Analytics workspace using a diagnostic setting similar to how you collect resource logs. See [Collect and analyze Azure activity logs in Log Analytics workspace in Azure Monitor](diagnostic-settings-legacy.md).\n+Collecting the Activity Log in a Log Analytics workspace provides the following advantages:\n \n-The [Azure Activity Log](platform-logs-overview.md) provides insight into subscription-level events that have occurred in your Azure subscription. This article describes how to collect the Activity Log into a Log Analytics workspace and how to use the Activity Log Analytics [monitoring solution](../insights/solutions.md), which provides log queries and views for analyzing this data. \n+- No data ingestion or data retention charge for Activity log data stored in a Log Analytics workspace.\n+- Correlate Activity log data with other monitoring data collected by Azure Monitor.\n+- Use log queries to perform complex analysis and gain deep insights on Activity Log entries.\n+- Use log alerts with Activity entries allowing for more complex alerting logic.\n+- Store Activity log entries for longer than 90 days.\n+- Consolidate log entries from multiple Azure subscriptions and tenants into one location for analysis together.\n \n-Connecting the Activity Log to a Log Analytics workspace provides the following benefits:\n \n-- Consolidate the Activity Log from multiple Azure subscriptions into one location for analysis.\n-- Store Activity Log entries for longer than 90 days.\n-- Correlate Activity Log data with other monitoring data collected by Azure Monitor.\n-- Use [log queries](../log-query/log-query-overview.md) to perform complex analysis and gain deep insights on Activity Log entries.\n \n-## Connect to Log Analytics workspace\n-A single workspace can be connected to the Activity Log for multiple subscriptions in the same Azure tenant. For collection across multiple tenants, see [Collect Azure Activity Logs into a Log Analytics workspace across subscriptions in different Azure Active Directory tenants](activity-log-collect-tenants.md).\n+## Collecting Activity log\n+The Activity log is collected automatically for [viewing in the Azure portal](activity-log-view.md). To collect it in a Log Analytics workspace or to send it Azure storage or event hubs, create a [diagnostic setting](diagnostic-settings.md). This is the same method used by resource logs making it consistent for all [platform logs](platform-logs-overview.md).  \n+\n+To create a diagnostic setting for the Activity log, select **Diagnostic settings** from the **Activity log** menu in Azure Monitor. See [Create diagnostic setting to collect platform logs and metrics in Azure](diagnostic-settings.md) for details on creating the setting. See [Categories in the Activity log](activity-log-view.md#categories-in-the-activity-log) for a description of the categories you can filter. If you have any legacy settings, make sure you disable them before creating a diagnostic setting. Having both enabled may result in duplicate data.\n+\n+![Diagnostic settings](media/diagnostic-settings-subscription/diagnostic-settings.png)\n+\n+\n+> [!NOTE]\n+> Currently, you can only create a subscription level diagnostic setting using the Azure portal and a Resource Manager template. \n+\n+\n+## Legacy settings \n+While diagnostic settings are the preferred method to send the Activity log to different destinations, legacy methods will continue to work if you don't choose to replace with a diagnostic setting. Diagnostic settings have the following advantages over legacy methods, and it's recommended that you update your configuration:\n+\n+- Consistent method for collecting all platform logs.\n+- Collect Activity log across multiple subscriptions and tenants.\n+- Filter collection to only collect logs for particular categories.\n+- Collect all Activity log categories. Some categories are not collected using legacy method.\n+- Faster latency for log ingestion. The previous method has about 15 minutes latency while diagnostic settings adds only about 1 minute.\n \n-> [!IMPORTANT]\n-> You may receive an error with the following procedure if the Microsoft.OperationalInsights and Microsoft.OperationsManagement resource providers aren't registered for your subscription. See [Azure resource providers and types](../../azure-resource-manager/management/resource-providers-and-types.md) to register these providers.\n \n-Use the following procedure to connect the Activity Log to your Log Analytics workspace:\n+\n+### Log profiles\n+Log profiles are the legacy method for sending the Activity log to Azure storage or event hubs. Use the following procedure to continue working with a log profile or to disable it in preparation for migrating to a diagnostic setting.\n+\n+1. From the **Azure Monitor** menu in the Azure portal, select **Activity log**.\n+3. Click **Diagnostic settings**.\n+\n+   ![Diagnostic settings](media/diagnostic-settings-subscription/diagnostic-settings.png)\n+\n+4. Click the purple banner for the legacy experience.\n+\n+    ![Legacy experience](media/diagnostic-settings-subscription/legacy-experience.png)\n+\n+### Log Analytics workspace\n+The legacy method for collecting the Activity log into a Log Analytics workspace is connecting the log in the workspace configuration. \n \n 1. From the **Log Analytics workspaces** menu in the Azure portal, select the workspace to collect the Activity Log.\n 1. In the **Workspace Data Sources** section of the workspace's menu, select **Azure Activity log**.\n 1. Click the subscription you want to connect.\n \n-    ![Workspaces](media/activity-log-export/workspaces.png)\n+    ![Workspaces](media/activity-log-collect/workspaces.png)\n \n 1. Click **Connect** to connect the Activity log in the subscription to the selected workspace. If the subscription is already connected to another workspace, click **Disconnect** first to disconnect it.\n \n-    ![Connect Workspaces](media/activity-log-export/connect-workspace.png)\n+    ![Connect Workspaces](media/activity-log-collect/connect-workspace.png)\n+\n+\n+To disable the setting, perform the same procedure and click **Disconnect** to remove the subscription from the workspace.\n+\n+\n+## Analyze Activity log in Log Analytics workspace\n+When you connect an Activity Log to a Log Analytics workspace, entries will be written to the workspace into a table called *AzureActivity* that you can retrieve with a [log query](../log-query/log-query-overview.md). The structure of this table varies depending on the [category of the log entry](activity-log-view.md#categories-in-the-activity-log). See [Azure Activity Log event schema](activity-log-schema.md) for a description of each category.\n+\n+\n+### Data structure changes\n+Diagnostic settings collect the same data as the legacy method used to collect the Activity log with some changes to the structure of the *AzureActivity* table.\n+\n+The columns in the following table have been deprecated in the updated schema. They still exist in *AzureActivity* but they will have no data. The replacement for these columns are not new, but they contain the same data as the deprecated column. They are in a different format, so you may need to modify log queries that use them. \n+\n+| Deprecated column | Replacement column |\n+|:---|:---|\n+| ActivityStatus    | ActivityStatusValue    |\n+| ActivitySubstatus | ActivitySubstatusValue |\n+| OperationName     | OperationNameValue     |\n+| ResourceProvider  | ResourceProviderValue  |\n+\n+> [!IMPORTANT]\n+> In some cases, the values in these columns may be in all uppercase. If you have a query that includes these columns, you should use the [=~ operator](https://docs.microsoft.com/azure/kusto/query/datatypes-string-operators) to do a case insensitive comparison.\n+\n+The following column have been added to *AzureActivity* in the updated schema:\n+\n+- Authorization_d\n+- Claims_d\n+- Properties_d\n \n-## Analyze in Log Analytics workspace\n-When you connect an Activity Log to a Log Analytics workspace, entries will be written to the workspace into a table called **AzureActivity** that you can retrieve with a [log query](../log-query/log-query-overview.md). The structure of this table varies depending on the [category of log entry](activity-log-view.md#categories-in-the-activity-log). See [Azure Activity Log event schema](activity-log-schema.md) for a description of each category.\n \n ## Activity Logs Analytics monitoring solution\n-The Azure Log Analytics monitoring solution includes multiple log queries and views for analyzing the Activity Log records in your Log Analytics workspace.\n+The Azure Log Analytics monitoring solution will be deprecated soon and replaced by a workbook using the updated schema in the Log Analytics workspace. You can still use the solution if you already have it enabled, but it can only be used if you're collecting the Activity log using legacy settings. \n+\n \n-### Install the solution\n-Use the procedure in [Install a monitoring solution](../insights/solutions.md#install-a-monitoring-solution) to install the **Activity Log Analytics** solution. There is no additional configuration required.\n \n ### Use the solution\n Monitoring solutions are accessed from the **Monitor** menu in the Azure portal. Select **More** in the **Insights** section to open the **Overview** page with the solution tiles. The **Azure Activity Logs** tile displays a count of the number of **AzureActivity** records in your workspace.\n@@ -60,12 +116,96 @@ Click the **Azure Activity Logs** tile to open the **Azure Activity Logs** view.\n \n ![Azure Activity Logs dashboard](media/collect-activity-logs/activity-log-dash.png)\n \n-| Visualization part | Description |\n-| --- | --- |\n-| Azure Activity Log Entries | Shows a bar chart of the top Azure Activity Log entry record totals for the date range that you have selected and shows a list of the top 10 activity callers. Click the bar chart to run a log search for `AzureActivity`. Click a caller item to run a log search returning all Activity Log entries for that item. |\n-| Activity Logs by Status | Shows a doughnut chart for Azure Activity Log status for the selected date range and a list of the top ten status records. Click the chart to run a log query for `AzureActivity | summarize AggregatedValue = count() by ActivityStatus`. Click a status item to run a log search returning all Activity Log entries for that status record. |\n-| Activity Logs by Resource | Shows the total number of resources with Activity Logs and lists the top ten resources with record counts for each resource. Click the total area to run a log search for `AzureActivity | summarize AggregatedValue = count() by Resource`, which shows all Azure resources available to the solution. Click a resource to run a log query returning all activity records for that resource. |\n-| Activity Logs by Resource Provider | Shows the total number of resource providers that produce Activity Logs and lists the top ten. Click the total area to run a log query for `AzureActivity | summarize AggregatedValue = count() by ResourceProvider`, which shows all Azure resource providers. Click a resource provider to run a log query returning all activity records for the provider. |\n+\n+### Enable the solution for new subscriptions\n+You will soon no longer be able to add the Activity Logs Analytics solution to your subscription using the Azure portal. You can add it using the following procedure with a resource manager template. \n+\n+1. Copy the following json into a file called *ActivityLogTemplate*.json.\n+\n+    ```json\n+    {\n+    \"$schema\": \"https://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json#\",\n+    \"contentVersion\": \"1.0.0.0\",\n+    \"parameters\": {\n+        \"workspaceName\": {\n+            \"type\": \"String\",\n+            \"defaultValue\": \"my-workspace\",\n+            \"metadata\": {\n+              \"description\": \"Specifies the name of the workspace.\"\n+            }\n+        },\n+        \"location\": {\n+            \"type\": \"String\",\n+            \"allowedValues\": [\n+              \"east us\",\n+              \"west us\",\n+              \"australia central\",\n+              \"west europe\"\n+            ],\n+            \"defaultValue\": \"australia central\",\n+            \"metadata\": {\n+              \"description\": \"Specifies the location in which to create the workspace.\"\n+            }\n+        }\n+      },\n+        \"resources\": [\n+        {\n+            \"type\": \"Microsoft.OperationalInsights/workspaces\",\n+            \"name\": \"[parameters('workspaceName')]\",\n+            \"apiVersion\": \"2015-11-01-preview\",\n+            \"location\": \"[parameters('location')]\",\n+            \"properties\": {\n+                \"features\": {\n+                    \"searchVersion\": 2\n+                }\n+            }\n+        },\n+        {\n+            \"type\": \"Microsoft.OperationsManagement/solutions\",\n+            \"apiVersion\": \"2015-11-01-preview\",\n+            \"name\": \"[concat('AzureActivity(', parameters('workspaceName'),')')]\",\n+            \"location\": \"[parameters('location')]\",\n+            \"dependsOn\": [\n+                \"[resourceId('microsoft.operationalinsights/workspaces', parameters('workspaceName'))]\"\n+            ],\n+            \"plan\": {\n+                \"name\": \"[concat('AzureActivity(', parameters('workspaceName'),')')]\",\n+                \"promotionCode\": \"\",\n+                \"product\": \"OMSGallery/AzureActivity\",\n+                \"publisher\": \"Microsoft\"\n+            },\n+            \"properties\": {\n+                \"workspaceResourceId\": \"[resourceId('microsoft.operationalinsights/workspaces', parameters('workspaceName'))]\",\n+                \"containedResources\": [\n+                    \"[concat(resourceId('microsoft.operationalinsights/workspaces', parameters('workspaceName')), '/views/AzureActivity(',parameters('workspaceName'))]\"\n+                ]\n+            }\n+        },\n+        {\n+          \"type\": \"Microsoft.OperationalInsights/workspaces/datasources\",\n+          \"kind\": \"AzureActivityLog\",\n+          \"name\": \"[concat(parameters('workspaceName'), '/', subscription().subscriptionId)]\",\n+          \"apiVersion\": \"2015-11-01-preview\",\n+          \"location\": \"[parameters('location')]\",\n+          \"dependsOn\": [\n+              \"[parameters('WorkspaceName')]\"\n+          ],\n+          \"properties\": {\n+              \"linkedResourceId\": \"[concat(subscription().Id, '/providers/microsoft.insights/eventTypes/management')]\"\n+          }\n+        }\n+      ]\n+    }    \n+    ```\n+\n+2. Deploy the template using the following PowerShell commands:\n+\n+    ```PowerShell\n+    Connect-AzAccount\n+    Select-AzSubscription <SubscriptionName>\n+    New-AzResourceGroupDeployment -Name activitysolution -ResourceGroupName <ResourceGroup> -TemplateFile <Path to template file>\n+    ```\n+\n \n ## Next steps\n "
  },
  {
    "Number": 108473,
    "Title": "Azure Monitor activity log transition",
    "ClosedAt": "2020-03-30T16:14:49Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/platform/activity-log-export.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -13,7 +13,7 @@ ms.subservice: logs\n # Export Azure Activity log to storage or Azure Event Hubs\n \n > [!IMPORTANT]\n-> The method for sending the Azure Activity log to Azure Storage and Azure Event Hubs has changed to [diagnostic settings](diagnostic-settings.md). This article describes the legacy method which is in the process of being deprecated. See Update to [Azure Activity log collection and export](diagnostic-settings-legacy.md) for a comparison.\n+> The method for sending the Azure Activity log to Azure Storage and Azure Event Hubs has changed to [diagnostic settings](diagnostic-settings.md). This article describes the legacy method which is in the process of being deprecated. See Update to [Collect and analyze Azure Activity log in Azure Monitor](activity-log-collect.md) for a comparison.\n \n \n The [Azure Activity Log](platform-logs-overview.md) provides insight into subscription-level events that have occurred in your Azure subscription. In addition to viewing the Activity log in the Azure portal or copying it to a Log Analytics workspace where it can be analyzed with other data collected by Azure Monitor, you can create a log profile to archive the Activity log to an Azure storage account or stream it to an Event Hub."
  },
  {
    "Number": 108473,
    "Title": "Azure Monitor activity log transition",
    "ClosedAt": "2020-03-30T16:14:49Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/platform/activity-log-view.md",
    "Addition": 0,
    "Delections": 29,
    "Changes": 29,
    "Patch": "@@ -165,35 +165,6 @@ GET https://management.azure.com/subscriptions/089bd33f-d4ec-47fe-8ba5-0753aa5c5\n ```\n \n \n-## Activity Logs Analytics monitoring solution\n-The Azure Log Analytics monitoring solution includes multiple log queries and views for analyzing the Activity Log records in your Log Analytics workspace.\n-\n-### Prerequisites\n-You must create a diagnostic setting to send the Activity log for your subscription to a Log Analytics workspace. See [Collect Azure platform logs in Log Analytics workspace in Azure Monitor](resource-logs-collect-workspace.md).\n-\n-### Install the solution\n-Use the procedure in [Install a monitoring solution](../insights/solutions.md#install-a-monitoring-solution) to install the **Activity Log Analytics** solution. There is no additional configuration required.\n-\n-### Use the solution\n-Click **Logs** at the top of the **Activity Log** page to open the [Activity Log Analytics monitoring solution](activity-log-collect.md) for the subscription. Or access all the monitoring solutions in your subscription **Monitor** menu in the Azure portal. Select **More** in the **Insights** section to open the **Overview** page with the solution tiles. The **Azure Activity Logs** tile displays a count of the number of **AzureActivity** records in your workspace.\n-\n-![Azure Activity Logs tile](media/collect-activity-logs/azure-activity-logs-tile.png)\n-\n-\n-Click the **Azure Activity Logs** tile to open the **Azure Activity Logs** view. The view includes the visualization parts in the following table. Each part lists up to 10 items matching that parts's criteria for the specified time range. You can run a log query that returns all  matching records by clicking **See all** at the bottom of the part.\n-\n-![Azure Activity Logs dashboard](media/collect-activity-logs/activity-log-dash.png)\n-\n-| Visualization part | Description |\n-| --- | --- |\n-| Azure Activity Log Entries | Shows a bar chart of the top Azure Activity Log entry record totals for the date range that you have selected and shows a list of the top 10 activity callers. Click the bar chart to run a log search for `AzureActivity`. Click a caller item to run a log search returning all Activity Log entries for that item. |\n-| Activity Logs by Status | Shows a doughnut chart for Azure Activity Log status for the selected date range and a list of the top ten status records. Click the chart to run a log query for `AzureActivity | summarize AggregatedValue = count() by ActivityStatus`. Click a status item to run a log search returning all Activity Log entries for that status record. |\n-| Activity Logs by Resource | Shows the total number of resources with Activity Logs and lists the top ten resources with record counts for each resource. Click the total area to run a log search for `AzureActivity | summarize AggregatedValue = count() by Resource`, which shows all Azure resources available to the solution. Click a resource to run a log query returning all activity records for that resource. |\n-| Activity Logs by Resource Provider | Shows the total number of resource providers that produce Activity Logs and lists the top ten. Click the total area to run a log query for `AzureActivity | summarize AggregatedValue = count() by ResourceProvider`, which shows all Azure resource providers. Click a resource provider to run a log query returning all activity records for the provider. |\n-\n-\n-\n-\n ## Next steps\n \n * [Read an overview of platform logs](platform-logs-overview.md)"
  },
  {
    "Number": 108473,
    "Title": "Azure Monitor activity log transition",
    "ClosedAt": "2020-03-30T16:14:49Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/platform/diagnostic-settings-legacy.md",
    "Addition": 0,
    "Delections": 98,
    "Changes": 98,
    "Patch": "@@ -1,98 +0,0 @@\n----\n-title: Collect Azure Activity log with diagnostic settings - Azure Monitor | Microsoft Docs\n-description: Use diagnostic settings to forward Azure Activity logs to Azure Monitor Logs, Azure storage, or Azure Event Hubs.\n-author: bwren\n-\n-ms.subservice: logs\n-ms.topic: conceptual\n-ms.author: bwren\n-ms.date: 02/04/2020\n-\n----\n-\n-# Update to Azure Activity log collection and export\n-The [Azure Activity log](platform-logs-overview.md) is a [platform log](platform-logs-overview.md) that provides insight into subscription-level events that have occurred in Azure. The method to send Activity log entries to [an event hub or storage account](activity-log-export.md) or to a [Log Analytics workspace](activity-log-collect.md) has changed to use [diagnostic settings](diagnostic-settings.md). This article describes the difference between the methods and how to clear legacy settings in preparation to change to diagnostic settings.\n-\n-\n-## Differences between methods\n-\n-### Advantages\n-Using diagnostic settings has the following advantages over the current methods:\n-\n-- Consistent method for collecting all platform logs.\n-- Collect Activity log across multiple subscriptions and tenants.\n-- Filter collection to only collect logs for particular categories.\n-- Collect all Activity log categories. Some categories are not collected using legacy method.\n-- Faster latency for log ingestion. The previous method has about 15 minutes latency while diagnostic settings adds only about 1 minute.\n-\n-### Considerations\n-Consider the following details of Activity log collection using diagnostic settings before enabling this feature.\n-\n-- The retention setting for collecting the Activity log to Azure storage has been removed meaning that data will be stored indefinitely until you remove it.\n-- Currently, you can only create a subscription level diagnostic setting using the Azure portal. To use other methods such as PowerShell or CLI, you can create a Resource Manager template.\n-\n-\n-### Differences in data\n-Diagnostic settings collect the same data as the previous methods used to collect the Activity log with the following current differences:\n-\n-The following columns have been removed. The replacement for these columns are in a different format, so you may need to modify log queries that use them. You may still see removed columns in the schema, but they won't be populated with data.\n-\n-| Removed column | Replacement column |\n-|:---|:---|\n-| ActivityStatus    | ActivityStatusValue    |\n-| ActivitySubstatus | ActivitySubstatusValue |\n-| OperationName     | OperationNameValue     |\n-| ResourceProvider  | ResourceProviderValue  |\n-\n-The following column have been added:\n-\n-- Authorization_d\n-- Claims_d\n-- Properties_d\n-\n-> [!IMPORTANT]\n-> In some cases, the values in these columns may be in all uppercase. If you have a query that includes these columns, you should use the [=~ operator](https://docs.microsoft.com/azure/kusto/query/datatypes-string-operators) to do a case insensitive comparison.\n-\n-## Work with legacy settings\n-Legacy settings for collecting the Activity log will continue to work if you don't choose to replace with a diagnostic setting. Use the following method to manage the log profile for a subscription.\n-\n-1. From the **Azure Monitor** menu in the Azure portal, select **Activity log**.\n-3. Click **Diagnostic settings**.\n-\n-   ![Diagnostic settings](media/diagnostic-settings-subscription/diagnostic-settings.png)\n-\n-4. Click the purple banner for the legacy experience.\n-\n-    ![Legacy experience](media/diagnostic-settings-subscription/legacy-experience.png)\n-\n-\n-See the following articles for details on using the legacy collection methods.\n-\n-- [Collect and analyze Azure activity logs in Log Analytics workspace in Azure Monitor](activity-log-collect.md)\n-- [Collect Azure Activity logs into Azure Monitor across Azure Active Directory tenants](activity-log-collect-tenants.md)\n-- [Export Azure Activity log to storage or Azure Event Hubs](activity-log-export.md)\n-\n-## Disable existing settings\n-You should disable existing collection of the Activity before enabling it using diagnostic settings. Having both enabled may result in duplicate data.\n-\n-### Disable collection into Log Analytics workspace\n-\n-1. Open the **Log Analytics workspaces** menu in the Azure portal and select the workspace to collect the Activity Log.\n-2. In the **Workspace Data Sources** section of the workspace's menu, select **Azure Activity log**.\n-3. Click the subscription you want to disconnect.\n-4. Click **Disconnect** and then **Yes** when asked to confirm your choice.\n-\n-### Disable log profile\n-\n-1. Use the procedure described in [Work with legacy settings](#work-with-legacy-settings) to open legacy settings.\n-2. Disable any current collection to storage or event hubs.\n-\n-\n-\n-## Activity Log monitoring solution\n-The Azure Log Analytics monitoring solution includes multiple log queries and views for analyzing the Activity Log records in your Log Analytics workspace. This solution uses log data collected in a Log Analytics workspace and will continue to work without any changes if you collect the Activity log using diagnostic settings. See [Activity Logs Analytics monitoring solution](activity-log-collect.md#activity-logs-analytics-monitoring-solution) for details on this solution.\n-\n-## Next steps\n-\n-* [Learn more about the Activity Log](../../azure-resource-manager/management/view-activity-logs.md)\n-* [Learn more about diagnostic settings](diagnostic-settings.md)"
  },
  {
    "Number": 109499,
    "Title": "updated JSON template",
    "ClosedAt": "2020-03-30T18:37:07Z",
    "User": "MGoedtel",
    "FileName": "articles/automation/automation-update-management-deploy-template.md",
    "Addition": 83,
    "Delections": 81,
    "Changes": 164,
    "Patch": "@@ -6,7 +6,7 @@ ms.subservice: update-management\n ms.topic: conceptual\n author: mgoedtel\n ms.author: magoedte\n-ms.date: 02/27/2020\n+ms.date: 03/30/2020\n \n ---\n \n@@ -52,6 +52,7 @@ The following parameters in the template are set with a default value for the Lo\n \n * sku - defaults to the new Per-GB pricing tier released in the April 2018 pricing model\n * data retention - defaults to thirty days\n+* capacity reservation - defaults to 100 GB\n \n >[!WARNING]\n >If creating or configuring a Log Analytics workspace in a subscription that has opted into the new April 2018 pricing model, the only valid Log Analytics pricing tier is **PerGB2018**.\n@@ -67,89 +68,89 @@ The following parameters in the template are set with a default value for the Lo\n     ```json\n     {\n     \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n-\t\"contentVersion\": \"1.0.0.0\",\n-\t\"parameters\": {\n-\t\t\"workspaceName\": {\n-\t\t\t\"type\": \"string\",\n-\t\t\t\"metadata\": {\n-\t\t\t\t\"description\": \"Workspace name\"\n-\t\t\t}\n-\t\t},\n-\t\t\"pricingTier\": {\n-\t\t\t\"type\": \"string\",\n-\t\t\t\"allowedValues\": [\n-\t\t\t\t\"pergb2018\",\n-\t\t\t\t\"Free\",\n-\t\t\t\t\"Standalone\",\n-\t\t\t\t\"PerNode\",\n-\t\t\t\t\"Standard\",\n-\t\t\t\t\"Premium\"\n-\t\t\t],\n-\t\t\t\"defaultValue\": \"pergb2018\",\n-\t\t\t\"metadata\": {\n-\t\t\t\t\"description\": \"Pricing tier: perGB2018 or legacy tiers (Free, Standalone, PerNode, Standard or Premium) which are not available to all customers.\"\n-\t\t\t}\n-\t\t},\n-\t\t\"dataRetention\": {\n-\t\t\t\"type\": \"int\",\n-\t\t\t\"defaultValue\": 30,\n-\t\t\t\"minValue\": 7,\n-\t\t\t\"maxValue\": 730,\n-\t\t\t\"metadata\": {\n-\t\t\t\t\"description\": \"Number of days of retention. Workspaces in the legacy Free pricing tier can only have 7 days.\"\n-\t\t\t}\n-\t\t},\n-\t\t\"immediatePurgeDataOn30Days\": {\n-\t\t\t\"type\": \"bool\",\n-\t\t\t\"defaultValue\": \"[bool('false')]\",\n-\t\t\t\"metadata\": {\n-\t\t\t\t\"description\": \"If set to true when changing retention to 30 days, older data will be immediately deleted. Use this with extreme caution. This only applies when retention is being set to 30 days.\"\n-\t\t\t}\n-\t\t},\n-\t\t\"location\": {\n-\t\t\t\"type\": \"string\",\n-\t\t\t\"allowedValues\": [\n-\t\t\t\t\"australiacentral\",\n-\t\t\t\t\"australiaeast\",\n-\t\t\t\t\"australiasoutheast\",\n-\t\t\t\t\"brazilsouth\",\n-\t\t\t\t\"canadacentral\",\n-\t\t\t\t\"centralindia\",\n-\t\t\t\t\"centralus\",\n-\t\t\t\t\"eastasia\",\n-\t\t\t\t\"eastus\",\n-\t\t\t\t\"eastus2\",\n-\t\t\t\t\"francecentral\",\n-\t\t\t\t\"japaneast\",\n-\t\t\t\t\"koreacentral\",\n-\t\t\t\t\"northcentralus\",\n-\t\t\t\t\"northeurope\",\n-\t\t\t\t\"southafricanorth\",\n-\t\t\t\t\"southcentralus\",\n-\t\t\t\t\"southeastasia\",\n-\t\t\t\t\"uksouth\",\n-\t\t\t\t\"ukwest\",\n-\t\t\t\t\"westcentralus\",\n-\t\t\t\t\"westeurope\",\n-\t\t\t\t\"westus\",\n-\t\t\t\t\"westus2\"\n-\t\t\t],\n-\t\t\t\"metadata\": {\n-\t\t\t\t\"description\": \"Specifies the location in which to create the workspace.\"\n-\t\t\t}\n-\t\t},\n+    \"contentVersion\": \"1.0.0.0\",\n+    \"parameters\": {\n+        \"workspaceName\": {\n+            \"type\": \"string\",\n+            \"metadata\": {\n+                \"description\": \"Workspace name\"\n+            }\n+        },\n+        \"sku\": {\n+            \"type\": \"string\",\n+            \"allowedValues\": [\n+                \"pergb2018\",\n+                \"Free\",\n+                \"Standalone\",\n+                \"PerNode\",\n+                \"Standard\",\n+                \"Premium\"\n+            ],\n+            \"defaultValue\": \"pergb2018\",\n+            \"metadata\": {\n+                \"description\": \"Pricing tier: perGB2018 or legacy tiers (Free, Standalone, PerNode, Standard or Premium) which are not available to all customers.\"\n+            }\n+        },\n+        \"dataRetention\": {\n+            \"type\": \"int\",\n+            \"defaultValue\": 30,\n+            \"minValue\": 7,\n+            \"maxValue\": 730,\n+            \"metadata\": {\n+                \"description\": \"Number of days of retention. Workspaces in the legacy Free pricing tier can only have 7 days.\"\n+            }\n+        },\n+        \"immediatePurgeDataOn30Days\": {\n+            \"type\": \"bool\",\n+            \"defaultValue\": \"[bool('false')]\",\n+            \"metadata\": {\n+                \"description\": \"If set to true when changing retention to 30 days, older data will be immediately deleted. Use this with extreme caution. This only applies when retention is being set to 30 days.\"\n+            }\n+        },\n+        \"location\": {\n+            \"type\": \"string\",\n+            \"allowedValues\": [\n+                \"australiacentral\",\n+                \"australiaeast\",\n+                \"australiasoutheast\",\n+                \"brazilsouth\",\n+                \"canadacentral\",\n+                \"centralindia\",\n+                \"centralus\",\n+                \"eastasia\",\n+                \"eastus\",\n+                \"eastus2\",\n+                \"francecentral\",\n+                \"japaneast\",\n+                \"koreacentral\",\n+                \"northcentralus\",\n+                \"northeurope\",\n+                \"southafricanorth\",\n+                \"southcentralus\",\n+                \"southeastasia\",\n+                \"uksouth\",\n+                \"ukwest\",\n+                \"westcentralus\",\n+                \"westeurope\",\n+                \"westus\",\n+                \"westus2\"\n+            ],\n+            \"metadata\": {\n+                \"description\": \"Specifies the location in which to create the workspace.\"\n+            }\n+        },\n         \"automationAccountName\": {\n             \"type\": \"string\",\n             \"metadata\": {\n                 \"description\": \"Automation account name\"\n             }\n         },\n-\t\t\"automationAccountLocation\": {\n-\t\t    \"type\": \"string\",\n-\t\t\t\"metadata\": {\n-\t\t\t    \"description\": \"Specify the location in which to create the Automation account.\"\n-\t\t\t}\n-\t\t}\n+        \"automationAccountLocation\": {\n+            \"type\": \"string\",\n+            \"metadata\": {\n+                \"description\": \"Specify the location in which to create the Automation account.\"\n+            }\n+        }\n     },\n     \"variables\": {\n         \"Updates\": {\n@@ -164,7 +165,8 @@ The following parameters in the template are set with a default value for the Lo\n             \"apiVersion\": \"2017-03-15-preview\",\n             \"location\": \"[parameters('location')]\",\n             \"properties\": {\n-                \"sku\": { \n+                \"sku\": {\n+                    \"Name\": \"[parameters('sku')]\",\n                     \"name\": \"CapacityReservation\",\n                     \"capacityReservationLevel\": 100\n                 },\n@@ -209,7 +211,7 @@ The following parameters in the template are set with a default value for the Lo\n                     \"name\": \"Basic\"\n                 }\n             },\n-\t\t},\n+        },\n         {\n             \"apiVersion\": \"2015-11-01-preview\",\n             \"type\": \"Microsoft.OperationalInsights/workspaces/linkedServices\",\n@@ -227,7 +229,7 @@ The following parameters in the template are set with a default value for the Lo\n     }\n     ```\n \n-2. Edit the template to meet your requirements.\n+2. Edit the template to meet your requirements. Consider creating a [Resource Manager parameters file](../azure-resource-manager/templates/parameter-files.md) instead of passing parameters as inline values.\n \n 3. Save this file as deployUMSolutiontemplate.json to a local folder.\n "
  },
  {
    "Number": 109499,
    "Title": "updated JSON template",
    "ClosedAt": "2020-03-30T18:37:07Z",
    "User": "MGoedtel",
    "FileName": "articles/azure-monitor/platform/template-workspace-configuration.md",
    "Addition": 13,
    "Delections": 14,
    "Changes": 27,
    "Patch": "@@ -71,7 +71,7 @@ For capacity reservation, you define a selected capacity reservation for ingesti\n               \"description\": \"Specifies the name of the workspace.\"\n             }\n         },\n-      \"pricingTier\": {\n+      \"sku\": {\n         \"type\": \"string\",\n         \"allowedValues\": [\n           \"pergb2018\",\n@@ -127,7 +127,7 @@ For capacity reservation, you define a selected capacity reservation for ingesti\n             \"location\": \"[parameters('location')]\",\n             \"properties\": {\n                 \"sku\": {\n-\t\t  \"name\": \"[parameters('pricingTier')]\"\n+                    \"name\": \"[parameters('sku')]\"\n                 },\n                 \"retentionInDays\": 120,\n                 \"features\": {\n@@ -141,16 +141,15 @@ For capacity reservation, you define a selected capacity reservation for ingesti\n     }\n     ```\n \n-> [Information]\n-> for capacity reservation settings, use these properties under \"sku\":\n+   >[!NOTE]\n+   >For capacity reservation settings, use these properties under \"sku\":\n+   >* \"name\": \"CapacityReservation\",\n+   >* \"capacityReservationLevel\": 100\n \n->   \"name\": \"CapacityReservation\",\n+2. Edit the template to meet your requirements. Consider creating a [Resource Manager parameters file](../../azure-resource-manager/templates/parameter-files.md) instead of passing parameters as inline values. Review [Microsoft.OperationalInsights/workspaces template](https://docs.microsoft.com/azure/templates/microsoft.operationalinsights/workspaces) reference to learn what properties and values are supported. \n \n->   \"capacityReservationLevel\": 100\n-\n-\n-2. Edit the template to meet your requirements. Review [Microsoft.OperationalInsights/workspaces template](https://docs.microsoft.com/azure/templates/microsoft.operationalinsights/workspaces) reference to learn what properties and values are supported. \n 3. Save this file as **deploylaworkspacetemplate.json** to a local folder.\n+\n 4. You are ready to deploy this template. You use either PowerShell or the command line to create the workspace, specifying the workspace name and location as part of the command. The workspace name must be globally unique across all Azure subscriptions.\n \n    * For PowerShell use the following commands from the folder containing the template:\n@@ -173,7 +172,7 @@ The deployment can take a few minutes to complete. When it finishes, you see a m\n The following template sample illustrates how to:\n \n 1. Add solutions to the workspace\n-2. Create saved searches. To ensure that deployments don't override saved searches accidently, an eTag property should be added in the \"savedSearches\" resource to override and maintain the idempotency of saved searches.\n+2. Create saved searches. To ensure that deployments don't override saved searches accidentally, an eTag property should be added in the \"savedSearches\" resource to override and maintain the idempotency of saved searches.\n 3. Create a computer group\n 4. Enable collection of IIS logs from computers with the Windows agent installed\n 5. Collect Logical Disk perf counters from Linux computers (% Used Inodes; Free Megabytes; % Used Space; Disk Transfers/sec; Disk Reads/sec; Disk Writes/sec)\n@@ -194,7 +193,7 @@ The following template sample illustrates how to:\n         \"description\": \"Workspace name\"\n       }\n     },\n-    \"pricingTier\": {\n+    \"sku\": {\n       \"type\": \"string\",\n       \"allowedValues\": [\n         \"PerGB2018\",\n@@ -274,7 +273,7 @@ The following template sample illustrates how to:\n     \"metadata\": {\n       \"description\": \"The custom log name\"\n       }\n-\t }\n+     }\n     },\n     \"variables\": {\n       \"Updates\": {\n@@ -303,7 +302,7 @@ The following template sample illustrates how to:\n           \"immediatePurgeDataOn30Days\": \"[parameters('immediatePurgeDataOn30Days')]\"\n         },\n         \"sku\": {\n-          \"name\": \"[parameters('pricingTier')]\"\n+          \"name\": \"[parameters('sku')]\"\n         }\n       },\n       \"resources\": [\n@@ -602,7 +601,7 @@ The following template sample illustrates how to:\n       \"type\": \"string\",\n       \"value\": \"[reference(resourceId('Microsoft.OperationalInsights/workspaces', parameters('workspaceName')), '2015-11-01-preview').customerId]\"\n     },\n-    \"pricingTier\": {\n+    \"sku\": {\n       \"type\": \"string\",\n       \"value\": \"[reference(resourceId('Microsoft.OperationalInsights/workspaces', parameters('workspaceName')), '2015-11-01-preview').sku.name]\"\n     },"
  },
  {
    "Number": 109529,
    "Title": "Fix validation warning in morning publish PR",
    "ClosedAt": "2020-03-30T18:16:27Z",
    "User": "rmca14",
    "FileName": "articles/azure-monitor/monitor-reference.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -194,7 +194,7 @@ Other solutions are available for monitoring different applications and services\n |:---|:---|\n | [Active Directory health check](insights/ad-assessment.md) | Assess the risk and health of your Active Directory environments. |\n | [Active Directory replication status](insights/ad-replication-status.md) | Regularly monitors your Active Directory environment for any replication failures. |\n-| [Activity log analytics](platform/activity-log-view.md#activity-logs-analytics-monitoring-solution) | Analyze Activity log entries using predefined log queries and views. |\n+| [Activity log analytics](platform/activity-log-view.md#azure-portal) | View Activity Log entries. |\n | [DNS Analytics (preview)](insights/dns-analytics.md) | Collects, analyzes, and correlates Windows DNS analytic and audit logs and other related data from your DNS servers. |\n | [Cloud Foundry](../cloudfoundry/cloudfoundry-oms-nozzle.md) | Collect, view, and analyze your Cloud Foundry system health and performance metrics, across multiple deployments. |\n | [Containers](insights/containers.md) | View and manage Docker and Windows container hosts. |"
  },
  {
    "Number": 109446,
    "Title": "Convert connection strings doc to tabbed version",
    "ClosedAt": "2020-03-30T17:58:01Z",
    "User": "MS-jgol",
    "FileName": "articles/azure-monitor/app/sdk-connection-string.md",
    "Addition": 7,
    "Delections": 7,
    "Changes": 14,
    "Patch": "@@ -147,7 +147,7 @@ In this example, this connection string specifies explicit overrides for every s\n \n Connection Strings are supported in the following SDK versions:\n - .NET and .NET Core v2.12.0\n-- Java v2.5.1\n+- Java v2.5.1 and Java 3.0\n - Javascript v2.3.0\n - NodeJS v1.5.0\n - Python v1.0.0\n@@ -160,7 +160,7 @@ A connection string can be set by either in code, environment variable, or confi\n \n - Connection String: `APPLICATIONINSIGHTS_CONNECTION_STRING`\n \n-### .Net SDK example\n+# [.NET/.NetCore](#tab/net)\n \n TelemetryConfiguration.ConnectionString: https://github.com/microsoft/ApplicationInsights-dotnet/blob/add45ceed35a817dc7202ec07d3df1672d1f610d/BASE/src/Microsoft.ApplicationInsights/Extensibility/TelemetryConfiguration.cs#L271-L274\n \n@@ -193,10 +193,10 @@ NetCore config.json:\n ```\n \n \n-### Java SDK example\n+# [Java](#tab/java)\n \n \n-Java Explicitly Set:\n+Java (v2.5.x) Explicitly Set:\n ```java\n TelemetryConfiguration.getActive().setConnectionString(\"InstrumentationKey=00000000-0000-0000-0000-000000000000\");\n ```\n@@ -209,7 +209,7 @@ ApplicationInsights.xml\n </ApplicationInsights>\n ```\n \n-### Javascript SDK example\n+# [JavaScript](#tab/js)\n \n Important: Javascript doesn't support the use of Environment Variables.\n \n@@ -238,15 +238,15 @@ appInsights.loadAppInsights();\n appInsights.trackPageView();\n ```\n \n-### Node SDK example\n+# [Node.js](#tab/nodejs)\n \n ```javascript\n const appInsights = require(\"applicationinsights\");\n appInsights.setup(\"InstrumentationKey=00000000-0000-0000-0000-000000000000;\");\n appInsights.start();\n ```\n \n-### Python SDK example\n+# [Python](#tab/python)\n \n We recommend users set the environment variable.\n "
  },
  {
    "Number": 109526,
    "Title": "Some modifications.",
    "ClosedAt": "2020-03-30T17:42:02Z",
    "User": "yossi-y",
    "FileName": "articles/azure-monitor/platform/customer-managed-keys.md",
    "Addition": 18,
    "Delections": 35,
    "Changes": 53,
    "Patch": "@@ -187,9 +187,12 @@ The identity is assigned to the *Cluster* resource at creation time.\n \n 202 Accepted. This is a standard Resource Manager response for asynchronous operations.\n \n+>[!Important]\n+> It takes the provisioning of the underly ADX cluster a few minutes to complete. You can verify the provisioning state when performing GET REST API call on the *Cluster* resource and looking at the *provisioningState* value. It is *ProvisioningAccount* while provisioning and \"Succeeded\" when completed.\n+\n ### Azure Monitor data-store (ADX cluster) provisioning\n \n-During the early access period of the feature, the ADX cluster is provisioned manually by the product team once the previous steps are completed. Use your Microsoft channel for the provisioning while providing the *Cluster* resource response. \n+During the early access period of the feature, the ADX cluster is provisioned manually by the product team once the previous steps are completed. Use your Microsoft channel for this step and provide the *Cluster* resource response. \n \n ```rst\n GET https://management.azure.com/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.OperationalInsights/clusters/<cluster-name>?api-version=2019-08-01-preview\n@@ -224,9 +227,7 @@ Authorization: Bearer <token>\n }\n ```\n \n->[!Important]\n-> It takes the provisioning of the underly ADX cluster a few minutes to complete. The *provisioningState* value indicates its state, it is *ProvisioningAccount* while provisioning and \"Succeeded\" when provisioning is completed.\n-> The \"principalId\" GUID is generated by the managed identity service for the *Cluster* resource.\n+The \"principalId\" GUID is generated by the managed identity service for the *Cluster* resource.\n \n ### Grant Key Vault permissions\n \n@@ -559,48 +560,27 @@ PUT https://management.azure.com/subscriptions/<subscription-id>/resourceGroups/\n Authorization: Bearer <token>\n Content-type: application/json\n \n-{\n-  \"location\": \"<region-name>\",\n-  \"properties\": {\n-      \"clusterType\":\"ApplicationInsights\"\n-  },\n-  \"identity\": {\n-      \"type\": \"systemAssigned\"\n-  }\n-}\n-```\n-\n-**Response**\n-\n-Identity is assigned to the *Cluster* resource at creation time.\n-\n-```json\n-\n {\n   \"identity\": {\n-    \"type\": \"SystemAssigned\",\n-    \"tenantId\": \"tenant-id\",\n-    \"principalId\": \"principle-id\"\n-  },\n+    \"type\": \"systemAssigned\"\n+    },\n   \"sku\": {\n     \"name\": \"capacityReservation\",\n     \"Capacity\": 1000\n     },\n   \"properties\": {\n-    \"provisioningState\": \"Succeeded\",\n-    \"clusterType\": \"ApplicationInsights\", \n-    \"clusterId\": \"cluster-id\" \n+    \"clusterType\":\"ApplicationInsights\"\n     },\n-  \"id\": \"/subscriptions/subscription-id/resourceGroups/resource-group-name/providers/Microsoft.OperationalInsights/clusters/cluster-name\",\n-  \"name\": \"cluster-name\",\n-  \"type\": \"Microsoft.OperationalInsights/clusters\",\n-  \"location\": \"region-name\"\n+  \"location\": \"<region-name>\"\n }\n ```\n-\"principle-id\" is a GUID that was generated by the managed identity service.\n \n-> [!IMPORTANT]\n-> Copy and keep the \"principle-id\" value since you will need it in next steps.\n+**Response**\n+\n+202 Accepted. This is a standard Resource Manager response for asynchronous operations.\n+\n+>[!Important]\n+> It takes the provisioning of the underly ADX cluster a few minutes to complete. You can verify the provisioning state when performing GET REST API call on the *Cluster* resource and looking at the *provisioningState* value. It is *ProvisioningAccount* while provisioning and \"Succeeded\" when completed.\n \n ### Associate a component to a *Cluster* resource using [Components - Create Or Update](https://docs.microsoft.com/rest/api/application-insights/components/createorupdate) API\n \n@@ -648,6 +628,9 @@ Authorization: Bearer <token>\n   }\n ```\n \n+> [!IMPORTANT]\n+> Copy and keep the \"principle-id\" value since you will need it in next steps.\n+\n **Associate a component**\n \n ```rst"
  },
  {
    "Number": 109389,
    "Title": "Azure Monitor agent download",
    "ClosedAt": "2020-03-27T22:17:37Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/vminsights-enable-overview.md",
    "Addition": 3,
    "Delections": 3,
    "Changes": 6,
    "Patch": "@@ -5,7 +5,7 @@ ms.subservice:\n ms.topic: conceptual\n author: bwren\n ms.author: bwren\n-ms.date: 03/11/2020\n+ms.date: 03/27/2020\n \n ---\n \n@@ -171,8 +171,8 @@ You can download the Dependency agent from these locations:\n \n | File | OS | Version | SHA-256 |\n |:--|:--|:--|:--|\n-| [InstallDependencyAgent-Windows.exe](https://aka.ms/dependencyagentwindows) | Windows | 9.9.2 | 6DFF19B9690E42CA190E3B69137C77904B657FA02895033EAA4C3A6A41DA5C6A |\n-| [InstallDependencyAgent-Linux64.bin](https://aka.ms/dependencyagentlinux) | Linux | 9.9.1 | 1CB447EF30FC042FE7499A686638F3F9B4F449692FB9D80096820F8024BE4D7C |\n+| [InstallDependencyAgent-Windows.exe](https://aka.ms/dependencyagentwindows) | Windows | 9.10.2.9060 | B7725B6B205CF8C336D9AAD87956336C816412740E9D6499BCACB6F862AE3896  |\n+| [InstallDependencyAgent-Linux64.bin](https://aka.ms/dependencyagentlinux) | Linux | 9.10.2.9060 | C6995A67A7782AEC312647D74A99C3C823F68F5FFA490FD4BB6006A2FF2941B0 |\n \n ## Role-based access control\n "
  },
  {
    "Number": 109500,
    "Title": "Add limitation around multi-resource",
    "ClosedAt": "2020-03-30T15:20:56Z",
    "User": "harelbr",
    "FileName": "articles/azure-monitor/platform/alerts-metric-create-templates.md",
    "Addition": 4,
    "Delections": 0,
    "Changes": 4,
    "Patch": "@@ -1513,6 +1513,10 @@ This section will describe Azure Resource Manager templates for three scenarios\n - Monitoring all virtual machines (in one Azure region) in a subscription.\n - Monitoring a list of virtual machines (in one Azure region) in a subscription.\n \n+> [!NOTE]\n+>\n+> In a metric alert rule that monitors multiple resources, only one condition is allowed.\n+\n ### Static threshold alert on all virtual machines in one or more resource groups\n \n This template will create a static threshold metric alert rule that monitors Percentage CPU for all virtual machines (in one Azure region) in one or more resource groups."
  },
  {
    "Number": 109491,
    "Title": "Add note about multi-resource limitation",
    "ClosedAt": "2020-03-30T15:12:08Z",
    "User": "harelbr",
    "FileName": "articles/azure-monitor/platform/alerts-metric-overview.md",
    "Addition": 4,
    "Delections": 0,
    "Changes": 4,
    "Patch": "@@ -137,6 +137,10 @@ You can specify the scope of monitoring by a single metric alert rule in one of\n \n Creating metric alert rules that monitor multiple resources is like [creating any other metric alert](alerts-metric.md) that monitors a single resource. Only difference is that you would select all the resources you want to monitor. You can also create these rules through [Azure Resource Manager templates](../../azure-monitor/platform/alerts-metric-create-templates.md#template-for-a-metric-alert-that-monitors-multiple-resources). You will receive individual notifications for each monitored resource.\n \n+> [!NOTE]\n+>\n+> In a metric alert rule that monitors multiple resources, only one condition is allowed.\n+\n ## Typical latency\n \n For metric alerts, typically you will get notified in under 5 minutes if you set the alert rule frequency to be 1 min. In cases of heavy load for notification systems, you might see a longer latency."
  },
  {
    "Number": 109492,
    "Title": "Azure Monitor VM Insights log query fixes",
    "ClosedAt": "2020-03-30T14:41:07Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/vminsights-log-search.md",
    "Addition": 4,
    "Delections": 4,
    "Changes": 8,
    "Patch": "@@ -261,7 +261,7 @@ let Today = now(); VMComputer | extend DaysSinceBoot = Today - BootTime | summar\n ### Summary of Azure VMs by image, location, and SKU\n \n ```kusto\n-VMComputer | where AzureLocation != \"\" | summarize by ComputerName, AzureImageOffering, AzureLocation, AzureImageSku\n+VMComputer | where AzureLocation != \"\" | summarize by Computer, AzureImageOffering, AzureLocation, AzureImageSku\n ```\n \n ### List the physical memory capacity of all managed computers\n@@ -279,7 +279,7 @@ VMComputer | summarize arg_max(TimeGenerated, *) by _ResourceId | project Comput\n ### Find all processes with \"sql\" in the command line\n \n ```kusto\n-VMComputer | where CommandLine contains_cs \"sql\" | summarize arg_max(TimeGenerated, *) by _ResourceId\n+VMProcess | where CommandLine contains_cs \"sql\" | summarize arg_max(TimeGenerated, *) by _ResourceId\n ```\n \n ### Find a machine (most recent record) by resource name\n@@ -303,7 +303,7 @@ VMProcess | where Machine == \"m-559dbcd8-3130-454d-8d1d-f624e57961bc\" | summariz\n ### List all computers running SQL Server\n \n ```kusto\n-VMComputer | where AzureResourceName in ((search in (VMProcess) \"\\*sql\\*\" | distinct Machine)) | distinct Computer\n+VMComputer | where AzureResourceName in ((search in (VMProcess) \"*sql*\" | distinct Machine)) | distinct Computer\n ```\n \n ### List all unique product versions of curl in my datacenter\n@@ -315,7 +315,7 @@ VMProcess | where ExecutableName == \"curl\" | distinct ProductVersion\n ### Create a computer group of all computers running CentOS\n \n ```kusto\n-VMComputer | where OperatingSystemFullName contains_cs \"CentOS\" | distinct ComputerName\n+VMComputer | where OperatingSystemFullName contains_cs \"CentOS\" | distinct Computer\n ```\n \n ### Bytes sent and received trends"
  },
  {
    "Number": 109485,
    "Title": "Added troubleshooting",
    "ClosedAt": "2020-03-30T13:04:19Z",
    "User": "yossi-y",
    "FileName": "articles/azure-monitor/platform/delete-workspace.md",
    "Addition": 11,
    "Delections": 0,
    "Changes": 11,
    "Patch": "@@ -54,6 +54,17 @@ You can delete a workspace using [PowerShell](https://docs.microsoft.com/powersh\n PS C:\\>Remove-AzOperationalInsightsWorkspace -ResourceGroupName \"resource-group-name\" -Name \"workspace-name\"\n ```\n \n+### Troubleshooting\n+\n+You must have 'Log Analytics Contributor' permissions to delete Log Analytics workspace.<br>\n+If you get an error message '*This workspace name is already in use*' when creating a workspace, it could be since:\n+* The workspace name isn't available and being used by someone in your organization, or by other customer.\n+* The workspace was deleted in the last 14 days and its name kept reserved for the soft-delete period. To override the soft-delete and immediately delete your workspace and create a new workspace with the same name, follow these steps to recover the workspace first and perform permanent delete:<br>\n+   1. [Recover](https://docs.microsoft.com/azure/azure-monitor/platform/delete-workspace#recover-workspace) your workspace.\n+   2. [Permanently delete](https://docs.microsoft.com/azure/azure-monitor/platform/delete-workspace#permanent-workspace-delete) your workspace.\n+   3. Create a new workspace using the same workspace name.\n+\n+\n ## Permanent workspace delete\n The soft-delete method may not fit in some scenarios such as development and testing, where you need to repeat a deployment with the same settings and workspace name. In such cases you can permanently delete your workspace and \"override\" the soft-delete period. The permanent workspace delete operation releases the workspace name and you can create a new workspace using the same name.\n "
  },
  {
    "Number": 109434,
    "Title": "Some editing",
    "ClosedAt": "2020-03-29T07:15:39Z",
    "User": "yossi-y",
    "FileName": "articles/azure-monitor/platform/customer-managed-keys.md",
    "Addition": 6,
    "Delections": 13,
    "Changes": 19,
    "Patch": "@@ -172,7 +172,7 @@ Content-type: application/json\n     \"type\": \"systemAssigned\"\n     },\n   \"sku\": {\n-    \"name\": \"capacityReservationLevel\",\n+    \"name\": \"capacityReservation\",\n     \"Capacity\": 1000\n     },\n   \"properties\": {\n@@ -187,13 +187,6 @@ The identity is assigned to the *Cluster* resource at creation time.\n \n 202 Accepted. This is a standard Resource Manager response for asynchronous operations.\n \n-If you what to delete the *Cluster* resource for any reason -- for example, create it with a different name or clusterType, use this REST API:\n-\n-```rst\n-DELETE\n-https://management.azure.com/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.OperationalInsights/clusters/<cluster-name>?api-version=2019-08-01-preview\n-```\n-\n ### Azure Monitor data-store (ADX cluster) provisioning\n \n During the early access period of the feature, the ADX cluster is provisioned manually by the product team once the previous steps are completed. Use your Microsoft channel for the provisioning while providing the *Cluster* resource response. \n@@ -215,7 +208,7 @@ Authorization: Bearer <token>\n     \"principalId\": \"principal-id\"\n     },\n   \"sku\": {\n-    \"name\": \"capacityReservationLevel\",\n+    \"name\": \"capacityReservation\",\n     \"capacity\": 1000,\n     \"lastSkuUpdate\": \"Sun, 22 Mar 2020 15:39:29 GMT\"\n     },\n@@ -272,7 +265,7 @@ Content-type: application/json\n      \"type\": \"systemAssigned\" \n      },\n    \"sku\": {\n-     \"name\": \"capacityReservationLevel\",\n+     \"name\": \"capacityReservation\",\n      \"capacity\": 1000\n      },\n    \"properties\": {\n@@ -297,7 +290,7 @@ Content-type: application/json\n     \"principalId\": \"principle-id\"\n     },\n   \"sku\": {\n-    \"name\": \"capacityReservationLevel\",\n+    \"name\": \"capacityReservation\",\n     \"capacity\": 1000,\n     \"lastSkuUpdate\": \"Sun, 22 Mar 2020 15:39:29 GMT\"\n     },\n@@ -590,7 +583,7 @@ Identity is assigned to the *Cluster* resource at creation time.\n     \"principalId\": \"principle-id\"\n   },\n   \"sku\": {\n-    \"name\": \"capacityReservationLevel\",\n+    \"name\": \"capacityReservation\",\n     \"Capacity\": 1000\n     },\n   \"properties\": {\n@@ -634,7 +627,7 @@ Authorization: Bearer <token>\n     \"principalId\": \"principal-id\"\n     },\n   \"sku\": {\n-    \"name\": \"capacityReservationLevel\",\n+    \"name\": \"capacityReservation\",\n     \"capacity\": 1000,\n     \"lastSkuUpdate\": \"Sun, 22 Mar 2020 15:39:29 GMT\"\n     },"
  },
  {
    "Number": 107009,
    "Title": "new article GPU-aware cluster monitoring",
    "ClosedAt": "2020-03-27T20:06:12Z",
    "User": "MGoedtel",
    "FileName": "articles/azure-monitor/insights/container-insights-gpu-monitoring.md",
    "Addition": 42,
    "Delections": 0,
    "Changes": 42,
    "Patch": "@@ -0,0 +1,42 @@\n+---\n+title: Configure GPU monitoring with Azure Monitor for containers | Microsoft Docs\n+description: This article describes how you can configure monitoring Kubernetes clusters with NVIDIA and AMD GPU enabled nodes with Azure Monitor for containers.\n+ms.topic: conceptual\n+ms.date: 03/27/2020\n+---\n+\n+# Configure GPU monitoring with Azure Monitor for containers\n+\n+Starting with agent version *ciprod03022019*, Azure monitor for containers integrated agent now supports monitoring GPU (graphical processing units) usage on GPU-aware Kubernetes cluster nodes, and monitor pods/containers requesting and using GPU resources.\n+\n+## Supported GPU vendors\n+\n+Azure Monitor for Containers supports monitoring GPU clusters from following GPU vendors:\n+\n+- [NVIDIA](https://developer.nvidia.com/kubernetes-gpu)\n+\n+- [AMD](https://github.com/RadeonOpenCompute/k8s-device-plugin)\n+\n+Azure Monitor for containers automatically starts monitoring GPU usage on nodes, and GPU requesting pods and workloads by collecting the following metrics at 60sec intervals and storing them in the **InsightMetrics** table:\n+\n+|Metric name |Metric dimension (tags) |Description |\n+|------------|------------------------|------------|\n+|containerGpuDutyCycle |container.azm.ms/clusterId, container.azm.ms/clusterName, containerName, gpuId, gpuModel, gpuVendor|Percentage of time over the past sample period (60 seconds) during which GPU was busy/actively processing for a container. Duty cycle is a number between 1 and 100. |\n+|containerGpuLimits |container.azm.ms/clusterId, container.azm.ms/clusterName, containerName |Each container can specify limits as one or more GPUs. It is not possible to request or limit a fraction of a GPU. |\n+|containerGpuRequests |container.azm.ms/clusterId, container.azm.ms/clusterName, containerName |Each container can request one or more GPUs. It is not possible to request or limit a fraction of a GPU.|\n+|containerGpumemoryTotalBytes |container.azm.ms/clusterId, container.azm.ms/clusterName, containerName, gpuId, gpuModel, gpuVendor |Amount of GPU Memory in bytes available to use for a specific container. |\n+|containerGpumemoryUsedBytes |container.azm.ms/clusterId, container.azm.ms/clusterName, containerName, gpuId, gpuModel, gpuVendor |Amount of GPU Memory in bytes used by a specific container. |\n+|nodeGpuAllocatable |container.azm.ms/clusterId, container.azm.ms/clusterName, gpuVendor |Number of GPUs in a node that can be used by Kubernetes. |\n+|nodeGpuCapacity |container.azm.ms/clusterId, container.azm.ms/clusterName, gpuVendor |Total Number of GPUs in a node. |\n+\n+## GPU performance charts \n+\n+Azure Monitor for containers includes pre-configured charts for the metrics listed earlier in the table as a GPU workbook for every cluster. You can find the GPU workbook **Node GPU** directly from an AKS cluster by selecting **Workbooks** from the left-hand pane, and from the **View Workbooks** drop-down list in the Insight.\n+\n+## Next steps\n+\n+- See [Use GPUs for compute-intensive workloads on Azure Kubernetes Service](../../aks/gpu-cluster.md) (AKS) to learn how to deploy an AKS cluster that includes GPU-enabled nodes.\n+\n+- Learn more about [GPU Optimized VM SKUs in Microsoft Azure](../../virtual-machines/sizes-gpu.md).\n+\n+- Review [GPU support in Kubernetes](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/) to learn more about Kubernetes experimental support for managing GPUs across one or more nodes in a cluster.\n\\ No newline at end of file"
  },
  {
    "Number": 109360,
    "Title": "Adding note about special parameter when using color picker",
    "ClosedAt": "2020-03-27T18:15:18Z",
    "User": "vgorbenko",
    "FileName": "articles/azure-monitor/platform/metrics-charts.md",
    "Addition": 3,
    "Delections": 0,
    "Changes": 3,
    "Patch": "@@ -115,6 +115,9 @@ To change the color of a chart line, click on the colored bar in the legend that\n \n After the chart colors are configured, they will remain that way when you pin the chart to a dashboard. The following section shows you how to pin a chart.\n \n+> [!NOTE]\n+> Due to constraints of our release and publishing schedule, changing colors of the chart lines temporarily requires passing a special parameter **?feature.colorpicker=true** when starting Azure portal [https://portal.azure.com/?feature.colorpicker=true](https://portal.azure.com/?feature.colorpicker=true). This limitation will be removed soon. \n+\n ![metric image](./media/metrics-charts/018.png)\n \n ## Pin charts to dashboards"
  },
  {
    "Number": 109341,
    "Title": "Update telemetry processors for rolename",
    "ClosedAt": "2020-03-27T16:21:54Z",
    "User": "lzchen",
    "FileName": "articles/azure-monitor/app/api-filtering-sampling.md",
    "Addition": 8,
    "Delections": 0,
    "Changes": 8,
    "Patch": "@@ -376,6 +376,14 @@ You can add as many initializers as you like, and they are called in the order t\n \n Telemetry processors in OpenCensus Python are simply callback functions called to process telemetry before they are exported. The callback function must accept an [envelope](https://github.com/census-instrumentation/opencensus-python/blob/master/contrib/opencensus-ext-azure/opencensus/ext/azure/common/protocol.py#L86) data type as its parameter. To filter out telemetry from being exported,make sure the callback function returns `False`. You can see the schema for Azure Monitor data types in the envelopes [here](https://github.com/census-instrumentation/opencensus-python/blob/master/contrib/opencensus-ext-azure/opencensus/ext/azure/common/protocol.py).\n \n+> [!NOTE]\n+> You can modify the `cloud_RoleName` by changing the `ai.cloud.role` attribute in the `tags` field.\n+\n+```python\n+def callback_function(envelope):\n+\tenvelope.tags['ai.cloud.role'] = 'new_role_name.py'\n+```\n+\n ```python\n # Example for log exporter\n import logging"
  },
  {
    "Number": 109301,
    "Title": "(AzureCXP) fixes MicrosoftDocs/azure-docs#50700",
    "ClosedAt": "2020-03-27T09:16:04Z",
    "User": "SwathiDhanwada-MSFT",
    "FileName": "articles/azure-monitor/platform/manage-access.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -21,7 +21,7 @@ This article explains how to manage access to logs and to administer the workspa\n \n ## Configure access control mode\n \n-You can view the access control mode configured on a workspace from the Azure portal or with Azure PowerShell.  You can change this setting using one of the following supported methods:\n+You can view the [access control mode](design-logs-deployment.md) configured on a workspace from the Azure portal or with Azure PowerShell.  You can change this setting using one of the following supported methods:\n \n * Azure portal\n "
  },
  {
    "Number": 109206,
    "Title": "removed note about preview",
    "ClosedAt": "2020-03-26T19:51:07Z",
    "User": "MGoedtel",
    "FileName": "articles/azure-monitor/insights/container-insights-log-search.md",
    "Addition": 4,
    "Delections": 8,
    "Changes": 12,
    "Patch": "@@ -2,7 +2,7 @@\n title: How to Query Logs from Azure Monitor for containers | Microsoft Docs\n description: Azure Monitor for containers collects metrics and log data and this article describes the records and includes sample queries.\n ms.topic: conceptual\n-ms.date: 10/15/2019\n+ms.date: 03/26/2020\n \n ---\n \n@@ -24,23 +24,19 @@ Examples of records that are collected by Azure Monitor for containers and the d\n | Inventory of nodes part of a Kubernetes cluster | `KubeNodeInventory` | TimeGenerated, Computer, ClusterName, ClusterId, LastTransitionTimeReady, Labels, Status, KubeletVersion, KubeProxyVersion, CreationTimeStamp, SourceSystem | \n | Kubernetes Events | `KubeEvents` | TimeGenerated, Computer, ClusterId_s, FirstSeen_t, LastSeen_t, Count_d, ObjectKind_s, Namespace_s, Name_s, Reason_s, Type_s, TimeGenerated_s, SourceComponent_s, ClusterName_s, Message,  SourceSystem | \n | Services in the Kubernetes cluster | `KubeServices` | TimeGenerated, ServiceName_s, Namespace_s, SelectorLabels_s, ClusterId_s, ClusterName_s, ClusterIP_s, ServiceType_s, SourceSystem | \n-| Performance metrics for nodes part of the Kubernetes cluster | Perf &#124; where ObjectName == “K8SNode” | Computer, ObjectName, CounterName &#40;cpuAllocatableBytes, memoryAllocatableBytes, cpuCapacityNanoCores, memoryCapacityBytes, memoryRssBytes, cpuUsageNanoCores, memoryWorkingsetBytes, restartTimeEpoch&#41;, CounterValue, TimeGenerated, CounterPath, SourceSystem | \n-| Performance metrics for containers part of the Kubernetes cluster | Perf &#124; where ObjectName == “K8SContainer” | CounterName &#40; cpuRequestNanoCores, memoryRequestBytes, cpuLimitNanoCores, memoryWorkingSetBytes, restartTimeEpoch, cpuUsageNanoCores, memoryRssBytes&#41;, CounterValue, TimeGenerated, CounterPath, SourceSystem | \n+| Performance metrics for nodes part of the Kubernetes cluster | Perf &#124; where ObjectName == \"K8SNode\" | Computer, ObjectName, CounterName &#40;cpuAllocatableBytes, memoryAllocatableBytes, cpuCapacityNanoCores, memoryCapacityBytes, memoryRssBytes, cpuUsageNanoCores, memoryWorkingsetBytes, restartTimeEpoch&#41;, CounterValue, TimeGenerated, CounterPath, SourceSystem | \n+| Performance metrics for containers part of the Kubernetes cluster | Perf &#124; where ObjectName == \"K8SContainer\" | CounterName &#40; cpuRequestNanoCores, memoryRequestBytes, cpuLimitNanoCores, memoryWorkingSetBytes, restartTimeEpoch, cpuUsageNanoCores, memoryRssBytes&#41;, CounterValue, TimeGenerated, CounterPath, SourceSystem | \n | Custom Metrics |`InsightsMetrics` | Computer, Name, Namespace, Origin, SourceSystem, Tags<sup>1</sup>, TimeGenerated, Type, Va, _ResourceId | \n \n <sup>1</sup> The *Tags* property represents [multiple dimensions](../platform/data-platform-metrics.md#multi-dimensional-metrics) for the corresponding metric. For additional information about the metrics collected and stored in the `InsightsMetrics` table and a description of the record properties, see [InsightsMetrics overview](https://github.com/microsoft/OMS-docker/blob/vishwa/june19agentrel/docs/InsightsMetrics.md).\n \n->[!NOTE]\n->Support for Prometheus is a feature in public preview at this time.\n->\n-\n ## Search logs to analyze data\n \n Azure Monitor Logs can help you look for trends, diagnose bottlenecks, forecast, or correlate data that can help you determine whether the current cluster configuration is performing optimally. Pre-defined log searches are provided for you to immediately start using or to customize to return the information the way you want.\n \n You can perform interactive analysis of data in the workspace by selecting the **View Kubernetes event logs** or **View container logs** option in the preview pane from the **View in analytics** drop-down list. The **Log Search** page appears to the right of the Azure portal page that you were on.\n \n-![Analyze data in Log Analytics](./media/container-insights-analyze/container-health-log-search-example.png)   \n+![Analyze data in Log Analytics](./media/container-insights-analyze/container-health-log-search-example.png)\n \n The container logs output that's forwarded to your workspace are STDOUT and STDERR. Because Azure Monitor is monitoring Azure-managed Kubernetes (AKS), Kube-system is not collected today because of the large volume of generated data. \n "
  },
  {
    "Number": 109157,
    "Title": "Change SKU due to API changes",
    "ClosedAt": "2020-03-26T17:41:48Z",
    "User": "yossi-y",
    "FileName": "articles/azure-monitor/platform/customer-managed-keys.md",
    "Addition": 73,
    "Delections": 89,
    "Changes": 162,
    "Patch": "@@ -5,7 +5,7 @@ ms.subservice: logs\n ms.topic: conceptual\n author: yossi-y\n ms.author: yossiy\n-ms.date: 03/22/2020\n+ms.date: 03/26/2020\n \n ---\n # Azure Monitor customer-managed key configuration \n@@ -71,7 +71,7 @@ authenticate and access your Azure Key Vault via Azure Active Directory.\n \n ![CMK Overview](media/customer-managed-keys/cmk-overview.png)\n 1.\tCustomer’s Key Vault.\n-2.\tCustomer’s Log Analytics Cluster resource having managed identity with permissions to Key Vault – The identity is supported at the data-store (ADX cluster) level.\n+2.\tCustomer’s Log Analytics *Cluster* resource having managed identity with permissions to Key Vault – The identity is supported at the data-store (ADX cluster) level.\n 3.\tAzure Monitor dedicated ADX cluster.\n 4.\tCustomer’s workspaces associated to Cluster resource for CMK encryption.\n \n@@ -146,22 +146,19 @@ CMK capability is an early access feature. The subscriptions where you plan to c\n \n ### Storing encryption key (KEK)\n \n-Create or use an Azure Key Vault that you already have, to generate or import a key to be used for data encryption.\n-\n-The Azure Key Vault must be configured as recoverable to protect your key and the access to your data in Azure Monitor.\n+Create or use an Azure Key Vault that you already have to generate, or import a key to be used for data encryption. The Azure Key Vault must be configured as recoverable to protect your key and the access to your data in Azure Monitor. You can verify this configuration under properties in your in your Key Vault, both *Soft delete* and *Purge protection* should be enabled.\n \n These settings are available via CLI and PowerShell:\n - [Soft Delete](https://docs.microsoft.com/azure/key-vault/key-vault-ovw-soft-delete)\n-    must be turned on\n-- [Purge protection](https://docs.microsoft.com/azure/key-vault/key-vault-ovw-soft-delete#purge-protection) should be turned on to guard against force deletion of the secret / vault even after soft delete\n+- [Purge protection](https://docs.microsoft.com/azure/key-vault/key-vault-ovw-soft-delete#purge-protection) guards against force deletion of the secret / vault even after soft delete\n \n ### Create *Cluster* resource\n \n This resource is used as an intermediate identity connection between your Key Vault and your workspaces. After you receive confirmation that your subscriptions were whitelisted, create a Log Analytics *Cluster* resource at the region where your workspaces are located. Application Insights and Log Analytics require separate *Cluster* resources types. The type of the *Cluster* resource is defined at creation time by setting the \"clusterType\" property to either \"LogAnalytics\", or \"ApplicationInsights\". The Cluster resource type can’t be altered after.\n \n For Application Insights CMK configuration, follow the Appendix content.\n \n-You must specify the capacity reservation level (sku) for the *Cluster* resource. The capacity reservation level can be in the range of 1000 to 2000 and in steps of 100. If you need capacity reservation level higher than 2000, reach your Microsoft contact to enable it. This property doesn’t affect billing currently -- once pricing model for dedicated cluster is introduced, billing will apply to any existing CMK deployments.\n+You must specify the capacity reservation level (sku) for the *Cluster* resource when creating a *Cluster* resource. The capacity reservation level can be in the range of 1000 to 2000 and you can update it in steps of 100 later. If you need capacity reservation level higher than 2000, reach your Microsoft contact to enable it. This property doesn’t affect billing currently -- once pricing model for dedicated cluster is introduced, billing will apply to any existing CMK deployments.\n \n **Create**\n \n@@ -171,17 +168,17 @@ Authorization: Bearer <token>\n Content-type: application/json\n \n {\n-  \"location\": \"<region-name>\",\n-   \"properties\": {\n-      \"clusterType\": \"LogAnalytics\",\n-      \"sku\": {\n-       \"name\": \"CapacityReservation\",\n-       \"capacityReservationLevel\": 1000\n-       }\n+  \"identity\": {\n+    \"type\": \"systemAssigned\"\n     },\n-   \"identity\": {\n-      \"type\": \"systemAssigned\"\n-   }\n+  \"sku\": {\n+    \"name\": \"capacityReservationLevel\",\n+    \"Capacity\": 1000\n+    },\n+  \"properties\": {\n+    \"clusterType\": \"LogAnalytics\",\n+    },\n+  \"location\": \"<region-name>\",\n }\n ```\n The identity is assigned to the *Cluster* resource at creation time.\n@@ -199,17 +196,16 @@ https://management.azure.com/subscriptions/<subscription-id>/resourceGroups/<res\n \n ### Azure Monitor data-store (ADX cluster) provisioning\n \n-During the early access period of the feature, the ADX cluster is provisioned manually by the product team once the previous steps are completed. Use your Microsoft channel to provide the *Cluster* resource details. \n-\n-> [!IMPORTANT]\n-> Copy and provide the JSON response of the *Cluster* resource GET REST API\n-> You will need details from this response for later steps too\n+During the early access period of the feature, the ADX cluster is provisioned manually by the product team once the previous steps are completed. Use your Microsoft channel for the provisioning while providing the *Cluster* resource response. \n \n ```rst\n GET https://management.azure.com/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.OperationalInsights/clusters/<cluster-name>?api-version=2019-08-01-preview\n Authorization: Bearer <token>\n ```\n \n+> [!IMPORTANT]\n+> Copy and save the response since you will need these details in later steps\n+\n **Response**\n ```json\n {\n@@ -218,24 +214,26 @@ Authorization: Bearer <token>\n     \"tenantId\": \"tenant-id\",\n     \"principalId\": \"principal-id\"\n     },\n+  \"sku\": {\n+    \"name\": \"capacityReservationLevel\",\n+    \"capacity\": 1000,\n+    \"lastSkuUpdate\": \"Sun, 22 Mar 2020 15:39:29 GMT\"\n+    },\n   \"properties\": {\n     \"provisioningState\": \"ProvisioningAccount\",\n     \"clusterType\": \"LogAnalytics\", \n     \"clusterId\": \"cluster-id\"\n-    \"sku\": {\n-      \"name\": \"CapacityReservation\",\n-      \"capacityReservationLevel\": 1000,\n-      \"lastSkuUpdate\": \"Sun, 22 Mar 2020 15:39:29 GMT\"\n-      }\n-  },\n+    },\n   \"id\": \"/subscriptions/subscription-id/resourceGroups/resource-group-name/providers/Microsoft.OperationalInsights/clusters/cluster-name\",\n   \"name\": \"cluster-name\",\n   \"type\": \"Microsoft.OperationalInsights/clusters\",\n   \"location\": \"region-name\"\n-  }\n+}\n ```\n \n-\"principal-id\" is a GUID generated by the managed identity service for the *Cluster* resource.\n+>[!Important]\n+> It takes the provisioning of the underly ADX cluster a few minutes to complete. The *provisioningState* value indicates its state, it is *ProvisioningAccount* while provisioning and \"Succeeded\" when provisioning is completed.\n+> The \"principalId\" GUID is generated by the managed identity service for the *Cluster* resource.\n \n ### Grant Key Vault permissions\n \n@@ -261,23 +259,30 @@ details.\n \n **Update**\n \n+>[!Warning]\n+> You must provide a full body in *Cluster* resource update that includes *identity*, *sku*, *KeyVaultProperties* and *location*. Missing the *KeyVaultProperties* details will remove the key identifier from the *Cluster* resource and cause [key revocation](#cmk-kek-revocation).\n+\n ```rst\n PUT https://management.azure.com/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.OperationalInsights/clusters/<cluster-name>?api-version=2019-08-01-preview\n Authorization: Bearer <token>\n Content-type: application/json\n \n {\n+   \"identity\": { \n+     \"type\": \"systemAssigned\" \n+     },\n+   \"sku\": {\n+     \"name\": \"capacityReservationLevel\",\n+     \"capacity\": 1000\n+     },\n    \"properties\": {\n      \"KeyVaultProperties\": {\n        KeyVaultUri: \"https://<key-vault-name>.vault.azure.net\",\n        KeyName: \"<key-name>\",\n        KeyVersion: \"<current-version>\"\n        },\n    },\n-   \"location\":\"<region-name>\",\n-   \"identity\": { \n-     \"type\": \"systemAssigned\" \n-     }\n+   \"location\":\"<region-name>\"\n }\n ```\n \"KeyVaultProperties\" contains the Key Vault key identifier details.\n@@ -290,7 +295,12 @@ Content-type: application/json\n     \"type\": \"SystemAssigned\",\n     \"tenantId\": \"tenant-id\",\n     \"principalId\": \"principle-id\"\n-  },\n+    },\n+  \"sku\": {\n+    \"name\": \"capacityReservationLevel\",\n+    \"capacity\": 1000,\n+    \"lastSkuUpdate\": \"Sun, 22 Mar 2020 15:39:29 GMT\"\n+    },\n   \"properties\": {\n     \"KeyVaultProperties\": {\n       KeyVaultUri: \"https://key-vault-name.vault.azure.net\",\n@@ -310,42 +320,12 @@ Content-type: application/json\n \n ### Workspace association to *Cluster* resource\n \n-> [!IMPORTANT]\n-> This step should be carried after the ADX cluster provisioning. If you associate workspaces and ingest data prior to the provisioning, ingested data before the provisioning will be dropped and won't be recoverable.\n-> To verify that the ADX cluster is provisioned and you can start associating workspaces to it, execute the this REST API and check that \"provisioningState\" value in the response is \"Succeeded\".\n-\n-```rst\n-GET https://management.azure.com/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.OperationalInsights/clusters/<cluster-name>?api-version=2019-08-01-preview\n-Authorization: Bearer <token>\n-```\n-\n-**Response**\n-```json\n-{\n-  \"identity\": {\n-    \"type\": \"SystemAssigned\",\n-    \"tenantId\": \"tenant-id\",\n-    \"principalId\": \"principal-id\"\n-    },\n-  \"properties\": {\n-    \"provisioningState\": \"Succeeded\",\n-    \"clusterType\": \"LogAnalytics\", \n-    \"clusterId\": \"cluster-id\"\n-    \"sku\": {\n-      \"name\": \"CapacityReservation\",\n-      \"capacityReservationLevel\": 1000,\n-      \"lastSkuUpdate\": \"Sun, 22 Mar 2020 15:39:29 GMT\"\n-      }\n-  },\n-  \"id\": \"/subscriptions/subscription-id/resourceGroups/resource-group-name/providers/Microsoft.OperationalInsights/clusters/cluster-name\",\n-  \"name\": \"cluster-name\",\n-  \"type\": \"Microsoft.OperationalInsights/clusters\",\n-  \"location\": \"region-name\"\n-  }\n-```\n-\n For Application Insights CMK configuration, follow the Appendix content for this step.\n \n+> [!IMPORTANT]\n+> This step should be performed only after ADX cluster provisioning. If you associate workspaces and ingest data prior to the provisioning, ingested data will be dropped and won't be recoverable.\n+> To verify that the ADX cluster is provisioned, execute *Cluster* resource Get REST API and check that the *provisioningState* value is *Succeeded*.\n+\n You need to have 'write' permissions to both your workspace and *Cluster* resource to perform this operation, which include these actions:\n \n - In workspace: Microsoft.OperationalInsights/workspaces/write\n@@ -377,8 +357,7 @@ Content-type: application/json\n }\n ```\n \n-After the association, data that is sent to your workspaces is stored\n-encrypted with your managed key.\n+After the workspaces association, data ingested to your workspaces is stored encrypted with your managed key.\n \n ### Workspace association verification\n You can verify if a workspace is associated to a *Custer* resource by looking at the [Workspaces – Get](https://docs.microsoft.com/rest/api/loganalytics/workspaces/get) response. Associated workspace will have a 'clusterResourceId' property with the *Cluster* resource ID.\n@@ -403,7 +382,7 @@ GET https://management.azure.com/subscriptions/<subscription-id>/resourcegroups/\n     \"features\": {\n       \"legacy\": 0,\n       \"searchVersion\": 1,\n-      \"enableLogAccessUsingOnlyResourcePermissions\": true/false,\n+      \"enableLogAccessUsingOnlyResourcePermissions\": true,\n       \"clusterResourceId\": \"/subscriptions/subscription-id/resourceGroups/resource-group-name/providers/Microsoft.OperationalInsights/clusters/cluster-name\"\n     },\n     \"workspaceCapping\": {\n@@ -421,11 +400,7 @@ GET https://management.azure.com/subscriptions/<subscription-id>/resourcegroups/\n \n ## CMK (KEK) revocation\n \n-Azure Monitor Storage will always respect changes in key permissions\n-within an hour, normally sooner, and Storage will become unavailable. Any data ingested to workspaces associated with your *Cluster* resource is dropped and queries will fail. Previously ingested data remains\n-inaccessible in Azure Monitor Storage as long as your key is revoked,\n-and your workspaces aren't deleted. Inaccessible data is governed by the\n-data-retention policy and will be purged when retention is reached.\n+You can revoke your access to your data by disabling your key or deleting the *Cluster* resource access policy in your Key Vault. Azure Monitor Storage will always respect changes in key permissions within an hour, normally sooner, and Storage will become unavailable. Any data ingested to workspaces associated with your *Cluster* resource is dropped and queries will fail. Previously ingested data remains inaccessible in Azure Monitor Storage as long as your key is revoked, and your workspaces aren't deleted. Inaccessible data is governed by the data-retention policy and will be purged when retention is reached.\n \n Storage will periodically poll your Key Vault to attempt to unwrap the\n encryption key and once accessed, data ingestion and query resume within\n@@ -614,12 +589,16 @@ Identity is assigned to the *Cluster* resource at creation time.\n     \"tenantId\": \"tenant-id\",\n     \"principalId\": \"principle-id\"\n   },\n+  \"sku\": {\n+    \"name\": \"capacityReservationLevel\",\n+    \"Capacity\": 1000\n+    },\n   \"properties\": {\n     \"provisioningState\": \"Succeeded\",\n-    \"clusterType\": \"ApplicationInsights\",    //The value is ‘ApplicationInsights’ for Application Insights CMK\n+    \"clusterType\": \"ApplicationInsights\", \n     \"clusterId\": \"cluster-id\" \n-  },\n-  \"id\": \"/subscriptions/subscription-id/resourceGroups/resource-group-name/providers/Microsoft.OperationalInsights/clusters/cluster-name\", //The cluster resource Id\n+    },\n+  \"id\": \"/subscriptions/subscription-id/resourceGroups/resource-group-name/providers/Microsoft.OperationalInsights/clusters/cluster-name\",\n   \"name\": \"cluster-name\",\n   \"type\": \"Microsoft.OperationalInsights/clusters\",\n   \"location\": \"region-name\"\n@@ -638,8 +617,8 @@ You need to have 'write' permissions on both your component and *Cluster* resour\n - In *Cluster* resource: Microsoft.OperationalInsights/clusters/write\n \n > [!IMPORTANT]\n-> This step should be carried after the ADX cluster provisioning. If you associate a component and ingest data prior to the provisioning, ingested data before the provisioning will be dropped and won't be recoverable.\n-> To verify that the ADX cluster is provisioned and you can start associating component to it, execute the this REST API and check that \"provisioningState\" value in the response is \"Succeeded\".\n+> This step should be performed only after ADX cluster provisioning. If you associate components and ingest data prior to the provisioning, ingested data will be dropped and won't be recoverable.\n+> To verify that the ADX cluster is provisioned, execute *Cluster* resource Get REST API and check that the *provisioningState* value is *Succeeded*.\n \n ```rst\n GET https://management.azure.com/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.OperationalInsights/clusters/<cluster-name>?api-version=2019-08-01-preview\n@@ -654,16 +633,21 @@ Authorization: Bearer <token>\n     \"tenantId\": \"tenant-id\",\n     \"principalId\": \"principal-id\"\n     },\n+  \"sku\": {\n+    \"name\": \"capacityReservationLevel\",\n+    \"capacity\": 1000,\n+    \"lastSkuUpdate\": \"Sun, 22 Mar 2020 15:39:29 GMT\"\n+    },\n   \"properties\": {\n+    \"KeyVaultProperties\": {\n+      KeyVaultUri: \"https://key-vault-name.vault.azure.net\",\n+      KeyName: \"key-name\",\n+      KeyVersion: \"current-version\"\n+      },\n     \"provisioningState\": \"Succeeded\",\n     \"clusterType\": \"ApplicationInsights\", \n     \"clusterId\": \"cluster-id\"\n-    \"sku\": {\n-      \"name\": \"CapacityReservation\",\n-      \"capacityReservationLevel\": 1000,\n-      \"lastSkuUpdate\": \"Sun, 22 Mar 2020 15:39:29 GMT\"\n-      }\n-  },\n+    },\n   \"id\": \"/subscriptions/subscription-id/resourceGroups/resource-group-name/providers/Microsoft.OperationalInsights/clusters/cluster-name\",\n   \"name\": \"cluster-name\",\n   \"type\": \"Microsoft.OperationalInsights/clusters\","
  },
  {
    "Number": 108674,
    "Title": "Change SSL to TLS per 1679050",
    "ClosedAt": "2020-03-26T17:21:52Z",
    "User": "TimShererWithAquent",
    "FileName": "articles/azure-monitor/platform/agent-windows-troubleshoot.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -60,7 +60,7 @@ There are several ways you can verify if the agent is successfully communicating\n     |---------|-------|------------|-----------|\n     |2133 & 2129 |Health Service |Connection to the service from the agent failed |This error can occur when the agent cannot communicate directly or through a firewall/proxy server to the Azure Monitor service. Verify agent proxy settings or that the network firewall/proxy allows TCP traffic from the computer to the service.|\n     |2138 |Health Service Modules |Proxy requires authentication |Configure the agent proxy settings and specify the username/password required to authenticate with the proxy server. |\n-    |2129 |Health Service Modules |Failed connection/Failed SSL negotiation |Check your network adapter TCP/IP settings and agent proxy settings.|\n+    |2129 |Health Service Modules |Failed connection/Failed TLS negotiation |Check your network adapter TCP/IP settings and agent proxy settings.|\n     |2127 |Health Service Modules |Failure sending data received error code |If it only happens periodically during the day, it could just be a random anomaly that can be ignored. Monitor to understand how often it happens. If it happens often throughout the day, first check your network configuration and proxy settings. If the description includes HTTP error code 404 and it's the first time that the agent tries to send data to the service, it will include a 500 error with an inner 404 error code. 404 means not found, which indicates that the storage area for the new workspace is still being provisioned. On next retry, data will successfully write to the workspace as expected. An HTTP error 403 might indicate a permission or credentials issue. There is more information included with the 403 error to help troubleshoot the issue.|\n     |4000 |Service Connector |DNS name resolution failed |The machine could not resolve the Internet address used when sending data to the service. This might be DNS resolver settings on your machine, incorrect proxy settings, or maybe a temporary DNS issue with your provider. If it happens periodically, it could be caused by a transient network-related issue.|\n     |4001 |Service Connector |Connection to the service failed. |This error can occur when the agent cannot communicate directly or through a firewall/proxy server to the Azure Monitor service. Verify agent proxy settings or that the network firewall/proxy allows TCP traffic from the computer to the service.|"
  },
  {
    "Number": 108674,
    "Title": "Change SSL to TLS per 1679050",
    "ClosedAt": "2020-03-26T17:21:52Z",
    "User": "TimShererWithAquent",
    "FileName": "articles/azure-monitor/platform/data-security.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -147,7 +147,7 @@ All communication between connected systems and the Log Analytics service is enc\n Each type of agent collects data for Log Analytics. The type of data that is collected is depends on the types of solutions used. You can see a summary of data collection at [Add Log Analytics solutions from the Solutions Gallery](../../azure-monitor/insights/solutions.md). Additionally, more detailed collection information is available for most solutions. A solution is a bundle of predefined views, log search queries, data collection rules, and processing logic. Only administrators can use Log Analytics to import a solution. After the solution is imported, it is moved to the Operations Manager management servers (if used), and then to any agents that you have chosen. Afterward, the agents collect the data.\n \n ## 2. Send data from agents\n-You register all agent types with an enrollment key and a secure connection is established between the agent and the Log Analytics service using certificate-based authentication and SSL with port 443. Log Analytics uses a secret store to generate and maintain keys. Private keys are rotated every 90 days and are stored in Azure and are managed by the Azure operations who follow strict regulatory and compliance practices.\n+You register all agent types with an enrollment key and a secure connection is established between the agent and the Log Analytics service using certificate-based authentication and TLS with port 443. Log Analytics uses a secret store to generate and maintain keys. Private keys are rotated every 90 days and are stored in Azure and are managed by the Azure operations who follow strict regulatory and compliance practices.\n \n With Operations Manager, the management group registered with a Log Analytics workspace establishes a secure HTTPS connection with an Operations Manager management server.\n \n@@ -157,7 +157,7 @@ With any agent reporting to an Operations Manager management group that is integ\n \n The Windows or management server agent cached data is protected by the operating system's credential store. If the service cannot process the data after two hours, the agents will queue the data. If the queue becomes full, the agent starts dropping data types, starting with performance data. The agent queue limit is a registry key so you can modify it, if necessary. Collected data is compressed and sent to the service, bypassing the Operations Manager management group databases, so it does not add any load to them. After the collected data is sent, it is removed from the cache.\n \n-As described above, data from the management server or direct-connected agents is sent over SSL to Microsoft Azure datacenters. Optionally, you can use ExpressRoute to provide additional security for the data. ExpressRoute is a way to directly connect to Azure from your existing WAN network, such as a multi-protocol label switching (MPLS) VPN, provided by a network service provider. For more information, see [ExpressRoute](https://azure.microsoft.com/services/expressroute/).\n+As described above, data from the management server or direct-connected agents is sent over TLS to Microsoft Azure datacenters. Optionally, you can use ExpressRoute to provide additional security for the data. ExpressRoute is a way to directly connect to Azure from your existing WAN network, such as a multi-protocol label switching (MPLS) VPN, provided by a network service provider. For more information, see [ExpressRoute](https://azure.microsoft.com/services/expressroute/).\n \n ## 3. The Log Analytics service receives and processes data\n The Log Analytics service ensures that incoming data is from a trusted source by validating certificates and the data integrity with Azure authentication. The unprocessed raw data is then stored in an Azure Event Hub in the region the data will eventually be stored at rest. The type of data that is stored depends on the types of solutions that were imported and used to collect data. Then, the Log Analytics service processes the raw data and ingests it into the database."
  },
  {
    "Number": 109161,
    "Title": "add engineering's ARM template example to doc",
    "ClosedAt": "2020-03-26T16:43:54Z",
    "User": "mrbullwinkle",
    "FileName": "articles/azure-monitor/app/snapshot-debugger-appservice.md",
    "Addition": 43,
    "Delections": 1,
    "Changes": 44,
    "Patch": "@@ -4,7 +4,7 @@ description: Enable Snapshot Debugger for .NET apps in Azure App Service\n ms.topic: conceptual\n author: brahmnes\n ms.author: bfung\n-ms.date: 03/07/2019\n+ms.date: 03/26/2019\n \n ms.reviewer: mbullwin\n ---\n@@ -42,6 +42,48 @@ Application Insights Snapshot Debugger is pre-installed as part of the App Servi\n Follow the same steps as for **Enable Snapshot Debugger**, but switch both switches for Snapshot Debugger to **Off**.\n We recommend that you have Snapshot Debugger enabled on all your apps to ease diagnostics of application exceptions.\n \n+## Azure Resource Manager template\n+\n+For an Azure App Service, you can set app settings in an Azure Resource Manager template to enable Snapshot Debugger and Profiler. You add a config resource that contains the app settings as a child resource of the website:\n+\n+```json\n+{\n+  \"apiVersion\": \"2015-08-01\",\n+  \"name\": \"[parameters('webSiteName')]\",\n+  \"type\": \"Microsoft.Web/sites\",\n+  \"location\": \"[resourceGroup().location]\",\n+  \"dependsOn\": [\n+    \"[variables('hostingPlanName')]\"\n+  ],\n+  \"tags\": { \n+    \"[concat('hidden-related:', resourceId('Microsoft.Web/serverfarms', variables('hostingPlanName')))]\": \"empty\",\n+    \"displayName\": \"Website\"\n+  },\n+  \"properties\": {\n+    \"name\": \"[parameters('webSiteName')]\",\n+    \"serverFarmId\": \"[resourceId('Microsoft.Web/serverfarms', variables('hostingPlanName'))]\"\n+  },\n+  \"resources\": [\n+    {\n+      \"apiVersion\": \"2015-08-01\",\n+      \"name\": \"appsettings\",\n+      \"type\": \"config\",\n+      \"dependsOn\": [\n+        \"[parameters('webSiteName')]\",\n+        \"[concat('AppInsights', parameters('webSiteName'))]\"\n+      ],\n+      \"properties\": {\n+        \"APPINSIGHTS_INSTRUMENTATIONKEY\": \"[reference(resourceId('Microsoft.Insights/components', concat('AppInsights', parameters('webSiteName'))), '2014-04-01').InstrumentationKey]\",\n+        \"APPINSIGHTS_PROFILERFEATURE_VERSION\": \"1.0.0\",\n+        \"APPINSIGHTS_SNAPSHOTFEATURE_VERSION\": \"1.0.0\",\n+        \"DiagnosticServices_EXTENSION_VERSION\": \"~3\",\n+        \"ApplicationInsightsAgent_EXTENSION_VERSION\": \"~2\"\n+      }\n+    }\n+  ]\n+},\n+```\n+\n ## Next steps\n \n - Generate traffic to your application that can trigger an exception. Then, wait 10 to 15 minutes for snapshots to be sent to the Application Insights instance."
  },
  {
    "Number": 109159,
    "Title": "update to link to Python repo based instructions",
    "ClosedAt": "2020-03-26T15:58:50Z",
    "User": "mrbullwinkle",
    "FileName": "articles/azure-monitor/app/custom-endpoints.md",
    "Addition": 4,
    "Delections": 0,
    "Changes": 4,
    "Patch": "@@ -183,6 +183,10 @@ Live Metrics Endpoint: \"QuickPulse_Endpoint_Address\"\n </script>\n ```\n \n+### Python\n+\n+For guidance on modifying the ingestion endpoint for the opencensus-python SDK consult the [opencensus-python repo.](https://github.com/census-instrumentation/opencensus-python/blob/af284a92b80bcbaf5db53e7e0813f96691b4c696/contrib/opencensus-ext-azure/opencensus/ext/azure/common/__init__.py)\n+\n ## Regions that require endpoint modification\n \n Currently the only regions that require endpoint modifications are [Azure Government](https://docs.microsoft.com/azure/azure-government/documentation-government-services-monitoringandmanagement#application-insights) and [Azure China](https://docs.microsoft.com/azure/china/resources-developer-guide)."
  },
  {
    "Number": 109152,
    "Title": "update to the other process explanation",
    "ClosedAt": "2020-03-26T15:30:09Z",
    "User": "MGoedtel",
    "FileName": "articles/azure-monitor/faq.md",
    "Addition": 4,
    "Delections": 4,
    "Changes": 8,
    "Patch": "@@ -499,17 +499,17 @@ This Microsoft FAQ is a list of commonly asked questions about Azure Monitor for\n \n ### What does *Other Processes* represent under the Node view?\n \n-**Other processes** is intended to help you clearly understand the root cause of the high CPU usage on your node. This enables you to distinguish usage between containerized processes vs non-containerized processes. \n+**Other processes** is intended to help you clearly understand the root cause of the high resource usage on your node. This enables you to distinguish usage between containerized processes vs non-containerized processes.\n \n What are these **Other Processes**? \n \n These are non-containerized processes that run on your node.  \n \n How do we calculate this?\n \n-**Other Processes** = Total usage from CAdvisor - Usage from containerized process\n+**Other Processes** = *Total usage from CAdvisor* - *Usage from containerized process*\n \n-The **Other processes** can include: \n+The **Other processes** includes:\n \n - Self-managed or managed Kubernetes non-containerized processes \n \n@@ -519,7 +519,7 @@ The **Other processes** can include:\n \n - System processes running on your node \n \n-- Other non-kubernetes workloads running on node hardware or VM \n+- Other non-Kubernetes workloads running on node hardware or VM \n \n ### I don't see Image and Name property values populated when I query the ContainerLog table.\n "
  },
  {
    "Number": 109152,
    "Title": "update to the other process explanation",
    "ClosedAt": "2020-03-26T15:30:09Z",
    "User": "MGoedtel",
    "FileName": "articles/azure-monitor/insights/container-insights-analyze.md",
    "Addition": 16,
    "Delections": 2,
    "Changes": 18,
    "Patch": "@@ -2,7 +2,7 @@\n title: Kubernetes monitoring with Azure Monitor for containers | Microsoft Docs\n description: This article describes how you can view and analyze the performance of a Kubernetes cluster with Azure Monitor for containers.\n ms.topic: conceptual\n-ms.date: 01/07/2020\n+ms.date: 03/26/2020\n ---\n \n # Monitor your Kubernetes cluster performance with Azure Monitor for containers\n@@ -52,7 +52,7 @@ The health statuses included are:\n * **Warning**: One or more issues are detected that must be addressed or the health condition could become critical.\n * **Unknown**: If the service wasn't able to make a connection with the node or pod, the status changes to an Unknown state.\n * **Not found**: Either the workspace, the resource group, or subscription that contains the workspace for this solution was deleted.\n-* **Unauthorized**: User doesn’t have required permissions to read the data in the workspace.\n+* **Unauthorized**: User doesn't have required permissions to read the data in the workspace.\n * **Error**: An error occurred while attempting to read data from the workspace.\n * **Misconfigured**: Azure Monitor for containers wasn't configured correctly in the specified workspace.\n * **No data**: Data hasn't reported to the workspace for the last 30 minutes.\n@@ -195,6 +195,20 @@ The information that's presented when you view the **Nodes** tab is described in\n | Controller | Only for containers and pods. It shows which controller it resides in. Not all pods are in a controller, so some might display **N/A**. | \n | Trend Min&nbsp;%, Avg&nbsp;%, 50th&nbsp;%, 90th&nbsp;%, 95th&nbsp;%, Max&nbsp;% | Bar graph trend represents the average percentile metric percentage of the controller. |\n \n+You may notice a workload after expanding a node named **Other process**. It represents non-containerized processes that run on your node, and includes:\n+\n+* Self-managed or managed Kubernetes non-containerized processes\n+\n+* Container run-time processes  \n+\n+* Kubelet  \n+\n+* System processes running on your node\n+\n+* Other non-Kubernetes workloads running on node hardware or VM\n+\n+It is calculated by: *Total usage from CAdvisor* - *Usage from containerized process*.  \n+\n In the selector, select **Controllers**.\n \n ![Select Controllers view](./media/container-insights-analyze/containers-controllers-tab.png)"
  },
  {
    "Number": 109150,
    "Title": "PHP and WordPress are not supported",
    "ClosedAt": "2020-03-26T15:29:53Z",
    "User": "mrbullwinkle",
    "FileName": "articles/azure-monitor/app/azure-web-apps.md",
    "Addition": 4,
    "Delections": 0,
    "Changes": 4,
    "Patch": "@@ -391,6 +391,10 @@ This is due to the APPINSIGHTS_JAVASCRIPT_ENABLED application setting being set\n \n For the latest information on the Application Insights agent/extension, check out the [release notes](https://github.com/Microsoft/ApplicationInsights-Home/blob/master/app-insights-web-app-extensions-releasenotes.md).\n \n+### PHP and WordPress are not supported\n+\n+PHP and WordPress sites are not supported. There is currently no officially supported SDK/agent for server-side monitoring of these workloads. However, manually instrumenting client-side transactions on a PHP or WordPress site by adding the client-side javascript to your web pages can be accomplished by using the [JavaScript SDK](https://docs.microsoft.com/azure/azure-monitor/app/javascript). \n+\n ## Next steps\n * [Run the profiler on your live app](../app/profiler.md).\n * [Azure Functions](https://github.com/christopheranderson/azure-functions-app-insights-sample) - monitor Azure Functions with Application Insights"
  },
  {
    "Number": 109135,
    "Title": "updated CI section of FAQ",
    "ClosedAt": "2020-03-26T14:56:31Z",
    "User": "MGoedtel",
    "FileName": "articles/azure-monitor/faq.md",
    "Addition": 25,
    "Delections": 17,
    "Changes": 42,
    "Patch": "@@ -6,11 +6,10 @@ ms.subservice:\n ms.topic: conceptual\n author: bwren\n ms.author: bwren\n-ms.date: 03/12/2020\n+ms.date: 03/26/2020\n \n ---\n \n-\n # Azure Monitor Frequently Asked Questions\n \n This Microsoft FAQ is a list of commonly asked questions about Azure Monitor.\n@@ -58,10 +57,6 @@ There is no limit to the amount of metric data you can collect, but this data is\n ### How do I access data collected by Azure Monitor?\n Insights and solutions provide a custom experience for working with data stored in Azure Monitor. You can work directly with log data using a log query written in Kusto Query Language (KQL). In the Azure portal, you can write and run queries and interactively analyze data using Log Analytics. Analyze metrics in the Azure portal with the Metrics Explorer. See [Analyze log data in Azure Monitor](log-query/log-query-overview.md) and [Getting started with Azure Metrics Explorer](platform/metrics-getting-started.md).\n \n-\n-\n-\n-\n ## Solutions and insights\n \n ### What is an insight in Azure Monitor?\n@@ -74,11 +69,6 @@ Monitoring solutions are packaged sets of logic for monitoring a particular appl\n \n To view solutions in the Azure portal, click **More** in the **Insights** section of the **Monitor** menu. Click **Add** to add additional solutions to the workspace.\n \n-\n-\n-\n-\n-\n ## Logs\n \n ### What's the difference between Azure Monitor Logs and Azure Data Explorer?\n@@ -103,9 +93,6 @@ Many resource providers are automatically registered, but you may need to manual\n ### Why am I am getting no access error message when opening Log Analytics from a VM? \n To view VM Logs, you need to be granted with read permission to the workspaces that stores the VM logs. In these cases, your administrator must grant you with to permissions in Azure.\n \n-\n-\n-\n ## Alerts\n \n ### What is an alert in Azure Monitor?\n@@ -127,7 +114,6 @@ An action group is a collection of notifications and actions that can be trigger\n ### What is an action rule?\n An action rule allows you to modify the behavior of a set of alerts that match a certain criteria. This allows you to to perform such requirements as disable alert actions during a maintenance window. You can also apply an action group to a set of alerts rather than applying them directly to the alert rules. See [Action rules](platform/alerts-action-rules.md).\n \n-\n ## Agents\n \n ### Does Azure Monitor require an agent?\n@@ -196,7 +182,6 @@ See [Network firewall requirements](platform/log-analytics-agent.md#network-fire\n \n View Designer is only available for users assigned with Contributor permissions or higher in the Log Analytics workspace.\n \n-\n ## Application Insights\n \n ### Configuration problems\n@@ -508,11 +493,34 @@ Most Application Insights data has a latency of under 5 minutes. Some data can t\n [windows]: app/app-insights-windows-get-started.md\n \n \n-\n ## Azure Monitor for containers\n \n This Microsoft FAQ is a list of commonly asked questions about Azure Monitor for containers. If you have any additional questions about the solution, go to the [discussion forum](https://feedback.azure.com/forums/34192--general-feedback) and post your questions. When a question is frequently asked, we add it to this article so that it can be found quickly and easily.\n \n+### What does *Other Processes* represent under the Node view?\n+\n+**Other processes** is intended to help you clearly understand the root cause of the high CPU usage on your node. This enables you to distinguish usage between containerized processes vs non-containerized processes. \n+\n+What are these **Other Processes**? \n+\n+These are non-containerized processes that run on your node.  \n+\n+How do we calculate this?\n+\n+**Other Processes** = Total usage from CAdvisor - Usage from containerized process\n+\n+The **Other processes** can include: \n+\n+- Self-managed or managed Kubernetes non-containerized processes \n+\n+- Container Run-time processes  \n+\n+- Kubelet  \n+\n+- System processes running on your node \n+\n+- Other non-kubernetes workloads running on node hardware or VM \n+\n ### I don't see Image and Name property values populated when I query the ContainerLog table.\n \n For agent version ciprod12042019 and later, by default these two properties are not populated for every log line to minimize cost incurred on log data collected. There are two options to query the table that include these properties with their values:"
  },
  {
    "Number": 109128,
    "Title": "(AzureCXP) Fixed heading",
    "ClosedAt": "2020-03-26T14:21:02Z",
    "User": "BhargaviAnnadevara-MSFT",
    "FileName": "articles/azure-monitor/app/java-get-started.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -175,7 +175,7 @@ As you accumulate more data, you can run queries both to aggregate data and to f\n \n ![Example of Analytics](./media/java-get-started/0025.png)\n \n-## 7. Install your app on the server\n+## Install your app on the server\n Now publish your app to the server, let people use it, and watch the telemetry show up on the portal.\n \n * Make sure your firewall allows your application to send telemetry to these ports:"
  },
  {
    "Number": 109127,
    "Title": "(AzureCXP) Corrected property description",
    "ClosedAt": "2020-03-26T11:56:03Z",
    "User": "BhargaviAnnadevara-MSFT",
    "FileName": "articles/azure-monitor/app/export-telemetry.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -106,7 +106,7 @@ Here's the form of the path:\n Where\n \n * `blobCreationTimeUtc` is the time when blob was created in the internal staging storage\n-* `blobDeliveryTimeUtc` is the the time when blob is copied to the export destination storage\n+* `blobDeliveryTimeUtc` is the time when blob is copied to the export destination storage\n \n ## <a name=\"format\"></a> Data format\n * Each blob is a text file that contains multiple '\\n'-separated rows. It contains the telemetry processed over a time period of roughly half a minute."
  },
  {
    "Number": 109115,
    "Title": "Clarify code samples do not apply to .NET Core",
    "ClosedAt": "2020-03-26T08:23:07Z",
    "User": "mrbullwinkle",
    "FileName": "articles/azure-monitor/app/asp-net-exceptions.md",
    "Addition": 2,
    "Delections": 0,
    "Changes": 2,
    "Patch": "@@ -24,6 +24,8 @@ Exceptions in your live web app are reported by [Application Insights](../../azu\n   * [Web API 2.*](#web-api-2x)\n   * [WCF](#wcf)\n \n+  This article is specifically focused on .NET Framework apps from a code example perspective. Some of the methods that work for .NET Framework are obsolete in the .NET Core SDK. Refer to the [.NET Core SDK documentation](https://docs.microsoft.com/azure/azure-monitor/app/asp-net-core) if you have a .NET Core app.\n+\n ## Diagnosing exceptions using Visual Studio\n Open the app solution in Visual Studio to help with debugging.\n "
  },
  {
    "Number": 109114,
    "Title": "update per github issue",
    "ClosedAt": "2020-03-26T08:11:42Z",
    "User": "mrbullwinkle",
    "FileName": "articles/azure-monitor/app/asp-net-dependencies.md",
    "Addition": 4,
    "Delections": 2,
    "Changes": 6,
    "Patch": "@@ -2,7 +2,7 @@\n title: Dependency Tracking in Azure Application Insights | Microsoft Docs\n description: Monitor dependency calls from your on-premises or Microsoft Azure web application with Application Insights.\n ms.topic: conceptual\n-ms.date: 06/25/2019\n+ms.date: 03/26/2020\n \n ---\n \n@@ -30,13 +30,15 @@ If you're missing a dependency, or using a different SDK make sure it's in the l\n \n ## Setup automatic dependency tracking in Console Apps\n \n-To automatically track dependencies from .NET/.NET Core console apps, install the Nuget package `Microsoft.ApplicationInsights.DependencyCollector`, and initialize `DependencyTrackingTelemetryModule` as follows:\n+To automatically track dependencies from .NET console apps, install the Nuget package `Microsoft.ApplicationInsights.DependencyCollector`, and initialize `DependencyTrackingTelemetryModule` as follows:\n \n ```csharp\n     DependencyTrackingTelemetryModule depModule = new DependencyTrackingTelemetryModule();\n     depModule.Initialize(TelemetryConfiguration.Active);\n ```\n \n+For .NET Core console apps TelemetryConfiguration.Active is obsolete. Refer to the guidance in the [worker service documentation](https://docs.microsoft.com/azure/azure-monitor/app/worker-service) and the [ASP.NET Core monitoring documentation](https://docs.microsoft.com/azure/azure-monitor/app/asp-net-core)\n+\n ### How automatic dependency monitoring works?\n \n Dependencies are automatically collected by using one of the following techniques:"
  },
  {
    "Number": 109105,
    "Title": "remove outdated alerts guidance",
    "ClosedAt": "2020-03-26T08:05:43Z",
    "User": "mrbullwinkle",
    "FileName": "articles/azure-monitor/app/alerts.md",
    "Addition": 0,
    "Delections": 59,
    "Changes": 59,
    "Patch": "@@ -20,65 +20,6 @@ There are multiple types of alerts:\n * [**Web tests**][availability] tell you when your site is unavailable on the internet, or responding slowly. [Learn more][availability].\n * [**Proactive diagnostics**](../../azure-monitor/app/proactive-diagnostics.md) are configured automatically to notify you about unusual performance patterns.\n \n-## Set a Metric alert\n-\n-Open the Alert rules tab, and then use the add button.\n-\n-![In the Alert rules tab, choose Add Alert. Set your app as the resource to measure, provide a name for the alert, and choose a metric.](./media/alerts/01-set-metric.png)\n-\n-* Set the resource before the other properties. **Choose the \"(components)\" resource** if you want to set alerts on performance or usage metrics.\n-* The name that you give to the alert must be unique within the resource group (not just your application).\n-* Be careful to note the units in which you're asked to enter the threshold value.\n-* If you check the box \"Email owners...\", alerts are sent by email to everyone who has access to this resource group. To expand this set of people, add them to the [resource group or subscription](../../azure-monitor/app/resources-roles-access-control.md) (not the resource).\n-* If you specify \"Additional emails\", alerts are sent to those individuals or groups (whether or not you checked the \"email owners...\" box). \n-* Set a [webhook address](../../azure-monitor/platform/alerts-webhooks.md) if you have set up a web app that responds to alerts. It is called both when the alert is Activated and when it is Resolved. (But note that at present, query parameters are not passed through as webhook properties.)\n-* You can Disable or Enable the alert: see the buttons at the top.\n-\n-*I don't see the Add Alert button.*\n-\n-* Are you using an organizational account? You can set alerts if you have owner or contributor access to this application resource. Take a look at the Access Control tab. [Learn about access control][roles].\n-\n-> [!NOTE]\n-> In the alerts blade, you see that there's already an alert set up: [Proactive Diagnostics](../../azure-monitor/app/proactive-failure-diagnostics.md). The automatic alert monitors one particular metric, request failure rate. Unless you decide to disable the proactive alert, you don't need to set your own alert on request failure rate.\n-> \n-> \n-\n-## See your alerts\n-You get an email when an alert changes state between inactive and active. \n-\n-The current state of each alert is shown in the Alert rules tab.\n-\n-There's a summary of recent activity in the alerts drop-down:\n-\n-![Alerts drop-down](./media/alerts/010-alert-drop.png)\n-\n-The history of state changes is in the Activity Log:\n-\n-![On the Overview tab, click Settings, Audit logs](./media/alerts/09-alerts.png)\n-\n-## How alerts work\n-* An alert has three states: \"Never activated\", \"Activated\", and \"Resolved.\" Activated means the condition you specified was true, when it was last evaluated.\n-* A notification is generated when an alert changes state. (If the alert condition was already true when you created the alert, you might not get a notification until the condition goes false.)\n-* Each notification generates an email if you checked the emails box, or provided email addresses. You can also look at the Notifications drop-down list.\n-* An alert is evaluated each time a metric arrives, but not otherwise.\n-* The evaluation aggregates the metric over the preceding period, and then compares it to the threshold to determine the new state.\n-* The period that you choose specifies the interval over which metrics are aggregated. It doesn't affect how often the alert is evaluated: that depends on the frequency of arrival of metrics.\n-* If no data arrives for a particular metric for some time, the gap has different effects on alert evaluation and on the charts in metric explorer. In metric explorer, if no data is seen for longer than the chart's sampling interval, the chart shows a value of 0. But an alert based on the same metric is not be reevaluated, and the alert's state remains unchanged. \n-  \n-    When data eventually arrives, the chart jumps back to a non-zero value. The alert evaluates based on the data available for the period you specified. If the new data point is the only one available in the period, the aggregate is based just on that data point.\n-* An alert can flicker frequently between alert and healthy states, even if you set a long period. This can happen if the metric value hovers around the threshold. There is no hysteresis in the threshold: the transition to alert happens at the same value as the transition to healthy.\n-\n-## What are good alerts to set?\n-It depends on your application. To start with, it's best not to set too many metrics. Spend some time looking at your metric charts while your app is running, to get a feel for how it behaves normally. This practice helps you find ways to improve its performance. Then set up alerts to tell you when the metrics go outside the normal zone. \n-\n-Popular alerts include:\n-\n-* [Browser metrics][client], especially Browser **page load times**, are good for web applications. If your page has many scripts, you should look for **browser exceptions**. In order to get these metrics and alerts, you have to set up [web page monitoring][client].\n-* **Server response time** for the server side of web applications. As well as setting up alerts, keep an eye on this metric to see if it varies disproportionately with high request rates: variation might indicate that your app is running out of resources. \n-* **Server exceptions** - to see them, you have to do some [additional setup](../../azure-monitor/app/asp-net-exceptions.md).\n-\n-Don't forget that [proactive failure rate diagnostics](../../azure-monitor/app/proactive-failure-diagnostics.md) automatically monitor the rate at which your app responds to requests with failure codes.\n-\n ## How to set an exception alert using custom log search\n \n In this section, we will go through how to set a query based exception alert. For this example, let's say we want an alert when the failed rate is greater than 10% in the last 24 hours."
  },
  {
    "Number": 109113,
    "Title": "revert part of change",
    "ClosedAt": "2020-03-26T07:56:36Z",
    "User": "mrbullwinkle",
    "FileName": "articles/azure-monitor/app/api-custom-events-metrics.md",
    "Addition": 2,
    "Delections": 3,
    "Changes": 5,
    "Patch": "@@ -59,10 +59,9 @@ If you use AzureFunctions v2+ or Azure WebJobs v3+ - follow this document: https\n *C#*\n \n ```csharp\n-TelemetryConfiguration telemetry = TelemetryConfiguration.CreateDefault();\n-var TelemetryClient = new TelemetryClient(telemetry);\n+private TelemetryClient telemetry = new TelemetryClient();\n ```\n-For anyone using the now obsolete method please visit [microsoft/ApplicationInsights-dotnet#1152](https://github.com/microsoft/ApplicationInsights-dotnet/issues/1152) for further details.\n+For anyone seeing this method is obsolete messages please visit [microsoft/ApplicationInsights-dotnet#1152](https://github.com/microsoft/ApplicationInsights-dotnet/issues/1152) for further details.\n \n *Visual Basic*\n "
  },
  {
    "Number": 109103,
    "Title": "Clarify Failure Anomalies absence from the Smart Detection Table",
    "ClosedAt": "2020-03-26T07:48:40Z",
    "User": "mrbullwinkle",
    "FileName": "articles/azure-monitor/app/proactive-arm-config.md",
    "Addition": 23,
    "Delections": 24,
    "Changes": 47,
    "Patch": "@@ -25,8 +25,6 @@ You can configure the following settings for a smart detection rule:\n To allow configuring the rule settings via Azure Resource Manager, the smart detection rule configuration is now available as an inner resource within the Application Insights resource, named **ProactiveDetectionConfigs**.\n For maximal flexibility, each smart detection rule can be configured with unique notification settings.\n \n-## \n-\n ## Examples\n \n Below are a few examples showing how to configure the settings of smart detection rules using Azure Resource Manager templates.\n@@ -127,12 +125,33 @@ Make sure to replace the Application Insights resource name, and to specify the\n \n ```\n \n+\n+## Smart detection rule names\n+\n+Below is a table of smart detection rule names as they appear in the portal, along with their internal names, that should be used in the Azure Resource Manager template.\n+\n+> [!NOTE]\n+> Smart detection rules marked as _preview_ don’t support email notifications. Therefore, you can only set the _enabled_ property for these rules. \n+\n+| Azure portal rule name | Internal name\n+|:---|:---|\n+| Slow page load time |\tslowpageloadtime |\n+| Slow server response time | slowserverresponsetime |\n+| Long dependency duration | longdependencyduration |\n+| Degradation in server response time | degradationinserverresponsetime |\n+| Degradation in dependency duration | degradationindependencyduration |\n+| Degradation in trace severity ratio (preview) | extension_traceseveritydetector |\n+| Abnormal rise in exception volume (preview) | extension_exceptionchangeextension |\n+| Potential memory leak detected (preview) | extension_memoryleakextension |\n+| Potential security issue detected (preview) | extension_securityextensionspackage |\n+| Abnormal rise in daily data volume (preview) | extension_billingdatavolumedailyspikeextension |\n+\n ### Failure Anomalies alert rule\n \n This Azure Resource Manager template demonstrates configuring a Failure Anomalies alert rule with a severity of 2. This new version of the Failure Anomalies alert rule is part of the new Azure alerting platform, and replaces the classic version that is being retired as part of the [classic alerts retirement process](https://azure.microsoft.com/updates/classic-alerting-monitoring-retirement/).\n \n > [!NOTE]\n-> Failure Anomalies is a global service therefore rule location is create on the global location.\n+> Failure Anomalies is a global service therefore rule location is created on the global location.\n \n ```json\n {\n@@ -163,27 +182,7 @@ This Azure Resource Manager template demonstrates configuring a Failure Anomalie\n ```\n \n > [!NOTE]\n-> This Azure Resource Manager template is unique to the Failure Anomalies alert rule and is different from the other classic Smart Detection rules described in this article.\n-\n-## Smart detection rule names\n-\n-Below is a table of smart detection rule names as they appear in the portal, along with their internal names, that should be used in the Azure Resource Manager template.\n-\n-> [!NOTE]\n-> Smart detection rules marked as _preview_ don’t support email notifications. Therefore, you can only set the _enabled_ property for these rules. \n-\n-| Azure portal rule name | Internal name\n-|:---|:---|\n-| Slow page load time |\tslowpageloadtime |\n-| Slow server response time | slowserverresponsetime |\n-| Long dependency duration | longdependencyduration |\n-| Degradation in server response time | degradationinserverresponsetime |\n-| Degradation in dependency duration | degradationindependencyduration |\n-| Degradation in trace severity ratio (preview) | extension_traceseveritydetector |\n-| Abnormal rise in exception volume (preview) | extension_exceptionchangeextension |\n-| Potential memory leak detected (preview) | extension_memoryleakextension |\n-| Potential security issue detected (preview) | extension_securityextensionspackage |\n-| Abnormal rise in daily data volume (preview) | extension_billingdatavolumedailyspikeextension |\n+> This Azure Resource Manager template is unique to the Failure Anomalies alert rule and is different from the other classic Smart Detection rules described in this article. If you want to manage Failure Anomalies manually this is done in Azure Monitor Alerts whereas all other Smart Detection rules are managed in the Smart Detection pane of the UI.\n \n ## Next Steps\n "
  },
  {
    "Number": 109109,
    "Title": "update to clarify method",
    "ClosedAt": "2020-03-26T07:19:53Z",
    "User": "mrbullwinkle",
    "FileName": "articles/azure-monitor/app/api-custom-events-metrics.md",
    "Addition": 5,
    "Delections": 1,
    "Changes": 6,
    "Patch": "@@ -54,11 +54,15 @@ Get an instance of `TelemetryClient` (except in JavaScript in webpages):\n \n For [ASP.NET Core](asp-net-core.md#how-can-i-track-telemetry-thats-not-automatically-collected) apps and [Non HTTP/Worker for .NET/.NET Core](worker-service.md#how-can-i-track-telemetry-thats-not-automatically-collected) apps, it is recommended to get an instance of `TelemetryClient` from the dependency injection container as explained in their respective documentation.\n \n+If you use AzureFunctions v2+ or Azure WebJobs v3+ - follow this document: https://docs.microsoft.com/azure/azure-functions/functions-monitoring#version-2x-3\n+\n *C#*\n \n ```csharp\n-private TelemetryClient telemetry = new TelemetryClient();\n+TelemetryConfiguration telemetry = TelemetryConfiguration.CreateDefault();\n+var TelemetryClient = new TelemetryClient(telemetry);\n ```\n+For anyone using the now obsolete method please visit [microsoft/ApplicationInsights-dotnet#1152](https://github.com/microsoft/ApplicationInsights-dotnet/issues/1152) for further details.\n \n *Visual Basic*\n "
  },
  {
    "Number": 109098,
    "Title": "Continuous export",
    "ClosedAt": "2020-03-26T05:26:25Z",
    "User": "mrbullwinkle",
    "FileName": "articles/azure-monitor/app/export-telemetry.md",
    "Addition": 15,
    "Delections": 3,
    "Changes": 18,
    "Patch": "@@ -2,7 +2,7 @@\n title: Continuous export of telemetry from Application Insights | Microsoft Docs\n description: Export diagnostic and usage data to storage in Microsoft Azure, and download it from there.\n ms.topic: conceptual\n-ms.date: 07/25/2019\n+ms.date: 03/25/2020\n \n ---\n \n@@ -49,6 +49,18 @@ Once you've created your export, it starts going. You only get data that arrives\n \n There can be a delay of about an hour before data appears in the storage.\n \n+Once the first export is complete you will find a structure similar to the following in your Azure Blob storage container: (This will vary depending on the data you are collecting.)\n+\n+|Name | Description |\n+|:----|:------|\n+| [Availability](export-data-model.md#availability) | Reports [availability web tests](../../azure-monitor/app/monitor-web-app-availability.md).  |\n+| [Event](export-data-model.md#events) | Custom events generated by [TrackEvent()](../../azure-monitor/app/api-custom-events-metrics.md#trackevent). \n+| [Exceptions](export-data-model.md#exceptions) |Reports [exceptions](../../azure-monitor/app/asp-net-exceptions.md) in the server and in the browser.\n+| [Messages](export-data-model.md#trace-messages) | Sent by [TrackTrace](../../azure-monitor/app/api-custom-events-metrics.md#tracktrace), and by the [logging adapters](../../azure-monitor/app/asp-net-trace-logs.md).\n+| [Metrics](export-data-model.md#metrics) | Generated by metric API calls.\n+| [PerformanceCounters](export-data-model.md) | Performance Counters collected by Application Insights.\n+| [Requests](export-data-model.md#requests)| Sent by [TrackRequest](../../azure-monitor/app/api-custom-events-metrics.md#trackrequest). The standard modules use this to reports server response time, measured at the server.| \n+\n ### To edit continuous export\n \n Click on continuous export and select the storage account to edit.\n@@ -93,8 +105,8 @@ Here's the form of the path:\n \n Where\n \n-* `blobCreationTimeUtc` is time when blob was created in the internal staging storage\n-* `blobDeliveryTimeUtc` is the time when blob is copied to the export destination storage\n+* `blobCreationTimeUtc` is the time when blob was created in the internal staging storage\n+* `blobDeliveryTimeUtc` is the the time when blob is copied to the export destination storage\n \n ## <a name=\"format\"></a> Data format\n * Each blob is a text file that contains multiple '\\n'-separated rows. It contains the telemetry processed over a time period of roughly half a minute."
  },
  {
    "Number": 109101,
    "Title": "retire local forwarder docs",
    "ClosedAt": "2020-03-26T03:44:26Z",
    "User": "mrbullwinkle",
    "FileName": "articles/azure-monitor/app/distributed-tracing.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,6 +1,6 @@\n ---\n title: Distributed Tracing in Azure Application Insights | Microsoft Docs\n-description: Provides information about Microsoft's support for distributed tracing through our local forwarder and partnership in the OpenCensus project\n+description: Provides information about Microsoft's support for distributed tracing through our partnership in the OpenCensus project\n ms.topic: conceptual\n author: nikmd23\n ms.author: nimolnar"
  },
  {
    "Number": 109101,
    "Title": "retire local forwarder docs",
    "ClosedAt": "2020-03-26T03:44:26Z",
    "User": "mrbullwinkle",
    "FileName": "articles/azure-monitor/app/opencensus-go.md",
    "Addition": 0,
    "Delections": 220,
    "Changes": 220,
    "Patch": "@@ -1,220 +0,0 @@\n----\n-title: OpenCensus Go tracing with Azure Application Insights | Microsoft Docs\n-description: Provides instructions to integrate OpenCensus Go tracing with the local forwarder and Application Insights\n-ms.topic: conceptual\n-ms.date: 09/15/2018\n-\n----\n-\n-# Collect distributed traces from Go (Preview)\n-\n-Application Insights now supports distributed tracing of Go applications through integration with [OpenCensus](https://opencensus.io) and our new [local forwarder](./opencensus-local-forwarder.md). This article will walk you step-by-step through the process of setting up OpenCensus for Go and getting your trace data to Application Insights.\n-\n-## Prerequisites\n-\n-- You need an Azure Subscription.\n-- Go should be installed, this article uses the version 1.11 [Go Download](https://golang.org/dl/).\n-- Follow the instructions to install the [local forwarder as a Windows service](./opencensus-local-forwarder.md).\n-\n-If you don't have an Azure subscription, create a [free](https://azure.microsoft.com/free/) account before you begin.\n-\n-## Sign in to the Azure portal\n-\n-Sign in to the [Azure portal](https://portal.azure.com/).\n-\n-## Create Application Insights resource\n-\n-First you have to create an Application Insights resource which will generate an instrumentation key (ikey). The ikey is then used to configure the local forwarder to send distributed traces from your OpenCensus instrumented application, to Application Insights.   \n-\n-1. Select **Create a resource** > **Developer Tools** > **Application Insights**.\n-\n-   ![Adding Application Insights Resource](./media/opencensus-Go/0001-create-resource.png)\n-\n- > [!NOTE]\n-   >If this is your first time creating an Application Insights resource you can learn more by visiting the [Create an Application Insights Resource](https://docs.microsoft.com/azure/azure-monitor/app/create-new-resource) article.\n-\n-   A configuration box appears; use the following table to fill out the input fields.\n-\n-   | Settings        | Value           | Description  |\n-   | ------------- |:-------------|:-----|\n-   | **Name**      | Globally Unique Value | Name that identifies the app you are monitoring |\n-   | **Resource Group**     | myResourceGroup      | Name for the new resource group to host App Insights data |\n-   | **Location** | East US | Choose a location near you, or near where your app is hosted |\n-\n-2. Click **Create**.\n-\n-## Configure local forwarder\n-\n-1. Select **Overview** > **Essentials** > Copy your application's **Instrumentation Key**.\n-\n-   ![Screenshot of instrumentation key](./media/opencensus-Go/0003-instrumentation-key.png)\n-\n-2. Edit your `LocalForwarder.config` file and add your instrumentation key. If you followed the instructions in the [pre-requisite](./opencensus-local-forwarder.md) the file is located at `C:\\LF-WindowsServiceHost`\n-\n-    ```xml\n-      <OpenCensusToApplicationInsights>\n-        <!--\n-          Instrumentation key to track telemetry to.\n-          -->\n-        <InstrumentationKey>{enter-instrumentation-key}</InstrumentationKey>\n-      </OpenCensusToApplicationInsights>\n-    \n-      <!-- Describes aspects of processing Application Insights telemetry-->\n-      <ApplicationInsights>\n-        <LiveMetricsStreamInstrumentationKey>{enter-instrumentation-key}</LiveMetricsStreamInstrumentationKey>\n-      </ApplicationInsights>\n-    </LocalForwarderConfiguration>\n-    ```\n-\n-3. Restart the application **Local Forwarder** service.\n-\n-## OpenCensus Go packages\n-\n-1. Install the Open Census packages for Go from the command line:\n-\n-    ```go\n-    go get -u go.opencensus.io\n-    go get -u contrib.go.opencensus.io/exporter/ocagent\n-    ```\n-\n-2. Add the following code to a .go file and then build and run. (This example is derived from the official OpenCensus guidance with added code that facilitates the integration with the local forwarder)\n-\n-     ```go\n-        // Copyright 2018, OpenCensus Authors\n-        //\n-        // Licensed under the Apache License, Version 2.0 (the \"License\");\n-        // you may not use this file except in compliance with the License.\n-        // You may obtain a copy of the License at\n-        //\n-        //     https://www.apache.org/licenses/LICENSE-2.0\n-        //\n-        // Unless required by applicable law or agreed to in writing, software\n-        // distributed under the License is distributed on an \"AS IS\" BASIS,\n-        // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-        // See the License for the specific language governing permissions and\n-        // limitations under the License.\n-        package main\n-        \n-        import (\n-        \n-        \t\"bytes\"\n-        \t\"fmt\"\n-        \t\"log\"\n-        \t\"net/http\"\n-        \tos \"os\"\n-            \n-        \tocagent \"contrib.go.opencensus.io/exporter/ocagent\"\n-        \t\"go.opencensus.io/plugin/ochttp\"\n-        \t\"go.opencensus.io/plugin/ochttp/propagation/tracecontext\"\n-        \t\"go.opencensus.io/trace\"\n-        \n-        )\n-        \n-        func main() {\n-        \t// Register stats and trace exporters to export the collected data.\n-        \tserviceName := os.Getenv(\"SERVICE_NAME\")\n-        \tif len(serviceName) == 0 {\n-        \t\tserviceName = \"go-app\"\n-        \t}\n-        \tfmt.Printf(serviceName)\n-        \tagentEndpoint := os.Getenv(\"OCAGENT_TRACE_EXPORTER_ENDPOINT\")\n-\n-        \tif len(agentEndpoint) == 0 {\n-        \t\tagentEndpoint = fmt.Sprintf(\"%s:%d\", ocagent.DefaultAgentHost, ocagent.DefaultAgentPort)\n-        \t}\n-        \n-        \tfmt.Printf(agentEndpoint)\n-        \texporter, err := ocagent.NewExporter(ocagent.WithInsecure(), ocagent.WithServiceName(serviceName), ocagent.WithAddress(agentEndpoint))\n-        \n-        \tif err != nil {\n-        \t\tlog.Printf(\"Failed to create the agent exporter: %v\", err)\n-        \t}\n-        \n-        \ttrace.RegisterExporter(exporter)\n-        \n-        \ttrace.ApplyConfig(trace.Config{DefaultSampler: trace.AlwaysSample()})\n-        \n-        \tclient := &http.Client{Transport: &ochttp.Transport{Propagation: &tracecontext.HTTPFormat{}}}\n-        \n-        \thttp.HandleFunc(\"/\", func(w http.ResponseWriter, req *http.Request) {\n-        \t\tfmt.Fprintf(w, \"hello world\")\n-        \n-        \t\tvar jsonStr = []byte(`[ { \"url\": \"http://blank.org\", \"arguments\": [] } ]`)\n-        \t\tr, _ := http.NewRequest(\"POST\", \"http://blank.org\", bytes.NewBuffer(jsonStr))\n-        \t\tr.Header.Set(\"Content-Type\", \"application/json\")\n-        \n-        \t\t// Propagate the trace header info in the outgoing requests.\n-        \t\tr = r.WithContext(req.Context())\n-        \t\tresp, err := client.Do(r)\n-        \t\tif err != nil {\n-        \t\t\tlog.Println(err)\n-        \t\t} else {\n-        \t\t\t// TODO: handle response\n-        \t\t\tresp.Body.Close()\n-        \t\t}\n-        \t})\n-        \n-        \thttp.HandleFunc(\"/call_blank\", func(w http.ResponseWriter, req *http.Request) {\n-        \t\tfmt.Fprintf(w, \"hello world\")\n-        \n-        \t\tr, _ := http.NewRequest(\"GET\", \"http://blank.org\", nil)\n-\n-        \t\t// Propagate the trace header info in the outgoing requests.\n-        \t\tr = r.WithContext(req.Context())\n-        \t\tresp, err := client.Do(r)\n-        \n-        \t\tif err != nil {\n-        \t\t\tlog.Println(err)\n-        \t\t} else {\n-        \t\t\t// TODO: handle response\n-        \t\t\tresp.Body.Close()\n-        \t\t}        \n-        \t})\n-        \n-        \tlog.Fatal(http.ListenAndServe(\":50030\", &ochttp.Handler{Propagation: &tracecontext.HTTPFormat{}}))\n-        \n-        }\n-     ```\n-\n-3. Once the simple go app is running navigate to `http://localhost:50030`. Each refresh of the browser will generate the text \"hello world\" accompanied by corresponding span data that is picked up by the local forwarder.\n-\n-4. To confirm that the **local forwarder** is picking up the traces check the `LocalForwarder.config` file. If you followed the steps in the [prerequisite](https://docs.microsoft.com/azure/application-insights/local-forwarder), it will be located in `C:\\LF-WindowsServiceHost`.\n-\n-    In the image below of the log file, you can see that prior to running the second script where we added an exporter `OpenCensus input BatchesReceived` was 0. Once we started running the updated script `BatchesReceived` incremented equal to the number of values we entered:\n-    \n-    ![New App Insights resource form](./media/opencensus-go/0004-batches-received.png)\n-\n-## Start monitoring in the Azure portal\n-\n-1. You can now reopen the Application Insights **Overview** page in the Azure portal, to view details about your currently running application. Select **Live Metric Stream**.\n-\n-   ![Screenshot of overview pane with live metric stream selected in red box](./media/opencensus-go/0005-overview-live-metrics-stream.png)\n-\n-2. If you run the second Go app again and start refreshing the browser for `http://localhost:50030`, you will see live trace data as it arrives in Application Insights from the local forwarder service.\n-\n-   ![Screenshot of live metric stream with performance data displayed](./media/opencensus-go/0006-stream.png)\n-\n-3. Navigate back to the **Overview** page and select **Application Map** for a visual layout of the dependency relationships and call timing between your application components.\n-\n-    ![Screenshot of basic application map](./media/opencensus-go/0007-application-map.png)\n-\n-    Since we were only tracing one method call, our application map isn't as interesting. But application map can scale to visualize far more distributed applications:\n-\n-   ![Application Map](media/opencensus-go/application-map.png)\n-\n-4. Select **Investigate Performance** to perform detailed performance analysis and determine the root cause of slow performance.\n-\n-    ![Screenshot of performance pane](./media/opencensus-go/0008-performance.png)\n-\n-5. Selecting **Samples** and then clicking on any of the samples that appear in the right-hand pane will launch the end-to-end transaction details experience. While our sample app will just show us a single event, a more complex application would allow you to explore the end-to-end transaction down to level of an individual event's call stack.\n-\n-     ![Screenshot of end-to-end transaction interface](./media/opencensus-go/0009-end-to-end-transaction.png)\n-\n-## OpenCensus trace for Go\n-\n-We only covered the basics of integrating OpenCensus for Go with the local forwarder and Application Insights. The [official OpenCensus Go usage guidance](https://godoc.org/go.opencensus.io) covers more advanced topics.\n-\n-## Next steps\n-\n-* [Application map](./../../azure-monitor/app/app-map.md)\n-* [End-to-end performance monitoring](./../../azure-monitor/learn/tutorial-performance.md)"
  },
  {
    "Number": 109101,
    "Title": "retire local forwarder docs",
    "ClosedAt": "2020-03-26T03:44:26Z",
    "User": "mrbullwinkle",
    "FileName": "articles/azure-monitor/app/opencensus-local-forwarder.md",
    "Addition": 0,
    "Delections": 174,
    "Changes": 174,
    "Patch": "@@ -1,174 +0,0 @@\n----\n-title: Azure Application Insights OpenCensus local forwarder (Preview)\n-description: Forward OpenCensus distributed traces and spans from languages like Python and Go to Azure Application Insights\n-ms.topic: conceptual\n-ms.date: 09/18/2018\n-\n-ms.reviewer: nimolnar\n----\n-\n-# Local forwarder (Preview)\n-\n-Local forwarder is an agent that collects Application Insights or [OpenCensus](https://opencensus.io/) telemetry from a variety of SDKs and routes it to Application Insights. It's capable of running under Windows and Linux. You may also be able to run it under macOS, but that is not officially supported at this time.\n-\n-## Running local forwarder\n-\n-Local forwarder is an [open source project on GitHub](https://github.com/Microsoft/ApplicationInsights-LocalForwarder/releases). There are a variety of ways to run local forwarder on multiple platforms.\n-\n-### Windows\n-\n-#### Windows Service\n-\n-The easiest way of running local forwarder under Windows is by installing it as a Windows Service. The release comes with a Windows Service executable (*WindowsServiceHost/Microsoft.LocalForwarder.WindowsServiceHost.exe*) which can be easily registered with the operating system.\n-\n-> [!NOTE]\n-> The local forwarder service requires a minimum of .NET Framework 4.7. If you do not have .NET Framework 4.7 the service will install, but it won't start. To access the lastest version of the .NET Framework **[visit the .NET Framework download page](\n-https://www.microsoft.com/net/download/dotnet-framework-runtime/net472?utm_source=getdotnet&utm_medium=referral)**.\n-\n-1. Download the LF.WindowsServiceHost.zip file from the [local forwarder release page](https://github.com/Microsoft/ApplicationInsights-LocalForwarder/releases) on GitHub.\n-\n-    ![Screenshot of local forwarder release download page](./media/opencensus-local-forwarder/001-local-forwarder-windows-service-host-zip.png)\n-\n-2. In this example for ease of demonstration, we will just extract the .zip file to the path `C:\\LF-WindowsServiceHost`.\n-\n-    To register the service and configure it to start at system boot run the following from the command line as Administrator:\n-\n-    ```\n-    sc create \"Local Forwarder\" binpath=\"C:\\LF-WindowsServiceHost\\Microsoft.LocalForwarder.WindowsServiceHost.exe\" start=auto\n-    ```\n-    \n-    You should receive a response of:\n-    \n-    `[SC] CreateService SUCCESS`\n-    \n-    To examine your new service via the Services GUI type ``services.msc``\n-        \n-     ![Screenshot of local forwarder service](./media/opencensus-local-forwarder/002-services.png)\n-\n-3. **Right-click** the new local forwarder and select **Start**. Your service will now enter a running state.\n-\n-4. By default the service is created without any recovery actions. You can **right-click** and select **Properties** > **Recovery** to configure automatic responses to a service failure.\n-\n-    Or if you prefer to set automatic recovery options programmatically for when failures occur, you can use:\n-\n-    ```\n-    sc failure \"Local Forwarder\" reset= 432000 actions= restart/1000/restart/1000/restart/1000\n-    ```\n-\n-5. In the same location as your ``Microsoft.LocalForwarder.WindowsServiceHost.exe`` file, which in this example is ``C:\\LF-WindowsServiceHost`` there is a file called ``LocalForwarder.config``. This is an xml based file that allows you to adjust the configuration of your localforwader and specify the instrumentation key of the Application Insights resource you want your distributed tracing data forwarded. \n-\n-    After editing the ``LocalForwarder.config`` file to add your instrumentation key, be sure to restart the **Local Forwarder Service** to allow your changes to take effect.\n-    \n-6. To confirm that your desired settings are in place and that the local forwarder is listening for trace data as expected check the ``LocalForwarder.log`` file. You should see results similar to the image below at the bottom of the file:\n-\n-    ![Screenshot of LocalForwarder.log file](./media/opencensus-local-forwarder/003-log-file.png)\n-\n-#### Console application\n-\n-For certain use cases, it might be beneficial to run local forwarder as a console application. The release comes with the following executable versions of the console host:\n-* a framework-dependent .NET Core binary */ConsoleHost/publish/Microsoft.LocalForwarder.ConsoleHost.dll*. Running this binary requires a .NET Core runtime to be installed; refer to this download [page](https://www.microsoft.com/net/download/dotnet-core/2.1) for details.\n-  ```batchfile\n-  E:\\uncdrop\\ConsoleHost\\publish>dotnet Microsoft.LocalForwarder.ConsoleHost.dll\n-  ```\n-* a self-contained .NET Core set of binaries for x86 and x64 platforms. These don't require .NET Core runtime to run. */ConsoleHost/win-x86/publish/Microsoft.LocalForwarder.ConsoleHost.exe*, */ConsoleHost/win-x64/publish/Microsoft.LocalForwarder.ConsoleHost.exe*.\n-  ```batchfile\n-  E:\\uncdrop\\ConsoleHost\\win-x86\\publish>Microsoft.LocalForwarder.ConsoleHost.exe\n-  E:\\uncdrop\\ConsoleHost\\win-x64\\publish>Microsoft.LocalForwarder.ConsoleHost.exe\n-  ```\n-\n-### Linux\n-\n-As with Windows, the release comes with the following executable versions of the console host:\n-* a framework-dependent .NET Core binary */ConsoleHost/publish/Microsoft.LocalForwarder.ConsoleHost.dll*. Running this binary requires a .NET Core runtime to be installed; refer to this download [page](https://www.microsoft.com/net/download/dotnet-core/2.1) for details.\n-\n-```batchfile\n-dotnet Microsoft.LocalForwarder.ConsoleHost.dll\n-```\n-\n-* a self-contained .NET Core set of binaries for linux-64. This one doesn't require .NET Core runtime to run. */ConsoleHost/linux-x64/publish/Microsoft.LocalForwarder.ConsoleHost*.\n-\n-```batchfile\n-user@machine:~/ConsoleHost/linux-x64/publish$ sudo chmod +x Microsoft.LocalForwarder.ConsoleHost\n-user@machine:~/ConsoleHost/linux-x64/publish$ ./Microsoft.LocalForwarder.ConsoleHost\n-```\n-\n-Many Linux users will want to run local forwarder as a daemon. Linux systems come with a variety of solutions for service management, like Upstart, sysv, or systemd. Whatever your particular version is, you can use it to run local forwarder in a way that is most appropriate for your scenario.\n-\n-As an example, let's create a daemon service using systemd. We'll use the framework-dependent version, but the same can be done for a self-contained one as well.\n-\n-* create the following service file named *localforwarder.service* and place it into */lib/systemd/system*.\n-This sample assumes your user name is SAMPLE_USER and you've copied local forwarder framework-dependent binaries (from */ConsoleHost/publish*) to */home/SAMPLE_USER/LOCALFORWARDER_DIR*.\n-\n-```\n-# localforwarder.service\n-# Place this file into /lib/systemd/system/\n-# Use 'systemctl enable localforwarder' to start the service automatically on each boot\n-# Use 'systemctl start localforwarder' to start the service immediately\n-\n-[Unit]\n-Description=Local Forwarder service\n-After=network.target\n-StartLimitIntervalSec=0\n-\n-[Service]\n-Type=simple\n-Restart=always\n-RestartSec=1\n-User=SAMPLE_USER\n-WorkingDirectory=/home/SAMPLE_USER/LOCALFORWARDER_DIR\n-ExecStart=/usr/bin/env dotnet /home/SAMPLE_USER/LOCALFORWARDER_DIR/Microsoft.LocalForwarder.ConsoleHost.dll noninteractive\n-\n-[Install]\n-WantedBy=multi-user.target\n-```\n-\n-* Run the following command to instruct systemd to start local forwarder on every boot\n-\n-```\n-systemctl enable localforwarder\n-```\n-\n-* Run the following command to instruct systemd to start local forwarder immediately\n-\n-```\n-systemctl start localforwarder\n-```\n-\n-* Monitor the service by inspecting **.log* files in the /home/SAMPLE_USER/LOCALFORWARDER_DIR directory.\n-\n-### Mac\n-Local forwarder may work with macOS, but it is currently not officially supported.\n-\n-### Self-hosting\n-Local forwarder is also distributed as a .NET Standard NuGet package, allowing you to host it inside your own .NET application.\n-\n-```csharp\n-using Library;\n-...\n-Host host = new Host();\n-\n-// see section below on configuring local forwarder\n-string configuration = ...;\n-    \n-host.Run(config, TimeSpan.FromSeconds(5));\n-...\n-host.Stop();\n-```\n-\n-## Configuring local forwarder\n-\n-* When running one of local forwarder's own hosts (Console Host or Windows Service Host), you will find **LocalForwarder.config** placed next to the binary.\n-* When self-hosting the local forwarder NuGet, the configuration of the same format must be provided in code (see section on self-hosting). For the configuration syntax, check the [LocalForwarder.config](https://github.com/Microsoft/ApplicationInsights-LocalForwarder/blob/master/src/ConsoleHost/LocalForwarder.config) in the GitHub repository. \n-\n-> [!NOTE]\n-> Configuration may change from release to release, so pay attention to which version you're using.\n-\n-## Monitoring local forwarder\n-\n-Traces are written out to the file system next to the executable that runs local forwarder (look for **.log* files). You can place a file with a name of *NLog.config* next to the executable to provide your own configuration in place of the default one. See [documentation](https://github.com/NLog/NLog/wiki/Configuration-file#configuration-file-format) for the description of the format.\n-\n-If no configuration file is provided (which is the default), Local forwarder will use the default configuration, which can be found [here](https://github.com/Microsoft/ApplicationInsights-LocalForwarder/blob/master/src/Common/NLog.config).\n-\n-## Next steps\n-\n-* [Open Census](https://opencensus.io/)"
  },
  {
    "Number": 109075,
    "Title": "fix error in telemetry initializer code",
    "ClosedAt": "2020-03-25T23:09:57Z",
    "User": "mrbullwinkle",
    "FileName": "articles/azure-monitor/app/api-filtering-sampling.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -483,7 +483,7 @@ The following sample initializer adds a custom property to every tracked telemet\n public void Initialize(ITelemetry item)\n {\n   var itemProperties = item as ISupportProperties;\n-  if(itemProperties != null && !itemProperties.ContainsKey(\"customProp\"))\n+  if(itemProperties != null && !itemProperties.Properties.ContainsKey(\"customProp\"))\n     {\n         itemProperties.Properties[\"customProp\"] = \"customValue\";\n     }"
  },
  {
    "Number": 109068,
    "Title": "3/25 PM Publish",
    "ClosedAt": "2020-03-25T22:03:16Z",
    "User": "huypub",
    "FileName": "articles/active-directory/app-provisioning/check-status-user-account-provisioning.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -19,7 +19,7 @@ ms.collection: M365-identity-device-management\n \n # Tutorial: Reporting on automatic user account provisioning\n \n-Azure Active Directory (Azure AD) includes a [user account provisioning service](user-provisioning.md) that helps automate the provisioning de-provisioning of user accounts in SaaS apps and other systems, for the purpose of end-to-end identity lifecycle management. Azure AD supports pre-integrated user provisioning connectors for all of the applications and systems in the \"Featured\" section of the [Azure AD application gallery](https://azuremarketplace.microsoft.com/marketplace/apps/category/azure-active-directory-apps?page=1&subcategories=featured).\n+Azure Active Directory (Azure AD) includes a [user account provisioning service](user-provisioning.md) that helps automate the provisioning de-provisioning of user accounts in SaaS apps and other systems, for the purpose of end-to-end identity lifecycle management. Azure AD supports pre-integrated user provisioning connectors for all of the applications and systems with user provisioning tutorials [here](https://docs.microsoft.com/azure/active-directory/saas-apps/tutorial-list).\n \n This article describes how to check the status of provisioning jobs after they have been set up, and how to troubleshoot the provisioning of individual users and groups.\n "
  },
  {
    "Number": 109068,
    "Title": "3/25 PM Publish",
    "ClosedAt": "2020-03-25T22:03:16Z",
    "User": "huypub",
    "FileName": "articles/active-directory/managed-identities-azure-resources/overview.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -12,7 +12,7 @@ ms.subservice: msi\n ms.devlang:\n ms.topic: overview\n ms.custom: mvc\n-ms.date: 09/26/2019\n+ms.date: 03/25/2020\n ms.author: markvi\n \n #As a developer, I'd like to securely manage the credentials that my application uses for authenticating to cloud services without having the credentials in my code or checked into source control."
  },
  {
    "Number": 109068,
    "Title": "3/25 PM Publish",
    "ClosedAt": "2020-03-25T22:03:16Z",
    "User": "huypub",
    "FileName": "articles/active-directory/managed-identities-azure-resources/services-support-managed-identities.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -27,8 +27,8 @@ The following Azure services support managed identities for Azure resources:\n \n | Managed identity type | All Generally Available<br>Global Azure Regions | Azure Government | Azure Germany | Azure China 21Vianet |\n | --- | :-: | :-: | :-: | :-: |\n-| System assigned | ![Available][check] | Preview | Preview | Preview | \n-| User assigned | ![Available][check] | Preview | Preview | Preview |\n+| System assigned | ![Available][check] | ![Available][check] | Preview | Preview | \n+| User assigned | ![Available][check] | ![Available][check] | Preview | Preview |\n \n Refer to the following list to configure managed identity for Azure Virtual Machines (in regions where available):\n "
  },
  {
    "Number": 109068,
    "Title": "3/25 PM Publish",
    "ClosedAt": "2020-03-25T22:03:16Z",
    "User": "huypub",
    "FileName": "articles/active-directory/user-help/user-help-auth-app-faq.md",
    "Addition": 2,
    "Delections": 1,
    "Changes": 3,
    "Patch": "@@ -10,7 +10,7 @@ ms.service: active-directory\n ms.workload: identity\n ms.subservice: user-help\n ms.topic: conceptual\n-ms.date: 03/09/2020\n+ms.date: 03/25/2020\n ms.author: curtand\n ms.reviewer: olhaun\n ---\n@@ -25,6 +25,7 @@ The Microsoft Authenticator app replaced the Azure Authenticator app, and is the\n \n | Question | Solution |\n | -------- | -------- |\n+| Can I take screenshots of my OTP codes on the Android Microsoft Authenticator? | As of Microsoft Authenticator Android release 6.2003.1704, by default, when a screenshot of the Authenticator is taken, all OTP codes are hidden to better protect our users. If a user would like to see their OTP codes in screenshots or allow other apps to capture the Authenticator's screen, they can do so by enabling the \"Screen Capture\" setting in their Authenticator app and then restarting the app. |\n | What data does the Authenticator store on my behalf and how can I delete it? | The Microsoft Authenticator app collects three types of information:<ul><li>Account info you provide when you add your account. This data can be removed by removing your account.</li><li>Diagnostic log data that stays only in the app until you select **Send Logs** the app's **Help** menu to send logs to Microsoft. These log files contain personal data, like your email addresses (such as, alain@contoso.com), server or IP addresses, and device data (such as, device name and operating system version), with the personal data limited to info necessary to help troubleshoot app issues. You can view these log files in the app at any time to see the info being gathered. If you send the log files, the Authentication app engineers can use it to troubleshoot customer-reported issues.</li><li>Non-personally identifiable usage data, such \"started add account flow/successfully added account,\" or \"notification approved.\" This data is an integral part of our engineering decisions and helps us determine what features are important to you, and where improvements need to be made in the form of updates to the apps. You, as an app user, see a notification of this data collection on first launch of the app, and are informed that it can be turned off on the app's **Settings** page. You can enable or disable this setting at any time.</li></ul> |\n | What are the codes in the app for? | When you open the Microsoft Authenticator app, you'll see your added accounts as tiles.<li>On an iOS device, your work or school accounts and your personal Microsoft accounts will have six – or eight - digit numbers visible in the full screen view of the account (accessed by tapping on the account tile).<br><br>![Accounts screen in the app](./media/user-help-auth-app-faq/auth-app-accounts.png)<li>For other accounts on an iOS device and all accounts on an Android device, you'll see a six- or eight-digit numbers in the Accounts page of the app. You'll use these codes as verification that you are who you say you are. After you sign in with your username and password, you'll type in the verification code that's associated with that account. For example, if you're Katy signing in to your Contoso account and you are using an iOS device, you'd tap on the account tile and then use the verification code to verify your identity. If you are Katy signing into your Outlook account, you'd follow the same steps.<br><br>![after tapping the account tile in the app](./media/user-help-auth-app-faq/katy-signin.png)<br><br>After tapping the Contoso account tile, Katy sees the verification code in the full screen view and enters 895823 to finish signing in.<br><br>![verification code screen in the app](./media/user-help-auth-app-faq/verification-code.png) |\n | Why does the number next to the code keep counting down? | You might see a 30-second timer counting down next to your active verification code. This timer is so that you never sign in using the same code twice. Unlike a password, we don't want you to remember this number. The idea is that only someone with access to your phone knows your code. |"
  },
  {
    "Number": 109068,
    "Title": "3/25 PM Publish",
    "ClosedAt": "2020-03-25T22:03:16Z",
    "User": "huypub",
    "FileName": "articles/active-directory/users-groups-roles/directory-assign-admin-roles.md",
    "Addition": 1,
    "Delections": 2,
    "Changes": 3,
    "Patch": "@@ -65,8 +65,7 @@ Users in this role can create application registrations when the \"Users can regi\n \n ### [Authentication Administrator](#authentication-administrator-permissions)\n \n-The Authentication administrator role is currently in public preview. Users with this role can set or reset non-password credentials and can update passwords for all users. Authentication Administrators can require users to re-register against existing non-password credential (for example, MFA or FIDO) and revoke **remember MFA on the device**, which prompts for MFA on the next sign-in of users who are non-administrators or assigned the following roles only:\n-\n+Users in this role can set or reset non-password credentials, update passwords, require to re-register against existing non-password credential (for example, MFA or FIDO) and revoke **remember MFA on the device** (which prompts for MFA on the next sign-in) of users who are non-administrators or assigned the following roles only:\n * Authentication Administrator\n * Directory Readers\n * Guest Inviter"
  },
  {
    "Number": 109068,
    "Title": "3/25 PM Publish",
    "ClosedAt": "2020-03-25T22:03:16Z",
    "User": "huypub",
    "FileName": "articles/active-directory/users-groups-roles/roles-delegate-by-task.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -4,7 +4,7 @@ description: Roles to delegate for identity tasks in Azure Active Directory\n services: active-directory\n documentationcenter: ''\n author: curtand\n-manager: mtillman\n+manager: daveba\n ms.service: active-directory\n ms.workload: identity\n ms.subservice: users-groups-roles"
  },
  {
    "Number": 109068,
    "Title": "3/25 PM Publish",
    "ClosedAt": "2020-03-25T22:03:16Z",
    "User": "huypub",
    "FileName": "articles/aks/node-auto-repair.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -11,7 +11,7 @@ ms.date: 03/10/2020\n AKS continuously checks the health state of worker nodes and performs automatic repair of the nodes if they become unhealthy. This documentation describes how Azure Kubernetes Service (AKS) monitors worker nodes, and repairs unhealthy worker nodes.  The documentation is to inform AKS operators on the behavior of node repair functionality. It is also important to note that Azure platform [performs maintenance on Virtual Machines][vm-updates] that experience issues. AKS and Azure work together to minimize service disruptions for your clusters.\n \n > [!Important]\n-> Noe auto-repair functionality isn't currently supported for Windows Server node pools.\n+> Node auto-repair functionality isn't currently supported for Windows Server node pools.\n \n ## How AKS checks for unhealthy nodes\n \n@@ -51,4 +51,4 @@ Use [Availability Zones][availability-zones] to increase high availability with\n \n <!-- LINKS - Internal -->\n [availability-zones]: ./availability-zones.md\n-[vm-updates]: ../virtual-machines/maintenance-and-updates.md\n\\ No newline at end of file\n+[vm-updates]: ../virtual-machines/maintenance-and-updates.md"
  },
  {
    "Number": 109068,
    "Title": "3/25 PM Publish",
    "ClosedAt": "2020-03-25T22:03:16Z",
    "User": "huypub",
    "FileName": "articles/application-gateway/ssl-overview.md",
    "Addition": 3,
    "Delections": 3,
    "Changes": 6,
    "Patch": "@@ -17,7 +17,7 @@ Secure Sockets Layer (SSL) is the standard security technology for establishing\n \n Application Gateway supports SSL termination at the gateway, after which traffic typically flows unencrypted to the backend servers. There are a number of advantages of doing SSL termination at the application gateway:\n \n-- **Improved performance** – The biggest performance hit when doing SSL decryption is the initial handshake. To improve performance, the server doing the decryption caches SSL session IDs and manages TLS session tickets. If this is done at the application gateway, all requests from the same client can use the cached values. If it’s done on the backend servers, then each time the client’s requests go to a different server the client has to re‑authenticate. The use of TLS tickets can help mitigate this issue, but they are not supported by all clients and can be difficult to configure and manage.\n+- **Improved performance** – The biggest performance hit when doing SSL decryption is the initial handshake. To improve performance, the server doing the decryption caches SSL session IDs and manages TLS session tickets. If this is done at the application gateway, all requests from the same client can use the cached values. If it’s done on the backend servers, then each time the client’s requests go to a different server the client must re‑authenticate. The use of TLS tickets can help mitigate this issue, but they are not supported by all clients and can be difficult to configure and manage.\n - **Better utilization of the backend servers** – SSL/TLS processing is very CPU intensive, and is becoming more intensive as key sizes increase. Removing this work from the backend servers allows them to focus on what they are most efficient at, delivering content.\n - **Intelligent routing** – By decrypting the traffic, the application gateway has access to the request content, such as headers, URI, and so on, and can use this data to route requests.\n - **Certificate management** – Certificates only need to be purchased and installed on the application gateway and not all backend servers. This saves both time and money.\n@@ -38,7 +38,7 @@ For the SSL connection to work, you need to ensure that the SSL certificate meet\n Application gateway supports the following types of certificates:\n \n - CA (Certificate Authority) certificate: A CA certificate is a digital certificate issued by a certificate authority (CA)\n-- EV (Extended Validation) certificate: An EV certificate is an industry standard certificate guidelines. This will turn the browser locator bar green and publish company name as well.\n+- EV (Extended Validation) certificate: An EV certificate is a certificate that conforms to industry standard certificate guidelines. This will turn the browser locator bar green and publish the company name as well.\n - Wildcard Certificate: This certificate supports any number of subdomains based on *.site.com, where your subdomain would replace the *. It doesn’t, however, support site.com, so in case the users are accessing your website without typing the leading \"www\", the wildcard certificate will not cover that.\n - Self-Signed certificates: Client browsers do not trust these certificates and will warn the user that the virtual service’s certificate is not part of a trust chain. Self-signed certificates are good for testing or environments where administrators control the clients and can safely bypass the browser’s security alerts. Production workloads should never use self-signed certificates.\n \n@@ -90,7 +90,7 @@ Authentication Certificates have been deprecated and replaced by Trusted Root Ce\n    \n > [!NOTE] \n >\n-> In order for an SSL certificate to be trusted, that certificate of the backend server must have been issued by a CA that is included in the trusted store of the Application Gateway.If the certificate was not issued by a trusted CA, the Application Gateway will then check to see if the certificate of the issuing CA was issued by a trusted CA, and so on until either a trusted CA is found (at which point a trusted, secure connection will be established) or no trusted CA can be found (at which point the Application Gateway will mark the backend unhealthy). Therefore, it is recommended the backend server certificate contain both the root and intermidiate CAs.\n+> In order for an SSL certificate to be trusted, that certificate of the backend server must have been issued by a CA that is included in the trusted store of the Application Gateway.If the certificate was not issued by a trusted CA, the Application Gateway will then check to see if the certificate of the issuing CA was issued by a trusted CA, and so on until either a trusted CA is found (at which point a trusted, secure connection will be established) or no trusted CA can be found (at which point the Application Gateway will mark the backend unhealthy). Therefore, it is recommended the backend server certificate contain both the root and intermediate CAs.\n \n - If the certificate is self-signed, or signed by unknown intermediaries, then to enable end to end SSL in v2 SKU a trusted root certificate must be defined. Application Gateway will only communicate with backends whose Server certificate’s root certificate matches one of the list of trusted root certificates in the backend http setting associated with the pool.\n "
  },
  {
    "Number": 109068,
    "Title": "3/25 PM Publish",
    "ClosedAt": "2020-03-25T22:03:16Z",
    "User": "huypub",
    "FileName": "articles/automation/automation-dsc-onboarding.md",
    "Addition": 106,
    "Delections": 140,
    "Changes": 246,
    "Patch": "@@ -15,18 +15,11 @@ manager: carmonm\n ## Why manage machines with Azure Automation State Configuration?\n \n Azure Automation State Configuration is a configuration management service\n-for Desired State Configuration (DSC) nodes in any cloud or on-premises datacenter.\n-It enables scalability across thousands of machines quickly and easily from a central, secure location.\n-You can easily onboard machines,\n-assign them declarative configurations,\n-and view reports showing each machine's\n-compliance to the desired state you specified.\n-The Azure Automation State Configuration service is to DSC\n-what Azure Automation runbooks are to PowerShell scripting.\n-In other words, in the same way that Azure Automation helps you manage PowerShell scripts,\n-it also helps you manage DSC configurations.\n-To learn more about the benefits of using Azure Automation State Configuration, see\n-[Azure Automation State Configuration overview](automation-dsc-overview.md).\n+for Desired State Configuration (DSC) nodes in any cloud or on-premises datacenter. It's accessed in the Azure portal by selecting **State configuration (DSC)** under **Configuration Management**. \n+\n+This service enables scalability across thousands of machines quickly and easily from a central, secure location. You can easily onboard machines, assign them declarative configurations, and view reports showing each machine's compliance with the desired state you specify.\n+\n+The Azure Automation State Configuration service is to DSC what Azure Automation runbooks are to PowerShell scripting. In other words, in the same way that Azure Automation helps you manage PowerShell scripts, it also helps you manage DSC configurations. To learn more about the benefits of using Azure Automation State Configuration, see [Azure Automation State Configuration overview](automation-dsc-overview.md).\n \n Azure Automation State Configuration can be used to manage a variety of machines:\n \n@@ -35,105 +28,91 @@ Azure Automation State Configuration can be used to manage a variety of machines\n - Physical/virtual Windows machines on-premises, or in a cloud other than Azure (including AWS EC2 instances)\n - Physical/virtual Linux machines on-premises, in Azure, or in a cloud other than Azure\n \n-In addition, if you are not ready to manage machine configuration from the cloud, Azure Automation\n-State Configuration can also be used as a report-only endpoint.\n-This allows you to set (push) configurations through DSC and view reporting details in Azure Automation.\n+If you are not ready to manage machine configuration from the cloud, you can use Azure Automation State Configuration as a report-only endpoint. This feature allows you to set (push) configurations through DSC and view reporting details in Azure Automation.\n \n > [!NOTE]\n-> Managing Azure VMs with State Configuration is included at no extra charge if the virtual machine DSC extension installed is greater than 2.70. For more information, see [**Automation pricing page**](https://azure.microsoft.com/pricing/details/automation/).\n+> Managing Azure VMs with Azure Automation State Configuration is included at no extra charge if the installed Azure VM Desired State Configuration extension version is greater than 2.70. For more information, see [**Automation pricing page**](https://azure.microsoft.com/pricing/details/automation/).\n+\n+The following sections of this article outline how you can onboard the machines listed above to Azure Automation State Configuration.\n \n-The following sections outline how you can onboard each type of machine to Azure Automation State Configuration.\n+## Onboarding Azure VMs\n+\n+Azure Automation State Configuration lets you easily onboard Azure VMs for configuration management, using the Azure portal, Azure Resource Manager templates, or PowerShell. Under the hood, and without an administrator having to remote into a VM, the Azure VM Desired State Configuration extension registers the VM with Azure Automation State Configuration. Since the Azure extension runs asynchronously, steps to track its progress or troubleshoot it are provided in the [Troubleshooting Azure virtual machine onboarding](#troubleshooting-azure-virtual-machine-onboarding) section of this article.\n \n > [!NOTE]\n->Deploying DSC to a Linux node uses the `/tmp` folder and modules like **nxAutomation** are temporarily downloaded for verification before installing them in their appropriate location. To ensure the modules install correctly, the Log Analytics agent for Linux needs read/write permission on `/tmp` folder. The Log Analytics agent for Linux runs as the `omsagent` user. \n->\n->To grant write permission to `omsagent` user, run the following commands: `setfacl -m u:omsagent:rwx /tmp`\n->\n+>Deploying DSC to a Linux node uses the **/tmp** folder. Modules such as `nxautomation` are temporarily downloaded for verification before installing them in their appropriate locations. To ensure that modules install correctly, the Log Analytics agent for Linux needs read/write permissions on the **/tmp** folder.<br><br>\n+>The Log Analytics agent for Linux runs as the `omsagent` user. To grant >write permission to the `omsagent` user, run the command `setfacl -m u:omsagent:rwx /tmp`.\n \n-## Azure virtual machines\n+### Onboard a VM using Azure portal\n \n-Azure Automation State Configuration lets you easily onboard Azure virtual machines for\n-configuration management, using either the Azure portal, Azure Resource Manager templates, or\n-PowerShell. Under the hood, and without an administrator having to remote into the VM, the Azure VM\n-Desired State Configuration extension registers the VM with Azure Automation State Configuration.\n-Since the Azure VM Desired State Configuration extension runs asynchronously, steps to track its\n-progress or troubleshoot it are provided in the following [**Troubleshooting Azure virtual machine onboarding**](#troubleshooting-azure-virtual-machine-onboarding) section.\n+To onboard an Azure VM to Azure Automation State Configuration through the [Azure portal](https://portal.azure.com/):\n \n-### Azure portal\n+1. Navigate to the Azure Automation account in which to onboard VMs. \n \n-In the [Azure portal](https://portal.azure.com/), navigate to the Azure Automation account where\n-you want to onboard virtual machines. On the State Configuration page and the **Nodes** tab, click\n-**+ Add**.\n+2. On the State Configuration page, select the **Nodes** tab, then click\n+**Add**.\n \n-Select an Azure virtual machine to onboard.\n+3. Choose a VM to onboard.\n \n-If the machine does not have the PowerShell desired state extension installed and the power state is running, click **Connect**.\n+4. If the machine doesn't have the PowerShell desired state extension installed and the power state is running, click **Connect**.\n \n-Under **Registration**, enter the [PowerShell DSC Local Configuration Manager values](/powershell/scripting/dsc/managing-nodes/metaConfig)\n-required for your use case, and optionally a node configuration to assign to the VM.\n+5. Under **Registration**, enter the [PowerShell DSC Local Configuration Manager values](/powershell/scripting/dsc/managing-nodes/metaConfig)\n+required for your use case. Optionally, you can enter a node configuration to assign to the VM.\n \n ![onboarding](./media/automation-dsc-onboarding/DSC_Onboarding_6.png)\n \n-### Azure Resource Manager templates\n+### Onboard a VM using Azure Resource Manager templates\n+\n+You can deploy and onboard a VM to Azure Automation State Configuration using Azure Resource Manager templates. See [Server managed by Desired State Configuration service](https://azure.microsoft.com/resources/templates/101-automation-configuration/) for an example template that onboards an existing VM to Azure Automation State Configuration. If you are managing a VM Scale Set, see the example template in [VM Scale Set Configuration managed by Azure Automation](https://azure.microsoft.com/resources/templates/201-vmss-automation-dsc/).\n \n-Azure virtual machines can be deployed and onboarded to Azure Automation State Configuration via\n-Azure Resource Manager templates. See [Server managed by Desired State Configuration service](https://azure.microsoft.com/resources/templates/101-automation-configuration/)\n-for an example template that onboards an existing VM to Azure Automation State Configuration.\n-If you are managing a Virtual Machine Scale Set, see the example template\n-[Virtual machine scale set Configuration managed by Azure Automation](https://azure.microsoft.com/resources/templates/201-vmss-automation-dsc/).\n+### Onboard machines using PowerShell\n \n-### PowerShell\n+You can use the [Register-AzAutomationDscNode](/powershell/module/az.automation/register-azautomationdscnode) cmdlet in PowerShell to onboard VMs to Azure Automation State Configuration. \n \n-The [Register-AzAutomationDscNode](/powershell/module/az.automation/register-azautomationdscnode)\n-cmdlet can be used to onboard virtual machines in Azure by using PowerShell.\n-However, this is currently only implemented for machines running Windows (the cmdlet\n-only triggers the Windows extension).\n+> [!NOTE]\n+>The `Register-AzAutomationDscNode` cmdlet is implemented currently only for machines running Windows, as it triggers just the Windows extension.\n+\n+### Register VMs across Azure subscriptions\n \n-### Registering virtual machines across Azure subscriptions\n+The best way to register VMs from other Azure subscriptions is to use the DSC extension in an Azure Resource Manager deployment template. Examples are provided in [Desired State Configuration extension with Azure Resource Manager templates](https://docs.microsoft.com/azure/virtual-machines/extensions/dsc-template).\n \n-The best way to register virtual machines from other Azure subscriptions is to use the DSC extension\n-in an Azure Resource Manager deployment template.\n-Examples are provided in\n-[Desired State Configuration extension with Azure Resource Manager templates](https://docs.microsoft.com/azure/virtual-machines/extensions/dsc-template).\n-To find the registration key and registration URL to use as parameters in the template,\n-see the following [**Secure registration**](#secure-registration) section.\n+To find the registration key and registration URL to use as parameters in the template, see the [Onboarding securely using registration](#onboarding-securely-using-registration) section in this article.\n \n-## Physical/virtual Windows machines on-premises, or in a cloud other than Azure (including AWS EC2 instances)\n+## Onboarding physical/virtual Windows machines on-premises, or in a cloud other than Azure (including AWS EC2 instances)\n \n-Windows servers running on-premises or in other cloud environments\n-can also be onboarded to Azure Automation State Configuration, as long as they have\n-[outbound access to Azure](automation-dsc-overview.md#network-planning):\n+You can onboard Windows servers running on-premises or in other cloud environments to Azure Automation State Configuration. The servers must have [outbound access to Azure](automation-dsc-overview.md#network-planning).\n \n-1. Make sure the latest version of [WMF 5](https://aka.ms/wmf5latest) is installed on the machines you want to onboard to Azure Automation State Configuration.\n-1. Follow the directions in following section [**Generating DSC metaconfigurations**](#generating-dsc-metaconfigurations) to generate a folder containing the needed DSC metaconfigurations.\n-1. Remotely apply the PowerShell DSC metaconfiguration to the machines you want to onboard. **The machine this command is run from must have the latest version of [WMF 5](https://aka.ms/wmf5latest) installed**:\n+1. Make sure that the latest version of [WMF 5](https://aka.ms/wmf5latest) is installed on the machines to onboard to Azure Automation State Configuration. In addition, WMF 5 must be installed on the computer that you are using for the onboarding operation.\n+1. Follow the directions in the section [Generating DSC metaconfigurations](#generating-dsc-metaconfigurations) to create a folder containing the required DSC metaconfigurations. \n+1. Use the following cmdlet to apply the PowerShell DSC metaconfigurations remotely to the machines that you want to onboard. \n \n    ```powershell\n    Set-DscLocalConfigurationManager -Path C:\\Users\\joe\\Desktop\\DscMetaConfigs -ComputerName MyServer1, MyServer2\n    ```\n \n-1. If you cannot apply the PowerShell DSC metaconfigurations remotely, copy the metaconfigurations folder from step 2 onto each machine to onboard. Then call **Set-DscLocalConfigurationManager** locally on each machine to onboard.\n-1. Using the Azure portal or cmdlets, check that the machines to onboard appear as State Configuration nodes registered in your Azure Automation account.\n+1. If you can't apply the PowerShell DSC metaconfigurations remotely, copy the **metaconfigurations** folder to the machines that you are onboarding. Then add code to call `Set-DscLocalConfigurationManager` locally on the machines.\n+1. Using the Azure portal or cmdlets, verify that the machines to onboard appear as a State Configuration nodes registered in your Azure Automation account.\n \n-## Physical/virtual Linux machines on-premises, or in a cloud other than Azure\n+## Onboarding physical/virtual Linux machines on-premises, or in a cloud other than Azure\n \n-Linux servers running on-premises or in other cloud environments\n-can also be onboarded to Azure Automation State Configuration, as long as they have\n-[outbound access to Azure](automation-dsc-overview.md#network-planning):\n+You can onboard Linux servers running on-premises or in other cloud environments to Azure Automation State Configuration. The servers must have [outbound access to Azure](automation-dsc-overview.md#network-planning).\n \n-1. Make sure the latest version of [PowerShell Desired State Configuration for Linux](https://github.com/Microsoft/PowerShell-DSC-for-Linux) is installed on the machines you want to onboard to Azure Automation State Configuration.\n-2. If the [PowerShell DSC Local Configuration Manager defaults](/powershell/scripting/dsc/managing-nodes/metaConfig4) match your use case, and you want to onboard machines such that they **both** pull from and report to Azure Automation State Configuration:\n+1. Make sure that the latest version of [PowerShell Desired State Configuration for Linux](https://github.com/Microsoft/PowerShell-DSC-for-Linux) is installed on the machines to onboard to Azure Automation State Configuration.\n+2. If the [PowerShell DSC Local Configuration Manager defaults](/powershell/scripting/dsc/managing-nodes/metaConfig4) match your use case, and you want to onboard machines so that they both pull from and report to Azure Automation State Configuration:\n \n-   - On each Linux machine to onboard to Azure Automation State Configuration, use `Register.py` to onboard using the PowerShell DSC Local Configuration Manager defaults:\n+   - On each Linux machine to onboard to Azure Automation State Configuration, use `Register.py` to onboard using the PowerShell DSC Local Configuration Manager defaults.\n \n      `/opt/microsoft/dsc/Scripts/Register.py <Automation account registration key> <Automation account registration URL>`\n \n-   - To find the registration key and registration URL for your Automation account, see the following [**Secure registration**](#secure-registration) section.\n+   - To find the registration key and registration URL for your Automation account, see the [Onboarding securely using registration](#onboarding-securely-using-registration) section if this article.\n+\n+3. If the PowerShell DSC Local Configuration Manager (LCM) defaults don't match your use case, or you want to onboard machines that only report to Azure Automation State Configuration, follow steps 4-7. Otherwise, proceed directly to step 7.\n \n-     If the PowerShell DSC Local Configuration Manager defaults **do not** match your use case, or you want to onboard machines such that they only report to Azure Automation State Configuration, follow steps 3 - 6. Otherwise, proceed directly to step 6.\n+4. Follow the directions in the [Generating DSC metaconfigurations](#generating-dsc-metaconfigurations) section to produce a folder containing the required DSC metaconfigurations.\n \n-3. Follow the directions in the following [**Generating DSC metaconfigurations**](#generating-dsc-metaconfigurations) section to generate a folder containing the needed DSC metaconfigurations.\n-4. Remotely apply the PowerShell DSC metaconfiguration to the machines you want to onboard:\n+5. Make sure that the latest version of [WMF 5](https://aka.ms/wmf5latest) is installed on the machine being used for onboarding.\n+\n+6. Add code as follows to apply the PowerShell DSC metaconfigurations remotely to the machines that you want to onboard.\n \n     ```powershell\n     $SecurePass = ConvertTo-SecureString -String '<root password>' -AsPlainText -Force\n@@ -146,32 +125,30 @@ can also be onboarded to Azure Automation State Configuration, as long as they h\n     Set-DscLocalConfigurationManager -CimSession $Session -Path C:\\Users\\joe\\Desktop\\DscMetaConfigs\n     ```\n \n-The machine this command is run from must have the latest version of [WMF 5](https://aka.ms/wmf5latest) installed.\n+7. If you can't apply the PowerShell DSC metaconfigurations remotely, copy the metaconfigurations corresponding to the remote machines from the folder described in step 4 to the Linux machines.\n \n-1. If you cannot apply the PowerShell DSC metaconfigurations remotely, copy the metaconfiguration corresponding to that machine from the folder in step 5 onto the Linux machine. Then call `SetDscLocalConfigurationManager.py` locally on each Linux machine you want to onboard to Azure Automation State Configuration:\n+8. Add code to call `Set-DscLocalConfigurationManager.py` locally on each Linux machine to onboard to Azure Automation State Configuration.\n \n    `/opt/microsoft/dsc/Scripts/SetDscLocalConfigurationManager.py -configurationmof <path to metaconfiguration file>`\n \n-2. Using the Azure portal or cmdlets, check that the machines to onboard now show up as DSC nodes registered in your Azure Automation account.\n+9. Using the Azure portal or cmdlets, ensure that the machines to onboard now show up as DSC nodes registered in your Azure Automation account.\n \n ## Generating DSC metaconfigurations\n \n-To generically onboard any machine to Azure Automation State Configuration, a [DSC metaconfiguration](/powershell/scripting/dsc/managing-nodes/metaConfig)\n-can be generated that tells the DSC\n-agent to pull from and/or report to Azure Automation State Configuration. DSC\n-metaconfigurations for Azure Automation State Configuration can be generated using either a\n-PowerShell DSC configuration, or the Azure Automation PowerShell cmdlets.\n+To onboard any machine to Azure Automation State Configuration, you can generate a [DSC metaconfiguration](/powershell/scripting/dsc/managing-nodes/metaConfig). This configuration tells the DSC agent to pull from and/or report to Azure Automation State Configuration. You can generate a DSC metaconfiguration for Azure Automation State Configuration using either a PowerShell DSC configuration or the Azure Automation PowerShell cmdlets.\n \n > [!NOTE]\n > DSC metaconfigurations contain the secrets needed to onboard a machine to an Automation account for management. Make sure to properly protect any DSC metaconfigurations you create, or delete them after use.\n \n-### Using a DSC Configuration\n+Proxy support for metaconfigurations is controlled by LCM, which is the Windows PowerShell DSC engine. The LCM runs on all target nodes and is responsible for calling the configuration resources that are included in a DSC metaconfiguration script. You can include proxy support in a metaconfiguration by including definitions of the proxy URL and the proxy credential as needed in the `ConfigurationRepositoryWeb`, `ResourceRepositoryWeb`, and `ReportServerWeb` blocks. See [Configuring the Local Configuration Manager](https://docs.microsoft.com/powershell/scripting/dsc/managing-nodes/metaconfig?view=powershell-7).\n+\n+### Generate DSC metaconfigurations using a DSC configuration\n \n-1. Open the VSCode (or your favorite editor) as an administrator in a machine in your local environment. The machine must have the latest version of [WMF 5](https://aka.ms/wmf5latest) installed.\n+1. Open VSCode (or your favorite editor) as an administrator on a machine in your local environment. The machine must have the latest version of [WMF 5](https://aka.ms/wmf5latest) installed.\n 1. Copy the following script locally. This script contains a PowerShell DSC configuration for creating metaconfigurations, and a command to kick off the metaconfiguration creation.\n \n-> [!NOTE]\n-> State Configuration Node Configuration names are case sensitive in the portal. If the case is mismatched the node will not show up under the **Nodes** tab.\n+    > [!NOTE]\n+    > State Configuration Node Configuration names are case-sensitive in the Azure portal. If the case is mismatched, the node will not show up under the **Nodes** tab.\n \n    ```powershell\n    # The DSC configuration that will generate metaconfigurations\n@@ -248,8 +225,8 @@ PowerShell DSC configuration, or the Azure Automation PowerShell cmdlets.\n \n                 ResourceRepositoryWeb AzureAutomationStateConfiguration\n                 {\n-                ServerUrl       = $RegistrationUrl\n-                RegistrationKey = $RegistrationKey\n+                    ServerUrl       = $RegistrationUrl\n+                    RegistrationKey = $RegistrationKey\n                 }\n             }\n \n@@ -283,24 +260,26 @@ PowerShell DSC configuration, or the Azure Automation PowerShell cmdlets.\n    DscMetaConfigs @Params\n    ```\n \n-1. Fill in the registration key and URL for your Automation account, as well as the names of the machines to onboard. All other parameters are optional. To find the registration key and registration URL for your Automation account, see the following [**Secure registration**](#secure-registration) section.\n-1. If you want the machines to report DSC status information to Azure Automation State Configuration, but not pull configuration or PowerShell modules, set the **ReportOnly** parameter to true.\n-1. Run the script. You should now have a folder called **DscMetaConfigs** in your working directory, containing the PowerShell DSC metaconfigurations for the machines to onboard (as an administrator):\n+1. Fill in the registration key and URL for your Automation account, as well as the names of the machines to onboard. All other parameters are optional. To find the registration key and registration URL for your Automation account, see the [Onboarding securely using registration](#onboarding-securely-using-registration) section.\n+\n+1. If you want the machines to report DSC status information to Azure Automation State Configuration, but not pull configuration or PowerShell modules, set the `ReportOnly` parameter to true.\n+\n+1. If `ReportOnly` is not set, the machines report DSC status information to Azure Automation State Configuration and pull configuration or PowerShell modules. Set parameters accordingly in the `ConfigurationRepositoryWeb`, `ResourceRepositoryWeb`, and `ReportServerWeb` blocks.\n+\n+1. Run the script. You should now have a working directory folder called **DscMetaConfigs**, containing the PowerShell DSC metaconfigurations for the machines to onboard (as an administrator).\n \n     ```powershell\n     Set-DscLocalConfigurationManager -Path ./DscMetaConfigs\n     ```\n \n-### Using the Azure Automation cmdlets\n+### Generate DSC metaconfigurations using Azure Automation cmdlets\n \n-If the PowerShell DSC Local Configuration Manager defaults match your use case, and you want to\n-onboard machines such that they both pull from and report to Azure Automation State Configuration,\n-the Azure Automation cmdlets provide a simplified method of generating the DSC metaconfigurations\n-needed:\n+If PowerShell DSC LCM defaults match your use case and you want to\n+onboard machines to both pull from and report to Azure Automation State Configuration, you can generate the needed DSC metaconfigurations more simply using the Azure Automation cmdlets.\n \n-1. Open the PowerShell console or VSCode as an administrator in a machine in your local environment.\n+1. Open the PowerShell console or VSCode as an administrator on a machine in your local environment.\n 2. Connect to Azure Resource Manager using `Connect-AzAccount`\n-3. Download the PowerShell DSC metaconfigurations for the machines you want to onboard from the Automation account to which you want to onboard nodes:\n+3. Download the PowerShell DSC metaconfigurations for the machines you want to onboard from the Automation account in which you are setting up nodes.\n \n    ```powershell\n    # Define the parameters for Get-AzAutomationDscOnboardingMetaconfig using PowerShell Splatting\n@@ -315,71 +294,58 @@ needed:\n    Get-AzAutomationDscOnboardingMetaconfig @Params\n    ```\n \n-1. You should now have a folder called ***DscMetaConfigs***, containing the PowerShell DSC metaconfigurations for the machines to onboard (as an administrator):\n+1. You should now have a folder called **DscMetaConfigs**, containing the PowerShell DSC metaconfigurations for the machines to onboard (as an administrator).\n \n     ```powershell\n     Set-DscLocalConfigurationManager -Path $env:UserProfile\\Desktop\\DscMetaConfigs\n     ```\n \n-## Secure registration\n+## Onboarding securely using registration\n \n-Machines can securely onboard to an Azure Automation account through the WMF 5 DSC registration\n-protocol, which allows a DSC node to authenticate to a PowerShell DSC Pull or Reporting server\n-(including Azure Automation State Configuration). The node registers to the server at a\n-**Registration URL**, authenticating using a **Registration key**. During registration, the DSC\n-node and DSC Pull/Reporting server negotiate a unique certificate for this node to use for\n-authentication to the server post-registration. This process prevents onboarded nodes from\n-impersonating one another, such as if a node is compromised and behaving maliciously. After\n-registration, the Registration key is not used for authentication again, and is deleted from the\n-node.\n+Machines can securely onboard to an Azure Automation account through the WMF 5 DSC registration protocol. This protocol allows a DSC node to authenticate to a PowerShell DSC pull or report server, including Azure Automation State Configuration. The node registers with the server at the registration URL and authenticates using a registration key. During registration, the DSC node and DSC pull/report server negotiate a unique certificate for the node to use for authentication to the server post-registration. This process prevents onboarded nodes from\n+impersonating one another, for example, if a node is compromised and behaving maliciously. After registration, the registration key is not used for authentication again, and is deleted from the node.\n \n-You can get the information required for the State Configuration registration protocol from\n-**Keys** under **Account Settings** in the Azure portal. Open this blade by clicking the key icon\n-on the **Essentials** panel for the Automation account.\n+You can get the information required for the State Configuration registration protocol from **Keys** under **Account Settings** in the Azure portal. \n \n ![Azure automation keys and URL](./media/automation-dsc-onboarding/DSC_Onboarding_4.png)\n \n-- Registration URL is the URL field in the Manage Keys blade.\n-- Registration key is the Primary Access Key or Secondary Access Key in the Manage Keys blade. Either key can be used.\n+- Registration URL is the URL field on the Keys page.\n+- Registration key is the value of the **Primary access key** field or the **Secondary access key** field on the Keys page. Either key can be used.\n+\n+For added security, you can regenerate the primary and secondary access keys of an Automation account at any time on the Keys page. Key regeneration prevents future node registrations from using previous keys.\n \n-For added security, the primary and secondary access keys of an Automation account can be\n-regenerated at any time (on the **Manage Keys** page) to prevent future node registrations using\n-previous keys.\n+## Re-registering a node\n \n-## Certificate expiration and re-registration\n+After registering a machine as a DSC node in Azure Automation State Configuration, there are several reasons why you might need to re-register that node in the future.\n \n-After registering a machine as a DSC node in Azure Automation State Configuration, there are a\n-number of reasons why you may need to re-register that node in the future:\n+- **Certificate renewal.** For versions of Windows Server before Windows Server 2019, each node automatically negotiates a unique certificate for authentication that expires after one year. If a certificate expires without renewal, the node is unable to communicate with Azure Automation and is marked `Unresponsive`. Currently, the PowerShell DSC registration protocol can't automatically renew certificates when they are nearing expiration, and you must re-register the nodes after a year's time. Before re-registering, ensure that each node is running WMF 5 RTM. \n \n-- For versions of Windows Server prior to Windows Server 2019, each node automatically negotiates a unique certificate for authentication that expires after one year. Currently, the PowerShell DSC registration protocol cannot automatically renew certificates when they are nearing expiration, so you need to re-register the nodes after a year's time. Before re-registering, ensure that each node is running Windows Management Framework 5.0 RTM. If a node's authentication certificate expires, and the node is not re-registered, the node is unable to communicate with Azure Automation and is marked 'Unresponsive.' Re-registration performed 90 days or less from the certificate expiration time, or at any point after the certificate expiration time, will result in a new certificate being generated and used.  A resolution to this issue is included in Windows Server 2019 and later.\n-- To change any [PowerShell DSC Local Configuration Manager values](/powershell/scripting/dsc/managing-nodes/metaConfig4) that were set during initial registration of the node, such as ConfigurationMode. Currently, these DSC agent values can only be changed through re-registration. The one exception is the Node Configuration assigned to the node -- this can be changed in Azure Automation DSC directly.\n+    Re-registration performed 90 days or less from the certificate expiration time, or at any point after the certificate expiration time, results in a new certificate being generated and used. A resolution to this issue is included in Windows Server 2019 and later.\n \n-Re-registration can be performed in the same way you registered the node initially, using any of the\n-onboarding methods described in this document. You do not need to unregister a node from Azure\n-Automation State Configuration before re-registering it.\n+- **Changes to DSC LCM values.** You might need to change [PowerShell DSC LCM values](/powershell/scripting/dsc/managing-nodes/metaConfig4) set during initial registration of the node, for example, `ConfigurationMode`. Currently, you can only change these DSC agent values through re-registration. The one exception is the Node Configuration value assigned to the node. You can change this in Azure Automation DSC directly.\n+\n+You can re-register a node in the same way that you registered the node initially, using any of the onboarding methods described in this document. You do not need to unregister a node from Azure Automation State Configuration before re-registering it.\n \n ## Troubleshooting Azure virtual machine onboarding\n \n-Azure Automation State Configuration lets you easily onboard Azure Windows VMs for configuration\n-management. Under the hood, the Azure VM Desired State Configuration extension is used to register\n-the VM with Azure Automation State Configuration. Since the Azure VM Desired State Configuration\n-extension runs asynchronously, tracking its progress and troubleshooting its execution may be\n-important.\n+Azure Automation State Configuration lets you easily onboard Azure Windows VMs for configuration management. Under the hood, the Azure VM Desired State Configuration extension is used to register the VM with Azure Automation State Configuration. Since the Azure VM Desired State Configuration extension runs asynchronously, tracking its progress and troubleshooting its execution can be important.\n \n > [!NOTE]\n-> Any method of onboarding an Azure Windows VM to Azure Automation State Configuration that uses the Azure VM Desired State Configuration extension could take up to an hour for the node to show up as registered in Azure Automation. This is due to the installation of Windows Management Framework 5.0 on the VM by the Azure VM DSC extension, which is required to onboard the VM to Azure Automation State Configuration.\n+> Any method of onboarding an Azure Windows VM to Azure Automation State Configuration that uses the Azure VM Desired State Configuration extension can take up to an hour for Azure Automation to show it as registered. This delay is due to the installation of WMF 5 on the VM by the Azure VM Desired State Configuration extension, which is required to onboard the VM to Azure Automation State Configuration.\n+\n+To troubleshoot or view the status of the Azure VM Desired State Configuration extension:\n \n-To troubleshoot or view the status of the Azure VM Desired State Configuration extension, in the\n-Azure portal navigate to the VM being onboarded, then click **Extensions** under **Settings**. Then\n-click **DSC** or **DSCForLinux** depending on your operating system. For more details, you can\n-click **View detailed status**.\n+1. In the Azure portal, navigate to the VM being onboarded.\n+2. Click **Extensions** under **Settings**. \n+3. Now select **DSC** or **DSCForLinux**, depending on your operating system. \n+4. For more details, you can click **View detailed status**.\n \n For more information on troubleshooting, see [Troubleshooting issues with Azure Automation Desired State Configuration (DSC)](./troubleshoot/desired-state-configuration.md).\n \n ## Next steps\n \n-- To get started, see [Getting started with Azure Automation State Configuration](automation-dsc-getting-started.md)\n-- To learn about compiling DSC configurations so that you can assign them to target nodes, see [Compiling configurations in Azure Automation State Configuration](automation-dsc-compile.md)\n-- For PowerShell cmdlet reference, see [Azure Automation State Configuration cmdlets](/powershell/module/az.automation#automation)\n-- For pricing information, see [Azure Automation State Configuration pricing](https://azure.microsoft.com/pricing/details/automation/)\n-- To see an example of using Azure Automation State Configuration in a continuous deployment pipeline, see [Continuous Deployment Using Azure Automation State Configuration and Chocolatey](automation-dsc-cd-chocolatey.md)\n+- To get started, see [Getting started with Azure Automation State Configuration](automation-dsc-getting-started.md).\n+- To learn about compiling DSC configurations so that you can assign them to target nodes, see [Compiling configurations in Azure Automation State Configuration](automation-dsc-compile.md).\n+- For PowerShell cmdlet reference, see [Azure Automation State Configuration cmdlets](/powershell/module/az.automation#automation).\n+- For pricing information, see [Azure Automation State Configuration pricing](https://azure.microsoft.com/pricing/details/automation/).\n+- For an example of using Azure Automation State Configuration in a continuous deployment pipeline, see [Usage Example: Continuous deployment to virtual machines Using Azure Automation State Configuration and Chocolatey](automation-dsc-cd-chocolatey.md)."
  },
  {
    "Number": 109068,
    "Title": "3/25 PM Publish",
    "ClosedAt": "2020-03-25T22:03:16Z",
    "User": "huypub",
    "FileName": "articles/automation/automation-dsc-overview.md",
    "Addition": 41,
    "Delections": 58,
    "Changes": 99,
    "Patch": "@@ -15,43 +15,34 @@ manager: carmonm\n \n Azure Automation State Configuration is an Azure service that allows you to write, manage, and\n compile PowerShell Desired State Configuration (DSC)\n-[configurations](/powershell/scripting/dsc/configurations/configurations), import [DSC Resources](/powershell/scripting/dsc/resources/resources),\n-and assign configurations to target nodes, all in the cloud.\n+[configurations](/powershell/scripting/dsc/configurations/configurations). The service also imports [DSC Resources](/powershell/scripting/dsc/resources/resources), and assigns configurations to target nodes, all in the cloud.\n \n ## Why use Azure Automation State Configuration\n \n Azure Automation State Configuration provides several advantages over using DSC outside of Azure.\n \n ### Built-in pull server\n \n-Azure Automation State Configuration provides a DSC pull server similar to the\n-[Windows Feature DSC-Service](/powershell/scripting/dsc/pull-server/pullserver) so that target nodes automatically receive\n-configurations, conform to the desired state, and report back on their compliance. The built-in pull\n-server in Azure Automation eliminates the need to set up and maintain your own pull server. Azure\n+Azure Automation State Configuration provides a DSC pull server similar to the [Windows Feature DSC-Service](/powershell/scripting/dsc/pull-server/pullserver). Target nodes can automatically receive\n+configurations, conform to the desired state, and report on their compliance. The built-in pull server in Azure Automation eliminates the need to set up and maintain your own pull server. Azure\n Automation can target virtual or physical Windows or Linux machines, in the cloud or on-premises.\n \n ### Management of all your DSC artifacts\n \n-Azure Automation State Configuration brings the same management layer to\n-[PowerShell Desired State Configuration](/powershell/scripting/dsc/overview/overview) as Azure Automation offers for PowerShell scripting.\n-\n-From the Azure portal, or from PowerShell, you can manage all your DSC configurations, resources,\n-and target nodes.\n+Azure Automation State Configuration brings the same management layer to [PowerShell Desired State Configuration](/powershell/scripting/dsc/overview/overview) as it offers for PowerShell scripting. From the Azure portal or from PowerShell, you can manage all your DSC configurations, resources, and target nodes.\n \n ![Screenshot of the Azure Automation page](./media/automation-dsc-overview/azure-automation-blade.png)\n \n-### Import reporting data into Azure Monitor logs\n+### Import of reporting data into Azure Monitor logs\n \n-Nodes that are managed with Azure Automation State Configuration send detailed reporting status\n-data to the built-in pull server. You can configure Azure Automation State Configuration to send\n-this data to your Log Analytics workspace. To learn how to send State Configuration status data to\n-your Log Analytics workspace, see [Forward Azure Automation State Configuration reporting data to Azure Monitor logs](automation-dsc-diagnostics.md).\n+Nodes that are managed with Azure Automation State Configuration send detailed reporting status data to the built-in pull server. You can configure Azure Automation State Configuration to send\n+this data to your Log Analytics workspace. See [Forward Azure Automation State Configuration reporting data to Azure Monitor logs](automation-dsc-diagnostics.md).\n \n-## Prerequisites\n+## Prerequisites for using Azure Automation State Configuration\n \n-Please consider the following requirements when using Azure Automation State Configuration (DSC).\n+Please consider the following requirements when using Azure Automation State Configuration for DSC.\n \n-### Operating System Requirements\n+### Operating system requirements\n \n For nodes running Windows, the following versions are supported:\n \n@@ -64,64 +55,56 @@ For nodes running Windows, the following versions are supported:\n - Windows 8.1\n - Windows 7\n \n-The [Microsoft Hyper-V Server](/windows-server/virtualization/hyper-v/hyper-v-server-2016)\n-standalone product sku does not contain an implementation of Desired State Configuraion\n-so it cannot be managed by PowerShell DSC or Azure Automation State Configuration.\n-\n-For nodes running Linux, the following distros/versions are supported:\n+>[!NOTE]\n+>The [Microsoft Hyper-V Server](/windows-server/virtualization/hyper-v/hyper-v-server-2016) standalone product SKU does not contain an implementation of DSC. Thus it can't be managed by PowerShell DSC or Azure Automation State Configuration.\n \n-The DSC Linux extension supports all the Linux distributions listed under [Supported Linux Distributions](https://github.com/Azure/azure-linux-extensions/tree/master/DSC#4-supported-linux-distributions).\n+For nodes running Linux, the DSC Linux extension supports all the Linux distributions listed under [Supported Linux Distributions](https://github.com/Azure/azure-linux-extensions/tree/master/DSC#4-supported-linux-distributions).\n \n ### DSC requirements\n \n For all Windows nodes running in Azure,\n [WMF 5.1](https://docs.microsoft.com/powershell/scripting/wmf/setup/install-configure)\n-will be installed during onboarding.  For nodes running Windows Server 2012 and Windows 7,\n-[WinRM will be enabled](https://docs.microsoft.com/powershell/scripting/dsc/troubleshooting/troubleshooting#winrm-dependency).\n+is installed during onboarding. For nodes running Windows Server 2012 and Windows 7,\n+[WinRM](https://docs.microsoft.com/powershell/scripting/dsc/troubleshooting/troubleshooting#winrm-dependency) is enabled.\n \n For all Linux nodes running in Azure,\n [PowerShell DSC for Linux](https://github.com/Microsoft/PowerShell-DSC-for-Linux)\n-will be installed during onboarding.\n+is installed during onboarding.\n \n-### <a name=\"network-planning\"></a>Configure private networks\n+### <a name=\"network-planning\"></a>Configuration of private networks\n \n-If your nodes are located within a private network,\n-the following port and URLs are required for State Configuration (DSC) to communicate with Automation:\n+If your nodes are located in a private network, the following port and URLs are required. These resources provide network connectivity for the managed node and allow DSC to communicate with Azure Automation.\n \n-* Port: Only TCP 443 is required for outbound internet access.\n-* Global URL: *.azure-automation.net\n-* Global URL of US Gov Virginia: *.azure-automation.us\n-* Agent service: https://\\<workspaceId\\>.agentsvc.azure-automation.net\n+* Port: Only TCP 443 required for outbound internet access\n+* Global URL: ***.azure-automation.net**\n+* Global URL of US Gov Virginia: ***.azure-automation.us**\n+* Agent service: **https://\\<workspaceId\\>.agentsvc.azure-automation.net**\n \n-This provides network connectivity for the managed node to communicate with Azure Automation.\n If you are using DSC resources that communicate between nodes,\n such as the [WaitFor* resources](https://docs.microsoft.com/powershell/scripting/dsc/reference/resources/windows/waitForAllResource),\n-you will also need to allow traffic between nodes.\n-See the documentation for each DSC resource to understand those network requirements.\n+you also need to allow traffic between nodes. See the documentation for each DSC resource to understand these network requirements.\n \n-#### Proxy Support\n+#### Proxy support\n \n-Proxy support for the DSC agent is available in Windows version 1809 and later.\n-To configure this option,\n-set the value for **ProxyURL** and **ProxyCredential** in the\n-[metaconfiguration script](automation-dsc-onboarding.md#generating-dsc-metaconfigurations)\n+Proxy support for the DSC agent is available in Windows version 1809 and later. This option is enabled by setting the values for `ProxyURL` and `ProxyCredential` in the [metaconfiguration script](automation-dsc-onboarding.md#generating-dsc-metaconfigurations)\n used to register nodes.\n-Proxy is not available in DSC for previous versions of Windows.\n \n-For Linux nodes,\n-the DSC agent supports proxy and will utilize the http_proxy variable to determine the url.\n+>[!NOTE]\n+>Azure Automation State Configuration does not provide DSC proxy support for previous versions of Windows.\n+\n+For Linux nodes, the DSC agent supports proxy and uses the `http_proxy` variable to determine the URL.\n \n-#### Azure State Configuration network ranges and namespace\n+#### Azure Automation State Configuration network ranges and namespace\n \n-It's recommended to use the addresses listed when defining exceptions. For IP addresses you can download the [Microsoft Azure Datacenter IP Ranges](https://www.microsoft.com/download/details.aspx?id=41653). This file is updated weekly, and has the currently deployed ranges and any upcoming changes to the IP ranges.\n+It's recommended to use the addresses listed below when defining exceptions. For IP addresses, you can download the [Microsoft Azure Datacenter IP Ranges](https://www.microsoft.com/download/details.aspx?id=41653). This file is updated weekly, and has the currently deployed ranges and any upcoming changes to the IP ranges.\n \n If you have an Automation account that's defined for a specific region, you can restrict communication to that regional datacenter. The following table provides the DNS record for each region:\n \n | **Region** | **DNS record** |\n | --- | --- |\n | West Central US | wcus-jobruntimedata-prod-su1.azure-automation.net</br>wcus-agentservice-prod-1.azure-automation.net |\n | South Central US |scus-jobruntimedata-prod-su1.azure-automation.net</br>scus-agentservice-prod-1.azure-automation.net |\n-| East US\t| eus-jobruntimedata-prod-su1.azure-automation.net</br>eus-agentservice-prod-1.azure-automation.net |\n+| East US    | eus-jobruntimedata-prod-su1.azure-automation.net</br>eus-agentservice-prod-1.azure-automation.net |\n | East US 2 |eus2-jobruntimedata-prod-su1.azure-automation.net</br>eus2-agentservice-prod-1.azure-automation.net |\n | Canada Central |cc-jobruntimedata-prod-su1.azure-automation.net</br>cc-agentservice-prod-1.azure-automation.net |\n | West Europe |we-jobruntimedata-prod-su1.azure-automation.net</br>we-agentservice-prod-1.azure-automation.net |\n@@ -138,15 +121,15 @@ For a list of region IP addresses instead of region names, download the [Azure D\n > [!NOTE]\n > The Azure Datacenter IP address XML file lists the IP address ranges that are used in the Microsoft Azure datacenters. The file includes compute, SQL, and storage ranges.\n >\n->An updated file is posted weekly. The file reflects the currently deployed ranges and any upcoming changes to the IP ranges. New ranges that appear in the file aren't used in the datacenters for at least one week.\n->\n-> It's a good idea to download the new XML file every week. Then, update your site to correctly identify services running in Azure. Azure ExpressRoute users should note that this file is used to update the Border Gateway Protocol (BGP) advertisement of Azure space in the first week of each month.\n+>An updated file is posted weekly. The file reflects the currently deployed ranges and any upcoming changes to the IP ranges. New ranges that appear in the file aren't used in the datacenters for at least one week. It's a good idea to download the new XML file every week. Then update your site to correctly identify services running in Azure. \n+\n+Azure ExpressRoute users should note that this file is used to update the Border Gateway Protocol (BGP) advertisement of Azure space in the first week of each month.\n \n ## Next steps\n \n-- To get started, see [Getting started with Azure Automation State Configuration](automation-dsc-getting-started.md)\n-- To learn how to onboard nodes, see [Onboarding machines for management by Azure Automation State Configuration](automation-dsc-onboarding.md)\n-- To learn about compiling DSC configurations so that you can assign them to target nodes, see [Compiling configurations in Azure Automation State Configuration](automation-dsc-compile.md)\n-- For PowerShell cmdlet reference, see [Azure Automation State Configuration cmdlets](/powershell/module/azurerm.automation/#automation)\n-- For pricing information, see [Azure Automation State Configuration pricing](https://azure.microsoft.com/pricing/details/automation/)\n-- To see an example of using Azure Automation State Configuration in a continuous deployment pipeline, see [Continuous Deployment Using Azure Automation State Configuration and Chocolatey](automation-dsc-cd-chocolatey.md)\n+- To get started using DSC in Azure Automation State Configuration, see [Getting started with Azure Automation State Configuration](automation-dsc-getting-started.md).\n+- To learn how to onboard nodes, see [Onboarding machines for management by Azure Automation State Configuration](automation-dsc-onboarding.md).\n+- To learn about compiling DSC configurations so that you can assign them to target nodes, see [Compiling configurations in Azure Automation State Configuration](automation-dsc-compile.md).\n+- For PowerShell cmdlet reference, see [Azure Automation State Configuration cmdlets](/powershell/module/azurerm.automation/#automation).\n+- For pricing information, see [Azure Automation State Configuration pricing](https://azure.microsoft.com/pricing/details/automation/).\n+- To see an example of using Azure Automation State Configuration in a continuous deployment pipeline, see [Continuous deployment using Azure Automation State Configuration and Chocolatey](automation-dsc-cd-chocolatey.md)."
  },
  {
    "Number": 109068,
    "Title": "3/25 PM Publish",
    "ClosedAt": "2020-03-25T22:03:16Z",
    "User": "huypub",
    "FileName": "articles/azure-functions/functions-create-first-azure-function-azure-cli.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -31,7 +31,7 @@ Before you begin, you must have the following:\n ::: zone-end\n \n ::: zone pivot=\"programming-language-python\"\n-+ [Python 3.8](https://www.python.org/downloads/release/python-382/), [Python 3.7](https://www.python.org/downloads/release/python-375/), [Python 3.6](https://www.python.org/downloads/release/python-368/), which are supported by Azure Functions. \n++ [Python 3.8](https://www.python.org/downloads/release/python-382/), [Python 3.7](https://www.python.org/downloads/release/python-375/), [Python 3.6](https://www.python.org/downloads/release/python-368/), which are supported by Azure Functions (x64).\n ::: zone-end\n ::: zone pivot=\"programming-language-powershell\"\n + [PowerShell Core](/powershell/scripting/install/installing-powershell-core-on-windows)"
  },
  {
    "Number": 109068,
    "Title": "3/25 PM Publish",
    "ClosedAt": "2020-03-25T22:03:16Z",
    "User": "huypub",
    "FileName": "articles/azure-functions/functions-create-first-function-vs-code.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -27,7 +27,7 @@ Before you get started, make sure you have the following requirements in place:\n + [Node.js](https://nodejs.org/), Active LTS and Maintenance LTS versions (10.14.1 recommended). Use the `npm --version` command to check your version.\n ::: zone-end \n ::: zone pivot=\"programming-language-python\"\n-+ [Python 3.8](https://www.python.org/downloads/release/python-381/), [Python 3.7](https://www.python.org/downloads/release/python-375/), [Python 3.6](https://www.python.org/downloads/release/python-368/) are supported by Azure Functions.\n++ [Python 3.8](https://www.python.org/downloads/release/python-381/), [Python 3.7](https://www.python.org/downloads/release/python-375/), [Python 3.6](https://www.python.org/downloads/release/python-368/) are supported by Azure Functions (x64).\n ::: zone-end   \n ::: zone pivot=\"programming-language-powershell\"\n + [PowerShell Core](/powershell/scripting/install/installing-powershell-core-on-windows)"
  },
  {
    "Number": 109068,
    "Title": "3/25 PM Publish",
    "ClosedAt": "2020-03-25T22:03:16Z",
    "User": "huypub",
    "FileName": "articles/azure-monitor/app/asp-net-core.md",
    "Addition": 8,
    "Delections": 8,
    "Changes": 16,
    "Patch": "@@ -24,7 +24,7 @@ The [Application Insights SDK for ASP.NET Core](https://nuget.org/packages/Micro\n * **IDE**: Visual Studio, VS Code, or command line.\n \n > [!NOTE]\n-> If you are using ASP.NET Core 3.0 along with Application Insights, please use the [2.8.0](https://www.nuget.org/packages/Microsoft.ApplicationInsights.AspNetCore/2.8.0) version or higher. This is the only version that supports ASP.NET Core 3.0.\n+> If you are using ASP.NET Core 3.X along with Application Insights, please use the [2.8.0](https://www.nuget.org/packages/Microsoft.ApplicationInsights.AspNetCore/2.8.0) version or higher. This is the only version that supports ASP.NET Core 3.X.\n \n ## Prerequisites\n \n@@ -137,7 +137,7 @@ Support for [performance counters](https://azure.microsoft.com/documentation/art\n \n ### EventCounter\n \n-`EventCounterCollectionModule` is enabled by default, and it will collect a default set of counters from .NET Core 3.0 apps. The [EventCounter](eventcounters.md) tutorial lists the default set of counters collected. It also has instructions on customizing the list.\n+`EventCounterCollectionModule` is enabled by default, and it will collect a default set of counters from .NET Core 3.X apps. The [EventCounter](eventcounters.md) tutorial lists the default set of counters collected. It also has instructions on customizing the list.\n \n ## Enable client-side telemetry for web applications\n \n@@ -360,11 +360,11 @@ The above does not prevent any auto collection modules from collecting telemetry\n \n ## Frequently asked questions\n \n-### Does Application Insights support ASP.NET Core 3.0?\n+### Does Application Insights support ASP.NET Core 3.X?\n \n-Yes. Update to [Application Insights SDK for ASP.NET Core](https://nuget.org/packages/Microsoft.ApplicationInsights.AspNetCore) version 2.8.0 or higher. Older versions of the SDK do not support ASP.NET Core 3.0.\n+Yes. Update to [Application Insights SDK for ASP.NET Core](https://nuget.org/packages/Microsoft.ApplicationInsights.AspNetCore) version 2.8.0 or higher. Older versions of the SDK do not support ASP.NET Core 3.X.\n \n-Also, if you are using Visual Studio based instructions from [here](#enable-application-insights-server-side-telemetry-visual-studio), update to the latest version of Visual Studio 2019 (16.3.0) to onboard. Previous versions of Visual Studio do not support automatic onboarding for ASP.NET Core 3.0 apps.\n+Also, if you are using Visual Studio based instructions from [here](#enable-application-insights-server-side-telemetry-visual-studio), update to the latest version of Visual Studio 2019 (16.3.0) to onboard. Previous versions of Visual Studio do not support automatic onboarding for ASP.NET Core 3.X apps.\n \n ### How can I track telemetry that's not automatically collected?\n \n@@ -397,7 +397,7 @@ For more information about custom data reporting in Application Insights, see [A\n \n ### Some Visual Studio templates used the UseApplicationInsights() extension method on IWebHostBuilder to enable Application Insights. Is this usage still valid?\n \n-While the extension method `UseApplicationInsights()` is still supported, it is marked obsolete in Application Insights SDK version 2.8.0 onwards. It will be removed in the next major version of the SDK. The recommended way to enable Application Insights telemetry is by using `AddApplicationInsightsTelemetry()` because it provides overloads to control some configuration. Also, in ASP.NET Core 3.0 apps, `services.AddApplicationInsightsTelemetry()` is the only way to enable application insights.\n+While the extension method `UseApplicationInsights()` is still supported, it is marked obsolete in Application Insights SDK version 2.8.0 onwards. It will be removed in the next major version of the SDK. The recommended way to enable Application Insights telemetry is by using `AddApplicationInsightsTelemetry()` because it provides overloads to control some configuration. Also, in ASP.NET Core 3.X apps, `services.AddApplicationInsightsTelemetry()` is the only way to enable application insights.\n \n ### I'm deploying my ASP.NET Core application to Web Apps. Should I still enable the Application Insights extension from Web Apps?\n \n@@ -444,9 +444,9 @@ using Microsoft.ApplicationInsights.WindowsServer.TelemetryChannel;\n     }\n ```\n \n-### Is this SDK supported for the new .NET Core 3.0 Worker Service template applications?\n+### Is this SDK supported for the new .NET Core 3.X Worker Service template applications?\n \n-This SDK requires `HttpContext`, and hence does not work in any non-HTTP applications, including the .NET Core 3.0 Worker Service applications. Refer to [this](worker-service.md) document for enabling application insights in such applications, using the newly released Microsoft.ApplicationInsights.WorkerService SDK.\n+This SDK requires `HttpContext`, and hence does not work in any non-HTTP applications, including the .NET Core 3.X Worker Service applications. Refer to [this](worker-service.md) document for enabling application insights in such applications, using the newly released Microsoft.ApplicationInsights.WorkerService SDK.\n \n ## Open-source SDK\n "
  },
  {
    "Number": 109068,
    "Title": "3/25 PM Publish",
    "ClosedAt": "2020-03-25T22:03:16Z",
    "User": "huypub",
    "FileName": "articles/azure-monitor/app/ilogger.md",
    "Addition": 5,
    "Delections": 7,
    "Changes": 12,
    "Patch": "@@ -322,22 +322,20 @@ The following examples apply filter rules to ApplicationInsightsLoggerProvider.\n \n ### Create filter rules in configuration with appsettings.json\n \n-For ApplicationInsightsLoggerProvider, the provider alias is `ApplicationInsights`. The following section of *appsettings.json* configures logs for *Warning* and above from all categories and *Error* and above from categories that start with \"Microsoft\" to be sent to `ApplicationInsightsLoggerProvider`.\n+For ApplicationInsightsLoggerProvider, the provider alias is `ApplicationInsights`. The following section of *appsettings.json* instructs logging providers generally to log at level *Warning* and above. It then overrides the `ApplicationInsightsLoggerProvider` to log categories that start with \"Microsoft\" at level *Error* and above.\n \n ```json\n {\n   \"Logging\": {\n+    \"LogLevel\": {\n+      \"Default\": \"Warning\"\n+    },\n     \"ApplicationInsights\": {\n       \"LogLevel\": {\n-        \"Default\": \"Warning\",\n         \"Microsoft\": \"Error\"\n       }\n-    },\n-    \"LogLevel\": {\n-      \"Default\": \"Warning\"\n     }\n-  },\n-  \"AllowedHosts\": \"*\"\n+  }\n }\n ```\n "
  },
  {
    "Number": 109068,
    "Title": "3/25 PM Publish",
    "ClosedAt": "2020-03-25T22:03:16Z",
    "User": "huypub",
    "FileName": "articles/azure-monitor/app/usage-overview.md",
    "Addition": 9,
    "Delections": 5,
    "Changes": 14,
    "Patch": "@@ -2,7 +2,7 @@\n title: Usage analysis with Azure Application Insights | Microsoft docs\n description: Understand your users and what they do with your app.\n ms.topic: conceptual\n-ms.date: 09/19/2019\n+ms.date: 03/25/2019\n \n ---\n \n@@ -125,10 +125,14 @@ To do this, [set up a telemetry initializer](../../azure-monitor/app/api-filteri\n     // Telemetry initializer class\n     public class MyTelemetryInitializer : ITelemetryInitializer\n     {\n-        public void Initialize (ITelemetry telemetry)\n-        {\n-            telemetry.Properties[\"AppVersion\"] = \"v2.1\";\n-        }\n+        public void Initialize(ITelemetry item)\n+            {\n+                var itemProperties = item as ISupportProperties;\n+                if (itemProperties != null && !itemProperties.Properties.ContainsKey(\"AppVersion\"))\n+                {\n+                    itemProperties.Properties[\"AppVersion\"] = \"v2.1\";\n+                }\n+            }\n     }\n ```\n "
  },
  {
    "Number": 109060,
    "Title": "fix code to use ISupportProperties to address GitHub Issue",
    "ClosedAt": "2020-03-25T21:36:19Z",
    "User": "mrbullwinkle",
    "FileName": "articles/azure-monitor/app/usage-overview.md",
    "Addition": 9,
    "Delections": 5,
    "Changes": 14,
    "Patch": "@@ -2,7 +2,7 @@\n title: Usage analysis with Azure Application Insights | Microsoft docs\n description: Understand your users and what they do with your app.\n ms.topic: conceptual\n-ms.date: 09/19/2019\n+ms.date: 03/25/2019\n \n ---\n \n@@ -125,10 +125,14 @@ To do this, [set up a telemetry initializer](../../azure-monitor/app/api-filteri\n     // Telemetry initializer class\n     public class MyTelemetryInitializer : ITelemetryInitializer\n     {\n-        public void Initialize (ITelemetry telemetry)\n-        {\n-            telemetry.Properties[\"AppVersion\"] = \"v2.1\";\n-        }\n+        public void Initialize(ITelemetry item)\n+            {\n+                var itemProperties = item as ISupportProperties;\n+                if (itemProperties != null && !itemProperties.Properties.ContainsKey(\"AppVersion\"))\n+                {\n+                    itemProperties.Properties[\"AppVersion\"] = \"v2.1\";\n+                }\n+            }\n     }\n ```\n "
  },
  {
    "Number": 94474,
    "Title": "Azure Monitor landing page fix",
    "ClosedAt": "2020-03-25T20:07:42Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/azure-monitor-app-hub.md",
    "Addition": 8,
    "Delections": 1,
    "Changes": 9,
    "Patch": "@@ -12,7 +12,14 @@ ms.date: 01/15/2019\n \n ---\n \n-#products\"></a>\n+<div id=\"main\" class=\"v2\">\n+    <div class=\"container\">\n+        <h1>Azure Monitor Application Insights Documentation</h1>\n+        <p>Azure Monitor Application Insights provides deep code-level application performance monitoring.</p>\n+        <hr style=\"margin: 30px 0;\" />\n+        <ul class=\"pivots\">\n+            <li>\n+                <a href=\"#products\"></a>\n                 <ul id=\"products\">\n                     <li>\n                         <a class=\"singlePanelNavItem selected\" style=\"display: none\" href=\"#indexA\" data-linktype=\"self-bookmark\"></a>"
  },
  {
    "Number": 108727,
    "Title": "Azure Monitor VM Insights Updates",
    "ClosedAt": "2020-03-25T19:20:45Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/monitor-vm-azure.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -161,7 +161,7 @@ Azure Monitor for VMs enables the collection of a predetermined set of performan\n \n | Data source | Requirements | Tables |\n |:---|:---|:---|\n-| Azure Monitor for VMs | Enable on each virtual machine. | InsightsMetrics<br>VMBoundPort<br>VMComputer<br>VMConnection<br>VMProcess<br>See [How to query logs from Azure Monitor for VMs](vminsights-log-search.md#sample-log-searches) for details. |\n+| Azure Monitor for VMs | Enable on each virtual machine. | InsightsMetrics<br>VMBoundPort<br>VMComputer<br>VMConnection<br>VMProcess<br>See [How to query logs from Azure Monitor for VMs](vminsights-log-search.md) for details. |\n | Activity log | Diagnostic setting for the subscription. | AzureActivity |\n | Host metrics | Diagnostic setting for the virtual machine. | AzureMetrics |\n | Data sources from the guest operating system | Enable Log Analytics agent and configure data sources. | See documentation for each data source. |"
  },
  {
    "Number": 108727,
    "Title": "Azure Monitor VM Insights Updates",
    "ClosedAt": "2020-03-25T19:20:45Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/vminsights-alerts.md",
    "Addition": 197,
    "Delections": 0,
    "Changes": 197,
    "Patch": "@@ -0,0 +1,197 @@\n+---\n+title: Alerts from Azure Monitor for VMs\n+description: Describes how to create alert rules from performance data collected by Azure Monitor for VMs.\n+ms.subservice: \n+ms.topic: conceptual\n+author: bwren\n+ms.author: bwren\n+ms.date: 03/23/2020\n+\n+---\n+\n+# How to create alerts from Azure Monitor for VMs\n+[Alerts in Azure Monitor](../platform/alerts-overview.md) proactively notify you of interesting data and patterns in your monitoring data. Azure Monitor for VMs does not include pre-configured alert rules, but you can create your own based on data that it collects. This article provides guidance on creating alert rules, including a set of sample queries.\n+\n+\n+## Alert rule types\n+Azure Monitor has [different types of alert rules](../platform/alerts-overview.md#what-you-can-alert-on) based on the data being used to create the alert. All data collected by Azure Monitor for VMs is stored in Azure Monitor Logs which supports [log alerts](../platform/alerts-log.md). You cannot currently use [metric alerts](../platform/alerts-log.md) with performance data collected from Azure Monitor for VMs because the data is not collected into Azure Monitor Metrics. To collect data for metric alerts, install the [diagnostics extension](../platform/diagnostics-extension-overview.md) for Windows VMs or the [Telegraf agent](../platform/collect-custom-metrics-linux-telegraf.md) for Linux VMs to collect performance data into Metrics.\n+\n+There are two types of log alerts in Azure Monitor:\n+\n+- [Number of results alerts](../platform/alerts-unified-log.md#number-of-results-alert-rules) create a single alert when a query returns at least a specified number of records. These are ideal for non-numeric data such and Windows and Syslog events collected by the [Log Analytics agent](../platform/log-analytics-agent.md) or for analyzing performance trends across multiple computers.\n+- [Metric measurement alerts](../platform/alerts-unified-log.md#metric-measurement-alert-rules) create a separate alert for each record in a query that has a value that exceeds a threshold defined in the alert rule. These alert rules are ideal for performance data collected by Azure Monitor for VMs since they can create individual alerts for each computer.\n+\n+\n+## Alert rule walkthrough\n+This section walks through the creation of a metric measurement alert rule using performance data from Azure Monitor for VMs. You can use this basic process with a variety of log queries to alert on different performance counters.\n+\n+Start by creating a new alert rule following the procedure in [Create, view, and manage log alerts using Azure Monitor](../platform/alerts-log.md). For the **Resource**, select the Log Analytics workspace that Azure Monitor VMs uses in your subscription. Since the target resource for log alert rules is always a Log Analytics workspace, the log query must include any filter for particular virtual machines or virtual machine scale sets. \n+\n+For the **Condition** of the alert rule, use one of the queries in the [section below](#sample-alert-queries) as the **Search query**. The query must return a numeric property called *AggregatedValue*. It should summarize the data by computer so that you can create a separate alert for each virtual machine that exceeds the threshold.\n+\n+In the **Alert logic**, select **Metric measurement** and then provide a **Threshold value**. In **Trigger Alert Based On**, specify how many times the threshold must be exceeded before an alert is created. For example, you probably don't care if the processor exceeds a threshold once and then returns to normal, but you do care if it continues to exceed the threshold over multiple consecutive measurements.\n+\n+The **Evaluated based on** section defines how often the query is run and the time window for the query. In the example shown below, the query will run every 15 minutes and evaluate performance values collected over the previous 15 minutes.\n+\n+\n+![Metric measurement alert rule](media/vminsights-alerts/metric-measurement-alert.png)\n+\n+## Sample alert queries\n+The following queries can be used with a metric measurement alert rule using performance data collected by Azure Monitor for VMs. Each summarizes data by computer so that an alert is created for each computer with a value that exceeds the threshold.\n+\n+### CPU utilization\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\" \n+| where Namespace == \"Processor\" and Name == \"UtilizationPercentage\" \n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), Computer, _ResourceId\n+```\n+\n+### Available Memory in MB\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where Namespace == \"Memory\" and Name == \"AvailableMB\"\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), Computer, _ResourceId\n+```\n+\n+### Available Memory in percentage\n+\n+```kusto\n+InsightsMetrics \n+| where Origin == \"vm.azm.ms\" \n+| where Namespace == \"Memory\" and Name == \"AvailableMB\" \n+| extend TotalMemory = toreal(todynamic(Tags)[\"vm.azm.ms/memorySizeMB\"])\n+| extend AvailableMemoryPercentage = (toreal(Val) / TotalMemory) * 100.0 \n+| summarize AggregatedValue = avg(AvailableMemoryPercentage) by bin(TimeGenerated, 15m), Computer, _ResourceId \n+```\n+\n+### Logical disk used - all disks on each computer\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where Namespace == \"LogicalDisk\" and Name == \"FreeSpacePercentage\"\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), Computer, _ResourceId \n+```\n+\n+### Logical disk used - individual disks\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where Namespace == \"LogicalDisk\" and Name == \"FreeSpacePercentage\"\n+| extend Disk=tostring(todynamic(Tags)[\"vm.azm.ms/mountId\"])\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), Computer, _ResourceId, Disk\n+```\n+\n+### Logical disk IOPS\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\" \n+| where Namespace == \"LogicalDisk\" and Name == \"TransfersPerSecond\"\n+| extend Disk=tostring(todynamic(Tags)[\"vm.azm.ms/mountId\"])\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m) ), Computer, _ResourceId, Disk\n+```\n+\n+### Logical disk data rate\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\" \n+| where Namespace == \"LogicalDisk\" and Name == \"BytesPerSecond\"\n+| extend Disk=tostring(todynamic(Tags)[\"vm.azm.ms/mountId\"])\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m) , Computer, _ResourceId, Disk\n+```\n+\n+### Network interfaces bytes received - all interfaces\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where Namespace == \"Network\" and Name == \"ReadBytesPerSecond\"\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), Computer, _ResourceId \n+```\n+\n+### Network interfaces bytes received - individual interfaces\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where Namespace == \"Network\" and Name == \"ReadBytesPerSecond\"\n+| extend NetworkInterface=tostring(todynamic(Tags)[\"vm.azm.ms/networkDeviceId\"])\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), Computer, _ResourceId, NetworkInterface\n+```\n+\n+### Network interfaces bytes sent - all interfaces\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where Namespace == \"Network\" and Name == \"WriteBytesPerSecond\"\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), Computer, _ResourceId\n+```\n+\n+### Network interfaces bytes sent - individual interfaces\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where Namespace == \"Network\" and Name == \"WriteBytesPerSecond\"\n+| extend NetworkInterface=tostring(todynamic(Tags)[\"vm.azm.ms/networkDeviceId\"])\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), Computer, _ResourceId, NetworkInterface\n+```\n+\n+### Virtual machine scale set\n+Modify with your subscription ID, resource group, and virtual machine scale set name.\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where _ResourceId startswith \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/my-resource-group/providers/Microsoft.Compute/virtualMachineScaleSets/my-vm-scaleset\" \n+| where Namespace == \"Processor\" and Name == \"UtilizationPercentage\"\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), _ResourceId\n+```\n+\n+### Specific virtual machine\n+Modify with your subscription ID, resource group, and VM name.\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where _ResourceId =~ \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/my-resource-group/providers/Microsoft.Compute/virtualMachines/my-vm\" \n+| where Namespace == \"Processor\" and Name == \"UtilizationPercentage\"\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m)\n+```\n+\n+### CPU utilization for all compute resources in a subscription\n+Modify with your subscription ID.\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where _ResourceId startswith \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" and (_ResourceId contains \"/providers/Microsoft.Compute/virtualMachines/\" or _ResourceId contains \"/providers/Microsoft.Compute/virtualMachineScaleSets/\")\n+| where Namespace == \"Processor\" and Name == \"UtilizationPercentage\"\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), _ResourceId\n+```\n+\n+### CPU utilization for all compute resources in a resource group\n+Modify with your subscription ID and resource group.\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where _ResourceId startswith \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/my-resource-group/providers/Microsoft.Compute/virtualMachines/\"\n+or _ResourceId startswith \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/my-resource-group/providers/Microsoft.Compute/virtualMachineScaleSets/\" \n+| where Namespace == \"Processor\" and Name == \"UtilizationPercentage\"\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), _ResourceId\n+\n+```\n+\n+## Next steps\n+\n+- Learn more about [alerts in Azure Monitor](../platform/alerts-overview.md).\n+- Learn more about [log queries using data from Azure Monitor for VMs](vminsights-log-search.md)."
  },
  {
    "Number": 108727,
    "Title": "Azure Monitor VM Insights Updates",
    "ClosedAt": "2020-03-25T19:20:45Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/vminsights-enable-at-scale-policy.md",
    "Addition": 9,
    "Delections": 0,
    "Changes": 9,
    "Patch": "@@ -21,6 +21,15 @@ To discover, manage, and enable Azure Monitor for VMs for all of your Azure virt\n \n If you're interested in accomplishing these tasks with Azure PowerShell or an Azure Resource Manager template, see [Enable Azure Monitor for VMs using Azure PowerShell or Azure Resource Manager templates](vminsights-enable-at-scale-powershell.md).\n \n+## Prerequisites\n+Prior to using Policy to onboard your Azure VMs and virtual machine scale sets to Azure Monitoring for VMs, you must enable the VMInsights solution on the workspace you will use to store your monitoring data. This task can be completed from the **Get Started** page in Azure Monitor on the **Other onboarding options** tab.  Select **Configure a workspace**, which will prompt you to select the workspace to be configured.\n+\n+![Configure workspace](media/vminsights-enable-at-scale-policy/configure-workspace.png)\n+\n+You can also configure your workspace by choosing **Enable using policy** and then select the **Configure workspace** toolbar button.  This will install the VMInsights solution on the selected workspace which will enable the workspace to store the monitoring data sent by the VMs and virtual machine scale sets you enable using Policy. \n+\n+![Enable using policy](media/vminsights-enable-at-scale-policy/enable-using-policy.png)\n+\n ## Manage Policy Coverage feature overview\n \n Azure Monitor for VMs Policy Coverage simplifies discovering, managing, and enabling at scale the **Enable Azure Monitor for VMs** initiative, which includes the policy definitions mentioned earlier. To access this feature,  select **Other onboarding options** from the **Get Started** tab in Azure Monitor for VMs. Select **Manage Policy Coverage** to open the **Azure Monitor for VMs Policy Coverage** page."
  },
  {
    "Number": 108727,
    "Title": "Azure Monitor VM Insights Updates",
    "ClosedAt": "2020-03-25T19:20:45Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/vminsights-ga-release-faq.md",
    "Addition": 2,
    "Delections": 10,
    "Changes": 12,
    "Patch": "@@ -10,22 +10,18 @@ ms.date: 01/31/2020\n ---\n \n # Azure Monitor for VMs Generally Available (GA) Frequently Asked Questions\n-\n-This General Availability FAQ covers changes that are happening in Azure Monitor for VMs as we prepare for our GA release. \n+This General Availability FAQ covers changes that were made in Q4 2019 and Q1 2020 as we prepared for GA.\n \n ## Updates for Azure Monitor for VMs\n+We released a new version of Azure Monitor for VMs in January 2020 ahead of our GA announcement. Customers enabling Azure Monitor for VMs will now receive the GA version, but existing customers using the version of Azure Monitor for VMs from Q4 2019 and earlier will be prompted to upgrade. This FAQ offers guidance to perform an upgrade at scale if you have large deployments across multiple workspaces.\n \n-We have released a new version of Azure Monitor for VMs. Customers enabling Azure Monitors for VMs will now receive the new version, but existing customers already using Azure Monitor for VMs will be prompted to upgrade. This FAQ and our documentation offers guidance to perform an upgrade at scale if you have large deployments across multiple workspaces.\n \n With this upgrade, Azure Monitor for VMs performance data are stored in the same *InsightsMetrics* table as [Azure Monitor for containers](container-insights-overview.md), which makes it easier for you to query the two data sets. Also, you are able to store more diverse data sets that we could not store in the table previously used. \n \n Our performance views are now using the data we store in the *InsightsMetrics* table.  If you have not yet upgraded to use the latest VMInsights solution on your workspace, your charts will no longer display information.  You can upgrade from our **Get Started** page as described below.\n \n-We realize that asking existing customers to upgrade causes disruption to their workflow, which is why we have chosen to do this now while in Public Preview rather than later after GA.\n-\n \n ## What is changing?\n-\n We have released a new solution, named VMInsights, that includes additional capabilities for data collection along with a new location for storing this data in your Log Analytics workspace. \n \n In the past, we enabled the ServiceMap solution on your workspace and setup performance counters in your Log Analytics workspace to send the data to the *Perf* table. This new solution sends the data to a table named *InsightsMetrics* that is also used by Azure Monitor for containers. This table schema allows us to store additional metrics and service data sets that are not compatible with the *Perf* table format.\n@@ -88,10 +84,6 @@ No, the two solutions share the map data sets that we store in `VMComputer` (for\n \n No, the two solutions share the map data sets that we store in `VMComputer` (formerly ServiceMapComputer_CL), `VMProcess` (formerly ServiceMapProcess_CL), `VMConnection`, and `VMBoundPort`. If you remove one of the solutions, these data sets notice that there is still a solution in place that uses the data and it remains in the Log Analytics workspace. You need to remove both solutions from your workspace in order for the data to be removed from it.\n \n-## When will this update be released?\n-\n-We expect to release the update for Azure Monitor for VMs in early January 2020. As we get closer to the release date in January, we'll post updates here and present notifications in the Azure portal when you open Azure Monitor.\n-\n ## Health feature is in limited public preview\n \n We have received a lot of great feedback from customers about our VM Health feature set. There is a lot of interest around this feature and excitement over its potential for supporting monitoring workflows. We are planning to make a series of changes to add functionality and address the feedback we have received. "
  },
  {
    "Number": 108727,
    "Title": "Azure Monitor VM Insights Updates",
    "ClosedAt": "2020-03-25T19:20:45Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/vminsights-log-search.md",
    "Addition": 44,
    "Delections": 2,
    "Changes": 46,
    "Patch": "@@ -210,7 +210,7 @@ Records with a type of *VMComputer* have inventory data for servers with the Dep\n |AzureServiceFabricClusterId | The unique identifer of the Azure Service Fabric cluster | \n |AzureServiceFabricClusterName | The name of the Azure Service Fabric cluster |\n \n-### VMProcess record\n+### VMProcess records\n \n Records with a type of *VMProcess* have inventory data for TCP-connected processes on servers with the Dependency agent. These records have the properties in the following table:\n \n@@ -243,7 +243,8 @@ Records with a type of *VMProcess* have inventory data for TCP-connected process\n |UserDomain | The domain under which the process is executing |\n |_ResourceId | The unique identifier for a process within the workspace |\n \n-## Sample log searches\n+\n+## Sample map queries\n \n ### List all known machines\n \n@@ -425,6 +426,47 @@ let remoteMachines = remote | summarize by RemoteMachine;\n | summarize Remote=makeset(iff(isempty(RemoteMachine), todynamic('{}'), pack('Machine', RemoteMachine, 'Process', Process1, 'ProcessName', ProcessName1))) by ConnectionId, Direction, Machine, Process, ProcessName, SourceIp, DestinationIp, DestinationPort, Protocol\n ```\n \n+## Performance records\n+Records with a type of *InsightsMetrics* have performance data from the guest operating system of the virtual machine. These records have the properties in the following table:\n+\n+\n+| Property | Description |\n+|:--|:--|\n+|TenantId | Unique identifier for the workspace |\n+|SourceSystem | *Insights* | \n+|TimeGenerated | Time the value was collected (UTC) |\n+|Computer | The computer FQDN | \n+|Origin | *vm.azm.ms* |\n+|Namespace | Category of the performance counter | \n+|Name | Name of the performance counter |\n+|Val | Collected value | \n+|Tags | Related details about the record. See the table below for tags used with different record types.  |\n+|AgentId | Unique identifier for each computer's agent |\n+|Type | *InsightsMetrics* |\n+|_ResourceId_ | Resource ID of the virtual machine |\n+\n+The performance counters currently collected into the *InsightsMetrics* table are listed in the following table:\n+\n+| Namespace | Name | Description | Unit | Tags |\n+|:---|:---|:---|:---|:---|\n+| Computer    | Heartbeat             | Computer Heartbeat                        | | |\n+| Memory      | AvailableMB           | Memory Available Bytes                    | Bytes          | memorySizeMB - Total memory size|\n+| Network     | WriteBytesPerSecond   | Network Write Bytes Per Second            | BytesPerSecond | NetworkDeviceId - Id of the device<br>bytes - Total sent bytes |\n+| Network     | ReadBytesPerSecond    | Network Read Bytes Per Second             | BytesPerSecond | networkDeviceId - Id of the device<br>bytes - Total received bytes |\n+| Processor   | UtilizationPercentage | Processor Utilization Percentage          | Percent        | totalCpus - Total CPUs |\n+| LogicalDisk | WritesPerSecond       | Logical Disk Writes Per Second            | CountPerSecond | mountId - Mount ID of the device |\n+| LogicalDisk | WriteLatencyMs        | Logical Disk Write Latency Millisecond    | MilliSeconds   | mountId - Mount ID of the device |\n+| LogicalDisk | WriteBytesPerSecond   | Logical Disk Write Bytes Per Second       | BytesPerSecond | mountId - Mount ID of the device |\n+| LogicalDisk | TransfersPerSecond    | Logical Disk Transfers Per Second         | CountPerSecond | mountId - Mount ID of the device |\n+| LogicalDisk | TransferLatencyMs     | Logical Disk Transfer Latency Millisecond | MilliSeconds   | mountId - Mount ID of the device |\n+| LogicalDisk | ReadsPerSecond        | Logical Disk Reads Per Second             | CountPerSecond | mountId - Mount ID of the device |\n+| LogicalDisk | ReadLatencyMs         | Logical Disk Read Latency Millisecond     | MilliSeconds   | mountId - Mount ID of the device |\n+| LogicalDisk | ReadBytesPerSecond    | Logical Disk Read Bytes Per Second        | BytesPerSecond | mountId - Mount ID of the device |\n+| LogicalDisk | FreeSpacePercentage   | Logical Disk Free Space Percentage        | Percent        | mountId - Mount ID of the device |\n+| LogicalDisk | FreeSpaceMB           | Logical Disk Free Space Bytes             | Bytes          | mountId - Mount ID of the device<br>diskSizeMB - Total disk size |\n+| LogicalDisk | BytesPerSecond        | Logical Disk Bytes Per Second             | BytesPerSecond | mountId - Mount ID of the device |\n+\n+\n ## Next steps\n \n * If you are new to writing log queries in Azure Monitor, review [how to use Log Analytics](../../azure-monitor/log-query/get-started-portal.md) in the Azure portal to write log queries."
  },
  {
    "Number": 108727,
    "Title": "Azure Monitor VM Insights Updates",
    "ClosedAt": "2020-03-25T19:20:45Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/vminsights-maps.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -5,7 +5,7 @@ ms.subservice:\n ms.topic: conceptual\n author: bwren\n ms.author: bwren\n-ms.date: 10/15/2019\n+ms.date: 03/20/2020\n \n ---\n "
  },
  {
    "Number": 108727,
    "Title": "Azure Monitor VM Insights Updates",
    "ClosedAt": "2020-03-25T19:20:45Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/vminsights-performance.md",
    "Addition": 0,
    "Delections": 4,
    "Changes": 4,
    "Patch": "@@ -114,11 +114,7 @@ Clicking on the pin icon at the upper right-hand corner of any one of the charts\n >[!NOTE]\n >You can also access a detailed performance view for a specific instance from the Instances view for your scale set. Navigate to **Instances** under the **Settings** section, and then choose **Insights**.\n \n-## Alerts  \n \n-Performance metrics enabled as part of Azure Monitor for VMs do not include pre-configured alert rules. However, we may only collect and store a subset of the performance metrics you require in the Log Analytics workspace. If your monitoring strategy requires analysis or alerting that includes other performance metrics in order to effectively evaluate capacity or health of the virtual machine, or you need the flexibility to specify your own alerting criteria or logic, you can configure [collection of those performance counters](../platform/data-sources-performance-counters.md) in Log Analytics and define [log alerts](../platform/alerts-log.md). While Log Analytics allows you to perform complex analysis with other data types, and provide longer retention to support trend analysis, metrics on the other hand, are lightweight and capable of supporting near real-time scenarios. They are collected by the [Azure Diagnostic agent](../../virtual-machines/windows/monitor.md) and stored in the Azure Monitor metrics store, allowing you to create alerts with lower latency and at a lower cost.\n-\n-Review the overview of [collection of metrics and logs with Azure Monitor](../platform/data-platform.md) to further understand the fundamental differences and other considerations before configuring collection of these additional metrics and alert rules.  \n \n ## Next steps\n "
  },
  {
    "Number": 108962,
    "Title": "Azure Monitor partners update",
    "ClosedAt": "2020-03-25T18:40:00Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/platform/partners.md",
    "Addition": 8,
    "Delections": 0,
    "Changes": 8,
    "Patch": "@@ -201,6 +201,14 @@ Serverless360 is a one platform tool to operate, manage, and monitor Azure serve\n \n [Learn more.][serverless360-doc]\n \n+## ServiceNow\n+\n+![ServiceNow Logo](./media/partners/servicenow.png)\n+\n+Reduce incidents and MTTR with NOW AIOps platform to eliminate noise, prioritize, identify root cause detection using ML, and remediate with ITX workflows.  Understand the current state of your Iaas/PaaS/FaaS services from Azure and build service maps from tags to build application service context for the business impact analysis.    \n+\n+[Learn more.](https://www.servicenow.com/solutions/aiops.html)\n+\n ## SignalFx\n \n ![SignalFX Logo](./media/partners/signalfx.png)"
  },
  {
    "Number": 62182,
    "Title": "added links to template ref for monitor",
    "ClosedAt": "2019-01-04T20:54:17Z",
    "User": "tfitzmac",
    "FileName": "articles/azure-monitor/platform/alerts-enable-template.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -398,4 +398,4 @@ An alert on a Resource Manager template is most often useful when creating an al\n ## Next Steps\n * [Read more about Alerts](alerts-overview.md)\n * [Add Diagnostic Settings](../../azure-monitor/platform/diagnostic-logs-stream-template.md) to your Resource Manager template\n-\n+* For the JSON syntax and properties, see [Microsoft.Insights/alertrules](/azure/templates/microsoft.insights/alertrules) template reference."
  },
  {
    "Number": 62182,
    "Title": "added links to template ref for monitor",
    "ClosedAt": "2019-01-04T20:54:17Z",
    "User": "tfitzmac",
    "FileName": "articles/azure-monitor/platform/alerts-metric-create-templates.md",
    "Addition": 1,
    "Delections": 0,
    "Changes": 1,
    "Patch": "@@ -1368,3 +1368,4 @@ az group deployment create \\\n ## Next steps\n * Read more about [alerts in Azure](alerts-overview.md)\n * Learn how to [create an action group with Resource Manager templates](action-groups-create-resource-manager-template.md)\n+* For the JSON syntax and properties, see [Microsoft.Insights/metricAlerts](/azure/templates/microsoft.insights/metricalerts) template reference."
  },
  {
    "Number": 62182,
    "Title": "added links to template ref for monitor",
    "ClosedAt": "2019-01-04T20:54:17Z",
    "User": "tfitzmac",
    "FileName": "articles/azure-monitor/platform/autoscale-virtual-machine-scale-sets.md",
    "Addition": 2,
    "Delections": 0,
    "Changes": 2,
    "Patch": "@@ -243,3 +243,5 @@ Use these links to learn more about autoscaling.\n [Manage Autoscale using CLI](cli-samples.md#autoscale)\n \n [Configure Webhook & Email Notifications for Autoscale](autoscale-webhook-email.md)\n+\n+[Microsoft.Insights/autoscalesettings](/azure/templates/microsoft.insights/autoscalesettings) template reference\n\\ No newline at end of file"
  },
  {
    "Number": 108579,
    "Title": "update due to APIs changes",
    "ClosedAt": "2020-03-25T15:23:33Z",
    "User": "yossi-y",
    "FileName": "articles/azure-monitor/platform/customer-managed-keys.md",
    "Addition": 115,
    "Delections": 42,
    "Changes": 157,
    "Patch": "@@ -5,7 +5,7 @@ ms.subservice: logs\n ms.topic: conceptual\n author: yossi-y\n ms.author: yossiy\n-ms.date: 02/24/2020\n+ms.date: 03/22/2020\n \n ---\n # Azure Monitor customer-managed key configuration \n@@ -106,10 +106,10 @@ For Application Insights CMK configuration, follow the Appendix content for step\n 1. Subscription whitelisting -- this is required for this early access\n     feature\n 2. Creating Azure Key Vault and storing key\n-3. Create a *Cluster* resource\n+3. Creating a *Cluster* resource\n 4. Azure Monitor data-store (ADX cluster) provisioning\n-5. Grant permissions to your Key Vault\n-6. Log Analytics workspaces association\n+5. Granting permissions to your Key Vault\n+6. Associating Log Analytics workspaces\n \n The procedure is not supported in the UI currently and the provisioning process is performed via REST API.\n \n@@ -142,13 +142,13 @@ You can acquire the token using one of these methods:\n CMK capability is an early access feature. The subscriptions where you plan to create *Cluster* resources must be whitelisted beforehand by the Azure product group. Use your contacts into Microsoft to provide your Subscriptions IDs.\n \n > [!IMPORTANT]\n-> CMK capability is regional. Your Azure Key Vault, Storage Account, *Cluster* resource and associated Log Analytics workspaces must be in the same region, but they can be in different subscriptions.\n+> CMK capability is regional. Your Azure Key Vault, *Cluster* resource and associated Log Analytics workspaces must be in the same region, but they can be in different subscriptions.\n \n ### Storing encryption key (KEK)\n \n-Create an Azure Key Vault resource, then generate or import a key to be used for data encryption.\n+Create or use an Azure Key Vault that you already have, to generate or import a key to be used for data encryption.\n \n-The Azure Key Vault must be configured as recoverable to protect your key and the access to your Azure Monitor data.\n+The Azure Key Vault must be configured as recoverable to protect your key and the access to your data in Azure Monitor.\n \n These settings are available via CLI and PowerShell:\n - [Soft Delete](https://docs.microsoft.com/azure/key-vault/key-vault-ovw-soft-delete)\n@@ -157,9 +157,11 @@ These settings are available via CLI and PowerShell:\n \n ### Create *Cluster* resource\n \n-This resource is used as intermediate identity connection between your Key Vault and your workspaces. After you receive confirmation that your subscriptions were whitelisted, create a Log Analytics *Cluster* resource at the region where your workspaces are located. Application Insights and Log Analytics require separate Cluster resources. The type of the *Cluster* resource is defined at creation time by setting the “clusterType” property to either ‘LogAnalytics’, or ‘ApplicationInsights’. The Cluster resource type can’t be altered.\n+This resource is used as an intermediate identity connection between your Key Vault and your workspaces. After you receive confirmation that your subscriptions were whitelisted, create a Log Analytics *Cluster* resource at the region where your workspaces are located. Application Insights and Log Analytics require separate *Cluster* resources types. The type of the *Cluster* resource is defined at creation time by setting the \"clusterType\" property to either \"LogAnalytics\", or \"ApplicationInsights\". The Cluster resource type can’t be altered after.\n \n-For Application Insights CMK configuration, follow the Appendix content for this step.\n+For Application Insights CMK configuration, follow the Appendix content.\n+\n+You must specify the capacity reservation level (sku) for the *Cluster* resource. The capacity reservation level can be in the range of 1000 to 2000 and in steps of 100. If you need capacity reservation level higher than 2000, reach your Microsoft contact to enable it. This property doesn’t affect billing currently -- once pricing model for dedicated cluster is introduced, billing will apply to any existing CMK deployments.\n \n **Create**\n \n@@ -171,15 +173,18 @@ Content-type: application/json\n {\n   \"location\": \"<region-name>\",\n    \"properties\": {\n-      \"clusterType\": \"LogAnalytics\"\n+      \"clusterType\": \"LogAnalytics\",\n+      \"sku\": {\n+       \"name\": \"CapacityReservation\",\n+       \"capacityReservationLevel\": 1000\n+       }\n     },\n    \"identity\": {\n       \"type\": \"systemAssigned\"\n    }\n }\n ```\n The identity is assigned to the *Cluster* resource at creation time.\n-\"clusterType\" value is \"ApplicationInsights\" for Application Insights CMK.\n \n **Response**\n \n@@ -194,7 +199,11 @@ https://management.azure.com/subscriptions/<subscription-id>/resourceGroups/<res\n \n ### Azure Monitor data-store (ADX cluster) provisioning\n \n-During the early access period of the feature, the ADX cluster is provisioned manually by the product team once the previous steps are completed. Use your Microsoft channel to provide the *Cluster* resource details. Copy the JSON response from the *Cluster* resource GET REST API:\n+During the early access period of the feature, the ADX cluster is provisioned manually by the product team once the previous steps are completed. Use your Microsoft channel to provide the *Cluster* resource details. \n+\n+> [!IMPORTANT]\n+> Copy and provide the JSON response of the *Cluster* resource GET REST API\n+> You will need details from this response for later steps too\n \n ```rst\n GET https://management.azure.com/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.OperationalInsights/clusters/<cluster-name>?api-version=2019-08-01-preview\n@@ -210,10 +219,15 @@ Authorization: Bearer <token>\n     \"principalId\": \"principal-id\"\n     },\n   \"properties\": {\n-    \"provisioningState\": \"Succeeded\",\n+    \"provisioningState\": \"ProvisioningAccount\",\n     \"clusterType\": \"LogAnalytics\", \n     \"clusterId\": \"cluster-id\"\n-    },\n+    \"sku\": {\n+      \"name\": \"CapacityReservation\",\n+      \"capacityReservationLevel\": 1000,\n+      \"lastSkuUpdate\": \"Sun, 22 Mar 2020 15:39:29 GMT\"\n+      }\n+  },\n   \"id\": \"/subscriptions/subscription-id/resourceGroups/resource-group-name/providers/Microsoft.OperationalInsights/clusters/cluster-name\",\n   \"name\": \"cluster-name\",\n   \"type\": \"Microsoft.OperationalInsights/clusters\",\n@@ -223,17 +237,9 @@ Authorization: Bearer <token>\n \n \"principal-id\" is a GUID generated by the managed identity service for the *Cluster* resource.\n \n-> [!IMPORTANT]\n-> Copy and keep the \"principal-id\" value since you will need it in next steps.\n-\n-\n ### Grant Key Vault permissions\n \n-> [!IMPORTANT]\n-> This step should be carried after you received confirmation from the product group through your Microsoft channel that the Azure Monitor data-store (ADX cluster) provisioning was fulfilled. Updating the Key Vault access policy prior to this provisioning may fail.\n-\n-Update your Key Vault with a new access policy that grant permissions to your *Cluster* resource. These permissions are used by the underlaying Azure Monitor Storage for data encryption.\n-Open your Key Vault in Azure portal and click \"Access Policies\" then \"+ Add Access Policy\" to create a new policy with these settings:\n+Update your Key Vault with a new access policy that grants permissions to your *Cluster* resource. These permissions are used by the underlay Azure Monitor Storage for data encryption. Open your Key Vault in Azure portal and click \"Access Policies\" then \"+ Add Access Policy\" to create a policy with these settings:\n \n - Key permissions: select 'Get', 'Wrap Key' and 'Unwrap Key' permissions.\n - Select principal: enter the principal-id value that returned in the response in the previous step.\n@@ -244,7 +250,9 @@ The *Get* permission is required to verify that your Key Vault is configured as\n \n ### Update Cluster resource with Key identifier details\n \n-This step applies for future key version updates in your Key Vault. Update the *Cluster* resource with Key Vault *Key identifier* details, to allow Azure Monitor Storage to use the new key version. Select the current version of your key in Azure Key Vault to get the Key identifier details.\n+This step applies per initial and future key version updates in your Key Vault. It informs Azure Monitor Storage about the new key version.\n+\n+To update the *Cluster* resource with your Key Vault *Key identifier* details, select the current version of your key in Azure Key Vault to get the Key identifier details.\n \n ![Grant Key Vault permissions](media/customer-managed-keys/key-identifier-8bit.png)\n \n@@ -284,11 +292,11 @@ Content-type: application/json\n     \"principalId\": \"principle-id\"\n   },\n   \"properties\": {\n-       \"KeyVaultProperties\": {\n-            KeyVaultUri: \"https://key-vault-name.vault.azure.net\",\n-            KeyName: \"key-name\",\n-            KeyVersion: \"current-version\"\n-            },\n+    \"KeyVaultProperties\": {\n+      KeyVaultUri: \"https://key-vault-name.vault.azure.net\",\n+      KeyName: \"key-name\",\n+      KeyVersion: \"current-version\"\n+      },\n     \"provisioningState\": \"Succeeded\",\n     \"clusterType\": \"LogAnalytics\", \n     \"clusterId\": \"cluster-id\"\n@@ -302,20 +310,48 @@ Content-type: application/json\n \n ### Workspace association to *Cluster* resource\n \n-> [!NOTE]\n-> This step should be carried **ONLY** after you received confirmation\n-> from the product group through your Microsoft channel that the\n-> **Azure Monitor data-store (ADX cluster) provisioning** was\n-> fulfilled. If you associate workspaces and ingest data prior to this\n-> **provisioning**, the data will be dropped and won't be recoverable.\n+> [!IMPORTANT]\n+> This step should be carried after the ADX cluster provisioning. If you associate workspaces and ingest data prior to the provisioning, ingested data before the provisioning will be dropped and won't be recoverable.\n+> To verify that the ADX cluster is provisioned and you can start associating workspaces to it, execute the this REST API and check that \"provisioningState\" value in the response is \"Succeeded\".\n+\n+```rst\n+GET https://management.azure.com/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.OperationalInsights/clusters/<cluster-name>?api-version=2019-08-01-preview\n+Authorization: Bearer <token>\n+```\n+\n+**Response**\n+```json\n+{\n+  \"identity\": {\n+    \"type\": \"SystemAssigned\",\n+    \"tenantId\": \"tenant-id\",\n+    \"principalId\": \"principal-id\"\n+    },\n+  \"properties\": {\n+    \"provisioningState\": \"Succeeded\",\n+    \"clusterType\": \"LogAnalytics\", \n+    \"clusterId\": \"cluster-id\"\n+    \"sku\": {\n+      \"name\": \"CapacityReservation\",\n+      \"capacityReservationLevel\": 1000,\n+      \"lastSkuUpdate\": \"Sun, 22 Mar 2020 15:39:29 GMT\"\n+      }\n+  },\n+  \"id\": \"/subscriptions/subscription-id/resourceGroups/resource-group-name/providers/Microsoft.OperationalInsights/clusters/cluster-name\",\n+  \"name\": \"cluster-name\",\n+  \"type\": \"Microsoft.OperationalInsights/clusters\",\n+  \"location\": \"region-name\"\n+  }\n+```\n \n For Application Insights CMK configuration, follow the Appendix content for this step.\n \n-You need to have ‘write’ permissions on both your workspace and *Cluster* resource to perform this operation, which include these actions:\n+You need to have 'write' permissions to both your workspace and *Cluster* resource to perform this operation, which include these actions:\n \n - In workspace: Microsoft.OperationalInsights/workspaces/write\n - In *Cluster* resource: Microsoft.OperationalInsights/clusters/write\n \n+**Associate a workspace**\n ```rst\n PUT https://management.azure.com/subscriptions/<subscription-id>/resourcegroups/<resource-group-name>/providers/microsoft.operationalinsights/workspaces/<workspace-name>/linkedservices/cluster?api-version=2019-08-01-preview \n Authorization: Bearer <token>\n@@ -414,7 +450,7 @@ All your data is accessible after the key rotation operation including data inge\n \n - CMK encryption applies to newly ingested data after the CMK\n     configuration. Data that was ingested prior to the CMK\n-    configuration, remaines encrypted with Microsoft key. You can query\n+    configuration, remains encrypted with Microsoft key. You can query\n     data before and after the CMK configuration seamlessly.\n \n - Once workspace is associated to a *Cluster* resource, it cannot be\n@@ -455,7 +491,7 @@ All your data is accessible after the key rotation operation including data inge\n \n - If you try to delete a *Cluster* resource that is associated to a workspace, the delete operation will fail.\n \n-- Use this API to get all *Cluster* resources for a resource group:\n+- Get all *Cluster* resources for a resource group:\n \n   ```rst\n   GET https://management.azure.com/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.OperationalInsights/clusters?api-version=2019-08-01-preview\n@@ -492,7 +528,7 @@ All your data is accessible after the key rotation operation including data inge\n   }\n   ```\n \n-- Use this API call to Get all *Cluster* resources for a subscription:\n+- Get all *Cluster* resources for a subscription:\n \n   ```rst\n   GET https://management.azure.com/subscriptions/<subscription-id>/providers/Microsoft.OperationalInsights/clusters?api-version=2019-08-01-preview\n@@ -503,8 +539,7 @@ All your data is accessible after the key rotation operation including data inge\n     \n   The same response as for '*Cluster* resources for a resource group', but in subscription scope.\n     \n-- Use this API call to delete a *Cluster* resource -- You need to delete all the associated workspaces before you can delete\n-your *Cluster* resource:\n+- Delete a *Cluster* resource -- a soft-delete operation is performed to allow the recovery of your *Cluster* resource, your data and associated workspaces within 14 days, whether the deletion was accidental or intentional. After the soft-delete period, your *Cluster* resource and data are non-recoverable. The *Cluster* resource name remains reserved during the soft-delete period and you can’t create a new cluster with that name.\n \n   ```rst\n   DELETE\n@@ -516,6 +551,8 @@ your *Cluster* resource:\n \n   200 OK\n \n+- Recover your *Cluster* resource and your data -- during the soft-delete period, create a *Cluster* resource with the same name and in the same subscription, resource group and region. Follow the **Create *Cluster* resource** step to recover your *Cluster* resource.\n+\n \n ## Appendix\n \n@@ -595,11 +632,47 @@ Identity is assigned to the *Cluster* resource at creation time.\n \n ### Associate a component to a *Cluster* resource using [Components - Create Or Update](https://docs.microsoft.com/rest/api/application-insights/components/createorupdate) API\n \n-You need to have ‘write’ permissions on both your component and *Cluster* resource to perform this operation, which include these actions:\n+You need to have 'write' permissions on both your component and *Cluster* resource to perform this operation, which include these actions:\n \n - In component: Microsoft.Insights/component/write\n - In *Cluster* resource: Microsoft.OperationalInsights/clusters/write\n \n+> [!IMPORTANT]\n+> This step should be carried after the ADX cluster provisioning. If you associate a component and ingest data prior to the provisioning, ingested data before the provisioning will be dropped and won't be recoverable.\n+> To verify that the ADX cluster is provisioned and you can start associating component to it, execute the this REST API and check that \"provisioningState\" value in the response is \"Succeeded\".\n+\n+```rst\n+GET https://management.azure.com/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.OperationalInsights/clusters/<cluster-name>?api-version=2019-08-01-preview\n+Authorization: Bearer <token>\n+```\n+\n+**Response**\n+```json\n+{\n+  \"identity\": {\n+    \"type\": \"SystemAssigned\",\n+    \"tenantId\": \"tenant-id\",\n+    \"principalId\": \"principal-id\"\n+    },\n+  \"properties\": {\n+    \"provisioningState\": \"Succeeded\",\n+    \"clusterType\": \"ApplicationInsights\", \n+    \"clusterId\": \"cluster-id\"\n+    \"sku\": {\n+      \"name\": \"CapacityReservation\",\n+      \"capacityReservationLevel\": 1000,\n+      \"lastSkuUpdate\": \"Sun, 22 Mar 2020 15:39:29 GMT\"\n+      }\n+  },\n+  \"id\": \"/subscriptions/subscription-id/resourceGroups/resource-group-name/providers/Microsoft.OperationalInsights/clusters/cluster-name\",\n+  \"name\": \"cluster-name\",\n+  \"type\": \"Microsoft.OperationalInsights/clusters\",\n+  \"location\": \"region-name\"\n+  }\n+```\n+\n+**Associate a component**\n+\n ```rst\n PUT https://management.azure.com/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.Insights/components/<component-name>?api-version=2015-05-01\n Authorization: Bearer <token>"
  },
  {
    "Number": 108910,
    "Title": "Some Github work",
    "ClosedAt": "2020-03-25T00:01:11Z",
    "User": "normesta",
    "FileName": "articles/storage/blobs/storage-blob-static-website-how-to.md",
    "Addition": 10,
    "Delections": 3,
    "Changes": 13,
    "Patch": "@@ -234,11 +234,15 @@ Write-Output $storageAccount.PrimaryEndpoints.Web\n \n Once you've enabled metrics, traffic statistics on files in the **$web** container are reported in the metrics dashboard.\n \n-1. Click on **Settings** > **Monitoring** > **Metrics**.\n+1. Click **Metrics** under the **Monitor** section of the storage account menu.\n \n-   Metrics data are generated by hooking into different metrics APIs. The portal only displays API members used within a given time frame in order to only focus on members that return data. In order to ensure you're able to select the necessary API member, the first step is to expand the time frame.\n+   > [!div class=\"mx-imgBorder\"]\n+   > ![Metrics link](./media/storage-blob-static-website/metrics-link.png)\n \n-2. Click on the time frame button and select **Last 24 hours** and then click **Apply**.\n+   > [!NOTE]\n+   > Metrics data are generated by hooking into different metrics APIs. The portal only displays API members used within a given time frame in order to only focus on members that return data. In order to ensure you're able to select the necessary API member, the first step is to expand the time frame.\n+\n+2. Click on the time frame button, choose a time frame, and then click **Apply**.\n \n    ![Azure Storage static websites metrics time range](./media/storage-blob-static-website/storage-blob-static-website-metrics-time-range.png)\n \n@@ -262,6 +266,9 @@ Once you've enabled metrics, traffic statistics on files in the **$web** contain\n \n    ![Azure Storage static websites metrics GetWebContent](./media/storage-blob-static-website/storage-blob-static-website-metrics-getwebcontent.png)\n \n+   >[!NOTE]\n+   > The **GetWebContent** checkbox appears only if that API member was used within a given time frame. The portal only displays API members used within a given time frame in order to only focus on members that return data. If you can't find a specific API member in this list, expand the time frame.\n+\n ## Next steps\n \n * Learn how to configure a custom domain with your static website. See [Map a custom domain to an Azure Blob Storage endpoint](storage-custom-domain-name.md)."
  },
  {
    "Number": 108910,
    "Title": "Some Github work",
    "ClosedAt": "2020-03-25T00:01:11Z",
    "User": "normesta",
    "FileName": "articles/storage/common/storage-metrics-in-azure-monitor.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -376,7 +376,7 @@ Azure Storage provides the following transaction metrics in Azure Monitor.\n | ------------------- | ----------------- |\n | Transactions | The number of requests made to a storage service or the specified API operation. This number includes successful and failed requests, as well as requests that produced errors. <br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Applicable dimensions: ResponseType, GeoType, ApiName, and Authentication ([Definition](#metrics-dimensions))<br/> Value example: 1024 |\n | Ingress | The amount of ingress data. This number includes ingress from an external client into Azure Storage as well as ingress within Azure. <br/><br/> Unit: Bytes <br/> Aggregation Type: Total <br/> Applicable dimensions: GeoType, ApiName, and Authentication ([Definition](#metrics-dimensions)) <br/> Value example: 1024 |\n-| Egress | The amount of egress data. This number includes egress from an external client into Azure Storage as well as egress within Azure. As a result, this number does not reflect billable egress. <br/><br/> Unit: Bytes <br/> Aggregation Type: Total <br/> Applicable dimensions: GeoType, ApiName, and Authentication ([Definition](#metrics-dimensions)) <br/> Value example: 1024 |\n+| Egress | The amount of egress data. This number includes egress to an external client from Azure Storage as well as egress within Azure. As a result, this number does not reflect billable egress. <br/><br/> Unit: Bytes <br/> Aggregation Type: Total <br/> Applicable dimensions: GeoType, ApiName, and Authentication ([Definition](#metrics-dimensions)) <br/> Value example: 1024 |\n | SuccessServerLatency | The average time used to process a successful request by Azure Storage. This value does not include the network latency specified in SuccessE2ELatency. <br/><br/> Unit: Milliseconds <br/> Aggregation Type: Average <br/> Applicable dimensions: GeoType, ApiName, and Authentication ([Definition](#metrics-dimensions)) <br/> Value example: 1024 |\n | SuccessE2ELatency | The average end-to-end latency of successful requests made to a storage service or the specified API operation. This value includes the required processing time within Azure Storage to read the request, send the response, and receive acknowledgment of the response. <br/><br/> Unit: Milliseconds <br/> Aggregation Type: Average <br/> Applicable dimensions: GeoType, ApiName, and Authentication ([Definition](#metrics-dimensions)) <br/> Value example: 1024 |\n | Availability | The percentage of availability for the storage service or the specified API operation. Availability is calculated by taking the total billable requests value and dividing it by the number of applicable requests, including those requests that produced unexpected errors. All unexpected errors result in reduced availability for the storage service or the specified API operation. <br/><br/> Unit: Percent <br/> Aggregation Type: Average <br/> Applicable dimensions: GeoType, ApiName, and Authentication ([Definition](#metrics-dimensions)) <br/> Value example: 99.99 |\n@@ -392,7 +392,7 @@ Azure Storage supports following dimensions for metrics in Azure Monitor.\n | **GeoType** | Transaction from Primary or Secondary cluster. The available values include **Primary** and **Secondary**. It applies to Read Access Geo Redundant Storage(RA-GRS) when reading objects from secondary tenant. |\n | **ResponseType** | Transaction response type. The available values include: <br/><br/> <li>**ServerOtherError**: All other server-side errors except described ones </li> <li>**ServerBusyError**: Authenticated request that returned an HTTP 503 status code. </li> <li>**ServerTimeoutError**: Timed-out authenticated request that returned an HTTP 500 status code. The timeout occurred due to a server error. </li> <li>**AuthorizationError**: Authenticated request that failed due to unauthorized access of data or an authorization failure. </li> <li>**NetworkError**: Authenticated request that failed due to network errors. Most commonly occurs when a client prematurely closes a connection before timeout expiration. </li>  <li>**ClientAccountBandwidthThrottlingError**: The request is throttled on bandwidth for exceeding [storage account scalability limits](scalability-targets-standard-account.md).</li><li>**ClientAccountRequestThrottlingError**: The request is throttled on request rate for exceeding [storage account scalability limits](scalability-targets-standard-account.md).<li>**ClientThrottlingError**: Other client-side throttling error. ClientAccountBandwidthThrottlingError and ClientAccountRequestThrottlingError are excluded.</li> <li>**ClientTimeoutError**: Timed-out authenticated request that returned an HTTP 500 status code. If the client's network timeout or the request timeout is set to a lower value than expected by the storage service, it is an expected timeout. Otherwise, it is reported as a ServerTimeoutError.</li> </li> <li>**ClientOtherError**: All other client-side errors except described ones. </li> <li>**Success**: Successful request</li> <li> **SuccessWithThrottling**: Successful request when a SMB client gets throttled in the first attempt(s) but succeeds after retries.</li> |\n | **ApiName** | The name of operation. For example: <br/> <li>**CreateContainer**</li> <li>**DeleteBlob**</li> <li>**GetBlob**</li> For all operation names, see [document](/rest/api/storageservices/storage-analytics-logged-operations-and-status-messages). |\n-| **Authentication** | Authentication type used in transactions. The available values include: <br/> <li>**AccountKey**: The transaction is authenticated with storage account key.</li> <li>**SAS**: The transaction is authenticated with shared access signatures.</li> <li>**OAuth**: The transaction is authenticated with OAuth access tokens.</li> <li>**Anonymous**: The transaction is requested anonymously. It doesn’t include preflight requests.</li> <li>**AnonymousPreflight**: The transaction is preflight request.</li> |\n+| **Authentication** | Authentication type used in transactions. The available values include: <br/> <li>**AccountKey**: The transaction is authenticated with storage account key.</li> <li>**SAS**: The transaction is authenticated with shared access signatures.</li> <li>**OAuth**: The transaction is authenticated with OAuth access tokens.</li> <li>**Anonymous**: The transaction is requested anonymously. It doesn't include preflight requests.</li> <li>**AnonymousPreflight**: The transaction is preflight request.</li> |\n \n For the metrics supporting dimensions, you need to specify the dimension value to see the corresponding metrics values. For example, if you look at  **Transactions** value for successful responses, you need to filter the **ResponseType** dimension with **Success**. Or if you look at **BlobCount** value for Block Blob, you need to filter the **BlobType** dimension with **BlockBlob**.\n "
  },
  {
    "Number": 108890,
    "Title": "note on CPU and Memory",
    "ClosedAt": "2020-03-24T21:43:25Z",
    "User": "spelluru",
    "FileName": "articles/service-bus-messaging/service-bus-metrics-azure-monitor.md",
    "Addition": 4,
    "Delections": 0,
    "Changes": 4,
    "Patch": "@@ -100,6 +100,10 @@ The following two types of errors are classified as user errors:\n \n > [!NOTE] \n > The following metrics are available only with the **premium** tier. \n+> \n+> The important metrics to monitor for any outages for a premium tier namespace are: **CPU usage per namespace** and **memory size per namespace**. [Set up alerts](../azure-monitor/platform/alerts-metric.md) for these metrics using Azure Monitor.\n+> \n+> The other metric you could monitor is: **throttled requests**. It shouldn't be an issue though as long as the namespace stays within its memory, CPU, and brokered connections limits. For more information, see [Throttling in Azure Service Bus Premium tier](service-bus-throttling.md#throttling-in-azure-service-bus-premium-tier)\n \n | Metric Name | Description |\n | ------------------- | ----------------- |"
  },
  {
    "Number": 108563,
    "Title": "Azure Monitor VM Insights GA 2",
    "ClosedAt": "2020-03-23T22:25:38Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/monitor-vm-azure.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -161,7 +161,7 @@ Azure Monitor for VMs enables the collection of a predetermined set of performan\n \n | Data source | Requirements | Tables |\n |:---|:---|:---|\n-| Azure Monitor for VMs | Enable on each virtual machine. | InsightsMetrics<br>VMBoundPort<br>VMComputer<br>VMConnection<br>VMProcess<br>See [How to query logs from Azure Monitor for VMs](vminsights-log-search.md#sample-log-searches) for details. |\n+| Azure Monitor for VMs | Enable on each virtual machine. | InsightsMetrics<br>VMBoundPort<br>VMComputer<br>VMConnection<br>VMProcess<br>See [How to query logs from Azure Monitor for VMs](vminsights-log-search.md) for details. |\n | Activity log | Diagnostic setting for the subscription. | AzureActivity |\n | Host metrics | Diagnostic setting for the virtual machine. | AzureMetrics |\n | Data sources from the guest operating system | Enable Log Analytics agent and configure data sources. | See documentation for each data source. |"
  },
  {
    "Number": 108563,
    "Title": "Azure Monitor VM Insights GA 2",
    "ClosedAt": "2020-03-23T22:25:38Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/vminsights-alerts.md",
    "Addition": 195,
    "Delections": 0,
    "Changes": 195,
    "Patch": "@@ -0,0 +1,195 @@\n+---\n+title: Alerts from Azure Monitor for VMs\n+description: Describes how to create alert rules from performance data collected by Azure Monitor for VMs.\n+ms.subservice: \n+ms.topic: conceptual\n+author: bwren\n+ms.author: bwren\n+ms.date: 03/23/2019\n+\n+---\n+\n+# How to create alerts from Azure Monitor for VMs\n+[Alerts in Azure Monitor](../platform/alerts-overview.md) proactively notify you of interesting data and patterns in your monitoring data. Azure Monitor for VMs does not include pre-configured alert rules, but you can create your own based on data that it collects. This article provides guidance on creating alert rules, including a set of sample queries.\n+\n+\n+## Alert rule types\n+Azure Monitor has [different types of alert rules](../platform/alerts-overview.md#what-you-can-alert-on) based on the data being used to create the alert. All data collected by Azure Monitor for VMs is stored in Azure Monitor Logs which supports [log alerts](../platform/alerts-log.md). You cannot currently use [metric alerts](../platform/alerts-log.md) with performance data collected from Azure Monitor for VMs because the data is not collected into Azure Monitor Metrics. To collect data for metric alerts, install the [diagnostics extension](../platform/diagnostics-extension-overview.md) for Windows VMs or the [Telegraf agent](../platform/collect-custom-metrics-linux-telegraf.md) for Linux VMs to collect performance data into Metrics.\n+\n+There are two types of log alerts in Azure Monitor:\n+\n+- [Number of results alerts](../platform/alerts-unified-log.md#number-of-results-alert-rules) create a single alert when a query returns at least a specified number of records. These are ideal for non-numeric data such and Windows and Syslog events collected by the [Log Analytics agent](../platform/log-analytics-agent.md) or for analyzing performance trends across multiple computers.\n+- [Metric measurement alerts](../platform/alerts-unified-log.md#metric-measurement-alert-rules) create a separate alert for each record in a query that has a value that exceeds a threshold defined in the alert rule. These alert rules are ideal for performance data collected by Azure Monitor for VMs since they can create individual alerts for each computer.\n+\n+\n+## Alert rule details\n+Since the target resource for log alert rules is always a Log Analytics workspace, the log query must include any filter for particular virtual machines or virtual machine scale sets. For metric measurement alert rules, summarize the query results by computer in order to evaluate each separately. \n+\n+Start by creating a new alert rule following the procedure in [Create, view, and manage log alerts using Azure Monitor](../platform/alerts-log.md). For the **Resource**, select the Log Analytics workspace that Azure Monitor VMs uses in your subscription.\n+\n+For the **Condition** of the alert rule, provide one of the queries in the section below as the **Search query**. The query must return a numeric property called *AggregatedValue*. It should summarize the data by computer so that you can create a separate alert for each virtual machine that exceeds the threshold.\n+\n+In the **Alert logic**, select **Metric measurement** and then provide a **Threshold value**. In **Trigger Alert Based On**, specify how many times the threshold must be exceeded before an alert is created. For example, you probably don't care if the processor exceeds a threshold once and then returns to normal, but you do care if it continues to exceed the threshold over multiple measurements.\n+\n+The **Evaluated based on** section defines how often the query is run and the time window for the query. In the example shown below, the query will run every 15 minutes and evalute performance collected over the previous 15 minutes.\n+\n+\n+![Metric measurement alert rule](media/vminsights-alerts/metric-measurement-alert.png)\n+\n+## Sample alert queries\n+The following queries can be used with a metric measurement alert rule using performance data collected by Azure Monitor for VMs. Each summarizes data by computer so that an alert is created for each computer with a value that exceeds the threshold.\n+\n+### CPU utilization\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\" \n+| where Namespace == \"Processor\" and Name == \"UtilizationPercentage\" \n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), Computer, _ResourceId\n+```\n+\n+### Available Memory in MB\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where Namespace == \"Memory\" and Name == \"AvailableMB\"\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), Computer, _ResourceId\n+```\n+\n+### Available Memory in percentage\n+\n+```kusto\n+InsightsMetrics \n+| where Origin == \"vm.azm.ms\" \n+| where Namespace == \"Memory\" and Name == \"AvailableMB\" \n+| extend TotalMemory = toreal(todynamic(Tags)[\"vm.azm.ms/memorySizeMB\"])\n+| extend AvailableMemoryPercentage = (toreal(Val) / TotalMemory) * 100.0 \n+| summarize AggregatedValue = avg(AvailableMemoryPercentage) by bin(TimeGenerated, 15m), Computer, _ResourceId \n+```\n+\n+### Logical disk used - all disks on each computer\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where Namespace == \"LogicalDisk\" and Name == \"FreeSpacePercentage\"\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), Computer, _ResourceId \n+```\n+\n+### Logical disk used - individual disks\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where Namespace == \"LogicalDisk\" and Name == \"FreeSpacePercentage\"\n+| extend Disk=tostring(todynamic(Tags)[\"vm.azm.ms/mountId\"])\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), Computer, _ResourceId, Disk\n+```\n+\n+### Logical disk IOPS\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\" \n+| where Namespace == \"LogicalDisk\" and Name == \"TransfersPerSecond\"\n+| extend Disk=tostring(todynamic(Tags)[\"vm.azm.ms/mountId\"])\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m) ), Computer, _ResourceId, Disk\n+```\n+\n+### Logical disk data rate\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\" \n+| where Namespace == \"LogicalDisk\" and Name == \"BytesPerSecond\"\n+| extend Disk=tostring(todynamic(Tags)[\"vm.azm.ms/mountId\"])\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m) , Computer, _ResourceId, Disk\n+```\n+\n+### Network interfaces bytes received - all interfaces\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where Namespace == \"Network\" and Name == \"ReadBytesPerSecond\"\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), Computer, _ResourceId \n+```\n+\n+### Network interfaces bytes received - individual interfaces\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where Namespace == \"Network\" and Name == \"ReadBytesPerSecond\"\n+| extend NetworkInterface=tostring(todynamic(Tags)[\"vm.azm.ms/networkDeviceId\"])\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), Computer, _ResourceId, NetworkInterface\n+```\n+\n+### Network interfaces bytes sent - all interfaces\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where Namespace == \"Network\" and Name == \"WriteBytesPerSecond\"\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), Computer, _ResourceId\n+```\n+\n+### Network interfaces bytes sent - individual interfaces\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where Namespace == \"Network\" and Name == \"WriteBytesPerSecond\"\n+| extend NetworkInterface=tostring(todynamic(Tags)[\"vm.azm.ms/networkDeviceId\"])\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), Computer, _ResourceId, NetworkInterface\n+```\n+\n+## Virtual machine scale set\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where _ResourceId startswith \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/my-resource-group/providers/Microsoft.Compute/virtualMachineScaleSets/my-vm-scaleset\" \n+| where Namespace == \"Processor\" and Name == \"UtilizationPercentage\"\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), _ResourceId\n+```\n+\n+## Specific virtual machine\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where _ResourceId =~ \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/my-resource-group/providers/Microsoft.Compute/virtualMachines/my-vm\" \n+| where Namespace == \"Processor\" and Name == \"UtilizationPercentage\"\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m)\n+```\n+\n+## CPU utilization for all compute resources in a subscription\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where _ResourceId startswith \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" and (_ResourceId contains \"/providers/Microsoft.Compute/virtualMachines/\" or _ResourceId contains \"/providers/Microsoft.Compute/virtualMachineScaleSets/\")\n+| where Namespace == \"Processor\" and Name == \"UtilizationPercentage\"\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), _ResourceId\n+```\n+\n+## CPU utilization for all compute resources in a resource group\n+\n+```kusto\n+InsightsMetrics\n+| where Origin == \"vm.azm.ms\"\n+| where _ResourceId startswith \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/my-resource-group/providers/Microsoft.Compute/virtualMachines/\"\n+or _ResourceId startswith \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/my-resource-group/providers/Microsoft.Compute/virtualMachineScaleSets/\" \n+| where Namespace == \"Processor\" and Name == \"UtilizationPercentage\"\n+| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 15m), _ResourceId\n+\n+```\n+\n+\n+\n+## Next steps\n+\n+- Learn more about [alerts in Azure Monitor](../platform/alerts-overview.md).\n+- Learn more about [log queries using data from Azure Monitor for VMs](vminsights-log-search.md)."
  },
  {
    "Number": 108563,
    "Title": "Azure Monitor VM Insights GA 2",
    "ClosedAt": "2020-03-23T22:25:38Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/vminsights-ga-release-faq.md",
    "Addition": 2,
    "Delections": 10,
    "Changes": 12,
    "Patch": "@@ -10,22 +10,18 @@ ms.date: 01/31/2020\n ---\n \n # Azure Monitor for VMs Generally Available (GA) Frequently Asked Questions\n-\n-This General Availability FAQ covers changes that are happening in Azure Monitor for VMs as we prepare for our GA release. \n+This General Availability FAQ covers changes that were made in Q4 2019 and Q1 2020 as we prepared for GA.\n \n ## Updates for Azure Monitor for VMs\n+We released a new version of Azure Monitor for VMs in January 2020 ahead of our GA announcement. Customers enabling Azure Monitor for VMs will now receive the GA version, but existing customers using the version of Azure Monitor for VMs from Q4 2019 and earlier will be prompted to upgrade. This FAQ offers guidance to perform an upgrade at scale if you have large deployments across multiple workspaces.\n \n-We have released a new version of Azure Monitor for VMs. Customers enabling Azure Monitors for VMs will now receive the new version, but existing customers already using Azure Monitor for VMs will be prompted to upgrade. This FAQ and our documentation offers guidance to perform an upgrade at scale if you have large deployments across multiple workspaces.\n \n With this upgrade, Azure Monitor for VMs performance data are stored in the same *InsightsMetrics* table as [Azure Monitor for containers](container-insights-overview.md), which makes it easier for you to query the two data sets. Also, you are able to store more diverse data sets that we could not store in the table previously used. \n \n Our performance views are now using the data we store in the *InsightsMetrics* table.  If you have not yet upgraded to use the latest VMInsights solution on your workspace, your charts will no longer display information.  You can upgrade from our **Get Started** page as described below.\n \n-We realize that asking existing customers to upgrade causes disruption to their workflow, which is why we have chosen to do this now while in Public Preview rather than later after GA.\n-\n \n ## What is changing?\n-\n We have released a new solution, named VMInsights, that includes additional capabilities for data collection along with a new location for storing this data in your Log Analytics workspace. \n \n In the past, we enabled the ServiceMap solution on your workspace and setup performance counters in your Log Analytics workspace to send the data to the *Perf* table. This new solution sends the data to a table named *InsightsMetrics* that is also used by Azure Monitor for containers. This table schema allows us to store additional metrics and service data sets that are not compatible with the *Perf* table format.\n@@ -88,10 +84,6 @@ No, the two solutions share the map data sets that we store in `VMComputer` (for\n \n No, the two solutions share the map data sets that we store in `VMComputer` (formerly ServiceMapComputer_CL), `VMProcess` (formerly ServiceMapProcess_CL), `VMConnection`, and `VMBoundPort`. If you remove one of the solutions, these data sets notice that there is still a solution in place that uses the data and it remains in the Log Analytics workspace. You need to remove both solutions from your workspace in order for the data to be removed from it.\n \n-## When will this update be released?\n-\n-We expect to release the update for Azure Monitor for VMs in early January 2020. As we get closer to the release date in January, we'll post updates here and present notifications in the Azure portal when you open Azure Monitor.\n-\n ## Health feature is in limited public preview\n \n We have received a lot of great feedback from customers about our VM Health feature set. There is a lot of interest around this feature and excitement over its potential for supporting monitoring workflows. We are planning to make a series of changes to add functionality and address the feedback we have received. "
  },
  {
    "Number": 108563,
    "Title": "Azure Monitor VM Insights GA 2",
    "ClosedAt": "2020-03-23T22:25:38Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/vminsights-log-search.md",
    "Addition": 44,
    "Delections": 2,
    "Changes": 46,
    "Patch": "@@ -210,7 +210,7 @@ Records with a type of *VMComputer* have inventory data for servers with the Dep\n |AzureServiceFabricClusterId | The unique identifer of the Azure Service Fabric cluster | \n |AzureServiceFabricClusterName | The name of the Azure Service Fabric cluster |\n \n-### VMProcess record\n+### VMProcess records\n \n Records with a type of *VMProcess* have inventory data for TCP-connected processes on servers with the Dependency agent. These records have the properties in the following table:\n \n@@ -243,7 +243,8 @@ Records with a type of *VMProcess* have inventory data for TCP-connected process\n |UserDomain | The domain under which the process is executing |\n |_ResourceId | The unique identifier for a process within the workspace |\n \n-## Sample log searches\n+\n+## Sample map queries\n \n ### List all known machines\n \n@@ -425,6 +426,47 @@ let remoteMachines = remote | summarize by RemoteMachine;\n | summarize Remote=makeset(iff(isempty(RemoteMachine), todynamic('{}'), pack('Machine', RemoteMachine, 'Process', Process1, 'ProcessName', ProcessName1))) by ConnectionId, Direction, Machine, Process, ProcessName, SourceIp, DestinationIp, DestinationPort, Protocol\n ```\n \n+## Performance records\n+Records with a type of *InsightsMetrics* have performance data from the guest operating system of the virtual machine. These records have the properties in the following table:\n+\n+\n+| Property | Description |\n+|:--|:--|\n+|TenantId | Unique identifier for the workspace |\n+|SourceSystem | *Insights* | \n+|TimeGenerated | Time the value was collected (UTC) |\n+|Computer | The computer FQDN | \n+|Origin | *vm.azm.ms* |\n+|Namespace | Category of the performance counter | \n+|Name | Name of the performance counter |\n+|Val | Collected value | \n+|Tags | Related details about the record. See the table below for tags used with different record types.  |\n+|AgentId | Unique identifier for each computer's agent |\n+|Type | *InsightsMetrics* |\n+|_ResourceId_ | Resource ID of the virtual machine |\n+\n+The performance counters currently collected into the *InsightsMetrics* table are listed in the following table:\n+\n+| Namespace | Name | Description | Unit | Tags |\n+|:---|:---|:---|:---|:---|\n+| Computer    | Heartbeat             | Computer Heartbeat                        | | |\n+| Memory      | AvailableMB           | Memory Available Bytes                    | Bytes          | memorySizeMB - Total memory size|\n+| Network     | WriteBytesPerSecond   | Network Write Bytes Per Second            | BytesPerSecond | NetworkDeviceId - Id of the device<br>bytes - Total sent bytes |\n+| Network     | ReadBytesPerSecond    | Network Read Bytes Per Second             | BytesPerSecond | networkDeviceId - Id of the device<br>bytes - Total received bytes |\n+| Processor   | UtilizationPercentage | Processor Utilization Percentage          | Percent        | totalCpus - Total CPUs |\n+| LogicalDisk | WritesPerSecond       | Logical Disk Writes Per Second            | CountPerSecond | mountId - Mount ID of the device |\n+| LogicalDisk | WriteLatencyMs        | Logical Disk Write Latency Millisecond    | MilliSeconds   | mountId - Mount ID of the device |\n+| LogicalDisk | WriteBytesPerSecond   | Logical Disk Write Bytes Per Second       | BytesPerSecond | mountId - Mount ID of the device |\n+| LogicalDisk | TransfersPerSecond    | Logical Disk Transfers Per Second         | CountPerSecond | mountId - Mount ID of the device |\n+| LogicalDisk | TransferLatencyMs     | Logical Disk Transfer Latency Millisecond | MilliSeconds   | mountId - Mount ID of the device |\n+| LogicalDisk | ReadsPerSecond        | Logical Disk Reads Per Second             | CountPerSecond | mountId - Mount ID of the device |\n+| LogicalDisk | ReadLatencyMs         | Logical Disk Read Latency Millisecond     | MilliSeconds   | mountId - Mount ID of the device |\n+| LogicalDisk | ReadBytesPerSecond    | Logical Disk Read Bytes Per Second        | BytesPerSecond | mountId - Mount ID of the device |\n+| LogicalDisk | FreeSpacePercentage   | Logical Disk Free Space Percentage        | Percent        | mountId - Mount ID of the device |\n+| LogicalDisk | FreeSpaceMB           | Logical Disk Free Space Bytes             | Bytes          | mountId - Mount ID of the device<br>diskSizeMB - Total disk size |\n+| LogicalDisk | BytesPerSecond        | Logical Disk Bytes Per Second             | BytesPerSecond | mountId - Mount ID of the device |\n+\n+\n ## Next steps\n \n * If you are new to writing log queries in Azure Monitor, review [how to use Log Analytics](../../azure-monitor/log-query/get-started-portal.md) in the Azure portal to write log queries."
  },
  {
    "Number": 108563,
    "Title": "Azure Monitor VM Insights GA 2",
    "ClosedAt": "2020-03-23T22:25:38Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/vminsights-maps.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -5,7 +5,7 @@ ms.subservice:\n ms.topic: conceptual\n author: bwren\n ms.author: bwren\n-ms.date: 10/15/2019\n+ms.date: 03/20/2020\n \n ---\n "
  },
  {
    "Number": 108563,
    "Title": "Azure Monitor VM Insights GA 2",
    "ClosedAt": "2020-03-23T22:25:38Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/vminsights-performance.md",
    "Addition": 0,
    "Delections": 4,
    "Changes": 4,
    "Patch": "@@ -114,11 +114,7 @@ Clicking on the pin icon at the upper right-hand corner of any one of the charts\n >[!NOTE]\n >You can also access a detailed performance view for a specific instance from the Instances view for your scale set. Navigate to **Instances** under the **Settings** section, and then choose **Insights**.\n \n-## Alerts  \n \n-Performance metrics enabled as part of Azure Monitor for VMs do not include pre-configured alert rules. However, we may only collect and store a subset of the performance metrics you require in the Log Analytics workspace. If your monitoring strategy requires analysis or alerting that includes other performance metrics in order to effectively evaluate capacity or health of the virtual machine, or you need the flexibility to specify your own alerting criteria or logic, you can configure [collection of those performance counters](../platform/data-sources-performance-counters.md) in Log Analytics and define [log alerts](../platform/alerts-log.md). While Log Analytics allows you to perform complex analysis with other data types, and provide longer retention to support trend analysis, metrics on the other hand, are lightweight and capable of supporting near real-time scenarios. They are collected by the [Azure Diagnostic agent](../../virtual-machines/windows/monitor.md) and stored in the Azure Monitor metrics store, allowing you to create alerts with lower latency and at a lower cost.\n-\n-Review the overview of [collection of metrics and logs with Azure Monitor](../platform/data-platform.md) to further understand the fundamental differences and other considerations before configuring collection of these additional metrics and alert rules.  \n \n ## Next steps\n "
  },
  {
    "Number": 108142,
    "Title": "Change SSL to TLS per 1679050",
    "ClosedAt": "2020-03-23T20:16:49Z",
    "User": "TimShererWithAquent",
    "FileName": "articles/azure-monitor/app/kubernetes.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -55,7 +55,7 @@ kubectl label namespace <my-app-namespace> istio-injection=enabled\n ```\n \n > [!NOTE]\n-> Since service mesh lifts data off the wire, we cannot intercept the encrypted traffic. For traffic that doesn't leave the cluster, use  an unencrypted protocol (for example, HTTP). For external traffic that must be encrypted, consider [setting up SSL termination](https://kubernetes.io/docs/concepts/services-networking/ingress/#tls) at the ingress controller.\n+> Since service mesh lifts data off the wire, we cannot intercept the encrypted traffic. For traffic that doesn't leave the cluster, use  an unencrypted protocol (for example, HTTP). For external traffic that must be encrypted, consider [setting up TLS termination](https://kubernetes.io/docs/concepts/services-networking/ingress/#tls) at the ingress controller.\n \n Applications running outside of the service mesh are not affected.\n "
  },
  {
    "Number": 108569,
    "Title": "Small updates",
    "ClosedAt": "2020-03-22T08:51:19Z",
    "User": "nolavime",
    "FileName": "articles/azure-monitor/platform/alerts-activity-log.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -192,8 +192,8 @@ To create an activity log alert rule by using an Azure Resource Manager template\n ```\n The previous sample JSON can be saved as, for example, sampleActivityLogAlert.json for the purpose of this walk-through and can be deployed by using [Azure Resource Manager in the Azure portal](../../azure-resource-manager/templates/deploy-portal.md).\n \n-The following fields are the fields that you can use in the Azure Resource Manager template for the conditions fields:\n-Notice that “Resource Health”, “Advisor” and “Service Health” have extra properties fields for their special fields. In the beginning.\n+The following fields are the options that you can use in the Azure Resource Manager template for the conditions fields:\n+Notice that “Resource Health”, “Advisor” and “Service Health” have extra properties fields for their special fields. \n 1. resourceId:\tThe resource ID of the impacted resource in the activity log event that the alert should be generated on.\n 2. category: The category of in the activity log event. For example: Administrative, ServiceHealth, ResourceHealth, Autoscale, Security, Recommendation, Policy.\n 3. caller: The email address or Azure Active Directory identifier of the user who performed the operation of the activity log event."
  },
  {
    "Number": 108568,
    "Title": "added info to the ITSM installation",
    "ClosedAt": "2020-03-22T07:58:21Z",
    "User": "nolavime",
    "FileName": "articles/azure-monitor/platform/itsmc-connections.md",
    "Addition": 3,
    "Delections": 1,
    "Changes": 4,
    "Patch": "@@ -176,6 +176,8 @@ The following sections provide details about how to connect your ServiceNow prod\n Ensure the following prerequisites are met:\n - ITSMC installed. More information: [Adding the IT Service Management Connector Solution](../../azure-monitor/platform/itsmc-overview.md#adding-the-it-service-management-connector-solution).\n - ServiceNow supported versions: New York, Madrid, London, Kingston, Jakarta, Istanbul, Helsinki, Geneva.\n+> [!NOTE]\n+> ITSMC supports only the official SaaS offering from Service Now. Private deployments of Service Now are not supported. \n \n **ServiceNow Admins must do the following in their ServiceNow instance**:\n - Generate client ID and client secret for the ServiceNow product. For information on how to generate client ID and secret, see the following information as required:\n@@ -216,7 +218,7 @@ Use the following procedure to create a ServiceNow connection:\n | **Partner type**   | Select **ServiceNow**. |\n | **Username**   | Type the integration user name that you created in the ServiceNow app to support the connection to ITSMC. More information: [Create ServiceNow app user role](#create-integration-user-role-in-servicenow-app).|\n | **Password**   | Type the password associated with this user name. **Note**: User name and password are used for generating authentication tokens only, and are not stored anywhere within the ITSMC service.  |\n-| **Server URL**   | Type the URL of the ServiceNow instance that you want to connect to ITSMC. |\n+| **Server URL**   | Type the URL of the ServiceNow instance that you want to connect to ITSMC. The URL should point to a supported SaaS version with suffix \".servicenow.com\".|\n | **Client ID**   | Type the client ID that you want to use for OAuth2 Authentication, which you generated earlier.  More information on generating client ID and secret:   [OAuth Setup](https://wiki.servicenow.com/index.php?title=OAuth_Setup). |\n | **Client Secret**   | Type the client secret, generated for this ID.   |\n | **Data Sync Scope**   | Select the ServiceNow work items that you want to sync to Azure Log Analytics, through the ITSMC.  The selected values are imported into log analytics.   **Options:**  Incidents and Change Requests.|"
  },
  {
    "Number": 108466,
    "Title": "shorten headings. Add log alerts link",
    "ClosedAt": "2020-03-21T01:06:08Z",
    "User": "rboucher",
    "FileName": "articles/azure-monitor/platform/alerts-troubleshoot.md",
    "Addition": 4,
    "Delections": 4,
    "Changes": 8,
    "Patch": "@@ -17,7 +17,7 @@ Azure Monitor alerts proactively notify you when important conditions are found\n \n If you can see a fired alert in the Azure portal, but have an issue with some of its actions or notifications, see the following sections.\n \n-## Expected an email, but did not receive it\n+## Did not receive expected email\n \n If you can see a fired alert in the Azure portal, but did not receive the email that you have configured about it, follow these steps: \n \n@@ -73,7 +73,7 @@ If you can see a fired alert in the Azure portal, but did not receive the email\n \n    If you would like to receive high-volume of notifications without rate limiting, consider using a different action, such as webhook, logic app, Azure function, or automation runbooks, none of which are rate limited. \n \n-## Expected an SMS, a voice call, or a push notification, but did not receive it\n+## Did not receive expected SMS, voice call, or push notification\n \n If you can see a fired alert in the portal, but did not receive the SMS, voice call or push notification that you have configured about it, follow these steps: \n \n@@ -236,5 +236,5 @@ If you received an error while trying to create, update or delete an [action rul\n \n \n ## Next steps\n-\n-Go back to the [Azure portal](https://portal.azure.com) to check if you've solved your issue with any of the advice above\n+- If using a log alert, also see [Troubleshooting Log Alerts](alert-log-troubleshoot.md).\n+- Go back to the [Azure portal](https://portal.azure.com) to check if you've solved your issue with guidance above "
  },
  {
    "Number": 108509,
    "Title": "Azure Monitor delete workspace title fix",
    "ClosedAt": "2020-03-20T20:48:08Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/platform/delete-workspace.md",
    "Addition": 6,
    "Delections": 6,
    "Changes": 12,
    "Patch": "@@ -9,7 +9,7 @@ ms.date: 01/14/2020\n \n ---\n \n-# Delete and restore Azure Log Analytics workspace\n+# Delete and recover Azure Log Analytics workspace\n \n This article explains the concept of Azure Log Analytics workspace soft-delete and how to recover deleted workspace. \n \n@@ -29,14 +29,14 @@ You want to exercise caution when you delete a workspace because there might be\n * Agents running on Windows and Linux computers in your environment\n * System Center Operations Manager\n \n-The soft-delete operation deletes the workspace resource and any associated users’ permission is broken. If users are associated with other workspaces, then they can continue using Log Analytics with those other workspaces.\n+The soft-delete operation deletes the workspace resource and any associated users' permission is broken. If users are associated with other workspaces, then they can continue using Log Analytics with those other workspaces.\n \n ## Soft-delete behavior\n \n The workspace delete operation removes the workspace Resource Manager resource, but its configuration and data are kept for 14 days, while giving the appearance that the workspace is deleted. Any agents and System Center Operations Manager management groups configured to report to the workspace remain in an orphaned state during the soft-delete period. The service further provides a mechanism for recovering the deleted workspace including its data and connected resources, essentially undoing the deletion.\n \n > [!NOTE] \n-> Installed solutions and linked services like your Azure Automation account are permanently removed from the workspace at deletion time and can’t be recovered. These should be reconfigured after the recovery operation to bring the workspace to its previously configured state.\n+> Installed solutions and linked services like your Azure Automation account are permanently removed from the workspace at deletion time and can't be recovered. These should be reconfigured after the recovery operation to bring the workspace to its previously configured state.\n \n You can delete a workspace using [PowerShell](https://docs.microsoft.com/powershell/module/azurerm.operationalinsights/remove-azurermoperationalinsightsworkspace?view=azurermps-6.13.0), [REST API](https://docs.microsoft.com/rest/api/loganalytics/workspaces/delete), or in the [Azure portal](https://portal.azure.com).\n \n@@ -55,11 +55,11 @@ PS C:\\>Remove-AzOperationalInsightsWorkspace -ResourceGroupName \"resource-group-\n ```\n \n ## Permanent workspace delete\n-The soft-delete method may not fit in some scenarios such as development and testing, where you need to repeat a deployment with the same settings and workspace name. In such cases you can permanently delete your workspace and “override” the soft-delete period. The permanent workspace delete operation releases the workspace name and you can create a new workspace using the same name.\n+The soft-delete method may not fit in some scenarios such as development and testing, where you need to repeat a deployment with the same settings and workspace name. In such cases you can permanently delete your workspace and \"override\" the soft-delete period. The permanent workspace delete operation releases the workspace name and you can create a new workspace using the same name.\n \n \n > [!IMPORTANT]\n-> Use permanent workspace delete operation with caution since its irreversible and you won’t be able to recover your workspace and its data.\n+> Use permanent workspace delete operation with caution since its irreversible and you won't be able to recover your workspace and its data.\n \n The permanent workspace delete can currently be performed via REST API.\n \n@@ -76,7 +76,7 @@ To permanently delete your workspace, use the [Workspaces - Delete REST]( https:\n > DELETE https://management.azure.com/subscriptions/<subscription-id>/resourcegroups/<resource-group-name>/providers/Microsoft.OperationalInsights/workspaces/<workspace-name>?api-version=2015-11-01-preview&force=true\n > Authorization: Bearer eyJ0eXAiOiJKV1Qi….\n > ```\n-Where ‘eyJ0eXAiOiJKV1Qi…’ represents the full authorization token.\n+Where 'eyJ0eXAiOiJKV1Qi…' represents the full authorization token.\n \n ## Recover workspace\n "
  },
  {
    "Number": 108431,
    "Title": "Added clarifications",
    "ClosedAt": "2020-03-20T09:18:31Z",
    "User": "harelbr",
    "FileName": "articles/azure-monitor/platform/alerts-metric-logs.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -37,7 +37,7 @@ There are many benefits for using **Metric Alerts for Logs** over query based [L\n  Metric alerts support alerting for metrics that use dimensions. You can use dimensions to filter your metric to the right level. The full list of metrics supported for Logs from [Log Analytics workspaces](../../azure-monitor/platform/metrics-supported.md#microsoftoperationalinsightsworkspaces) is listed; across supported solutions.\n \n > [!NOTE]\n-> To view supported metrics for being extracted from Log Analytics workspace via [Azure Monitor - Metrics](../../azure-monitor/platform/metrics-charts.md); a metric alert for log must be created for the said metric. The dimensions chosen in Metric Alert for logs - will only appear for exploration via Azure Monitor - Metrics.\n+> To view a supported metric extracted from a Log Analytics workspace via [Azure Monitor - Metrics](../../azure-monitor/platform/metrics-charts.md), a metric alert for log must be created on that specific metric. The dimensions chosen in the metric alert for logs - will only appear for exploration via Azure Monitor - Metrics.\n \n ## Creating metric alert for Log Analytics\n "
  },
  {
    "Number": 106208,
    "Title": "create alerts troubleshooting page",
    "ClosedAt": "2020-03-19T23:26:55Z",
    "User": "rboucher",
    "FileName": "articles/azure-monitor/platform/alerts-troubleshoot.md",
    "Addition": 240,
    "Delections": 0,
    "Changes": 240,
    "Patch": "@@ -0,0 +1,240 @@\n+---\n+title: Troubleshooting Azure Monitor alerts and notifications\n+description: Common issues with Azure Monitor alerts and possible solutions. \n+author: ofirmanor\n+ms.author: ofmanor\n+ms.topic: reference\n+ms.date: 03/16/2020\n+ms.subservice: alerts\n+---\n+# Troubleshooting problems in Azure Monitor alerts \n+\n+This article discusses common problems in Azure Monitor alerting.\n+\n+Azure Monitor alerts proactively notify you when important conditions are found in your monitoring data. They allow you to identify and address issues before the users of your system notice them. For more information on alerting, see [Overview of alerts in Microsoft Azure](alerts-overview.md).\n+\n+## Action or notification on my alert did not work as expected\n+\n+If you can see a fired alert in the Azure portal, but have an issue with some of its actions or notifications, see the following sections.\n+\n+## Expected an email, but did not receive it\n+\n+If you can see a fired alert in the Azure portal, but did not receive the email that you have configured about it, follow these steps: \n+\n+1. **Was the email suppressed by an [action rule](alerts-action-rules.md)**? \n+\n+    Check by clicking on the fired alert in the portal, and look at the history tab for suppressed [action groups](action-groups.md): \n+\n+    ![Alert action rule suppression history](media/alerts-troubleshoot/history-action-rule.png)\n+\n+1. **Is the type of action \"Email Azure Resource Manager Role\"?**\n+\n+    This action only looks at Azure Resource Manager role assignments that are at the subscription scope, and of type *User*.  Make sure that you have assigned the role at the subscription level, and not at the resource level or resource group level.\n+\n+1. **Are your email server and mailbox accepting external emails?**\n+\n+    Verify that emails from these three addresses are not blocked:\n+      - azure-noreply@microsoft.com  \n+      - azureemail-noreply@microsoft.com\n+      - alerts-noreply@mail.windowsazure.com\n+\n+    It is common that internal mailing lists or distribution lists block emails from external email addresses. You need to whitelist the above email addresses.  \n+    To test, add a regular work email address (not a mailing list) to the action group and see if alerts arrive to that email. \n+\n+1. **Was the email processed by inbox rules or a spam filter?** \n+\n+    Verify that there are no inbox rules that delete those emails or move them to a side folder. For example, inbox rules could catch specific senders or specific words in the subject.\n+\n+    Also, check:\n+    \n+      - the spam settings of your email client (like Outlook, Gmail)\n+      - the sender limits / spam settings / quarantine settings of your email server (like Exchange, Office 365, G-suite)\n+      - the settings of your email security appliance, if any (like Barracuda, Cisco). \n+\n+1. **Have you accidentally unsubscribed from the action group?** \n+\n+    The alert emails provide a link to unsubscribe from the action group. To check if you have accidentally unsubscribed from this action group, either:\n+\n+    1. Open the action group in the portal and check the Status column:\n+\n+    ![action group status column](media/alerts-troubleshoot/action-group-status.png)\n+\n+    2. Search your email for the unsubscribe confirmation:\n+\n+    ![unsubscribed from alert action group](media/alerts-troubleshoot/unsubscribe-action-group.png)\n+\n+    To subscribe again – either use the link in the unsubscribe confirmation email you have received, or remove the email address from the action group, and then add it back again. \n+ \n+1. **Have you been rated limited due to many emails going to a single email address?** \n+\n+    Email is [rate limited](alerts-rate-limiting.md) to no more than 100 emails every hour to each email address. If you pass this threshold, additional email notifications are dropped.  Check if you have received a message indicating that your email address has been temporarily rate limited: \n+ \n+   ![Email rate limited](media/alerts-troubleshoot/email-paused.png)\n+\n+   If you would like to receive high-volume of notifications without rate limiting, consider using a different action, such as webhook, logic app, Azure function, or automation runbooks, none of which are rate limited. \n+\n+## Expected an SMS, a voice call, or a push notification, but did not receive it\n+\n+If you can see a fired alert in the portal, but did not receive the SMS, voice call or push notification that you have configured about it, follow these steps: \n+\n+1. **Was the action suppressed by an [action rule](alerts-action-rules.md)?** \n+\n+    Check by clicking on the fired alert in the portal, and look at the history tab for suppressed [action groups](action-groups.md): \n+\n+    ![Alert action rule suppression history](media/alerts-troubleshoot/history-action-rule.png)\n+\n+   If that was unintentional, you can modify, disable, or delete the action rule.\n+ \n+1. **SMS / voice:  Is your phone number correct?**\n+\n+   Check the SMS action for typos in the country code or phone number. \n+ \n+1. **SMS / voice: have you been rate limited?** \n+\n+    SMS and voice calls are rate limited to no more than one notification every five minutes per phone number. If you pass this threshold, the notifications will be dropped. \n+\n+      - Voice call – check your call history and see if you had a different call from Azure in the preceding five minutes. \n+      - SMS - check your SMS history for a message indicating that your phone number has been rate limited. \n+\n+    If you would like to receive high-volume of notifications without rate limiting, consider using a different action, such as webhook, logic app, Azure function, or automation runbooks, none of which are rate limited. \n+ \n+1. **SMS: Have you accidentally unsubscribed from the action group?**\n+\n+    Open your SMS history and check if you have opted out of SMS delivery from this specific action group (using the DISABLE action_group_short_name reply) or from all action groups (using the  STOP reply). To subscribe again, either send the relevant SMS command (ENABLE action_group_short_name or START), or remove the SMS action from the action group, and then add it back again.  For more information, see [SMS alert behavior in action groups](alerts-sms-behavior.md).\n+\n+1. **Have you accidentally blocked the notifications on your phone?**\n+\n+   Most mobile phones allow you to block calls or SMS from specific phone numbers or short codes, or to block push notifications from specific apps (such as the Azure mobile app). To check if you accidentally blocked the notifications on your phone, search the documentation specific for your phone operating system and model, or test with a different phone and phone number. \n+\n+## Expected another type of action to trigger, but it did not \n+\n+If you can see a fired alert in the portal, but its configured action did not trigger, follow these steps: \n+\n+1. **Was the action suppressed by an action rule?** \n+\n+    Check by clicking on the fired alert in the portal, and look at the history tab for suppressed [action groups](action-groups.md): \n+\n+    ![Alert action rule suppression history](media/alerts-troubleshoot/history-action-rule.png)\n+ \n+    If that was unintentional, you can modify, disable, or delete the action rule. \n+\n+1. **Did a webhook not trigger?**\n+\n+    1. **Have the source IP addresses been blocked?**\n+    \n+       Whitelist the [IP addresses](action-groups.md#action-specific-information) that the webhook is called from.\n+\n+    1. **Does your webhook endpoint work correctly?**\n+\n+       Verify the webhook endpoint you have configured is correct and the endpoint is working correctly. Check your webhook logs or instrument its code so you could investigate (for example, log the incoming payload). \n+\n+    1. **Are you calling Slack or Microsoft Teams?**  \n+    Each of these endpoints expects a specific JSON format. Follow [these instructions](action-groups-logic-app.md) to configure a logic app action instead.\n+\n+    1. **Did your webhook became unresponsive or returned errors?** \n+\n+        Our timeout period for a webhook response is 10 seconds. The webhook call will be retried up to two additional times when the following HTTP status codes are returned: 408, 429, 503, 504, or when the HTTP endpoint does not respond. The first retry happens after 10 seconds. The second and final retry happens after 100 seconds. If the second retry fails, the endpoint will not be called again for 30 minutes for any action group.\n+\n+## Action or notification happened more than once \n+\n+If you have received a notification for an alert (such as an email or an SMS) more than once, or the alert's action (such as webhook or Azure function) was triggered multiple times, follow these steps: \n+\n+1. **Is it really the same alert?** \n+\n+    In some cases, multiple similar alerts are fired at around the same time. So, it might just seem like the same alert triggered its actions multiple times. For example, an activity log alert rule might be configured to fire both when an event has started, and when it has finished (succeeded or failed), by not filtering on the event status field. \n+\n+    To check if these actions or notifications came from different alerts, examine the alert details, such as its timestamp and either the alert id or its correlation id. Alternatively, check the list of fired alerts in the portal. If that is the case, you would need to adapt the alert rule logic or otherwise configure the alert source. \n+\n+1. **Does the action repeat in multiple action groups?** \n+\n+    When an alert is fired, each of its action groups is processed independently. So, if an action (such as an email address) appears in multiple triggered action groups, it would be called once per action group. \n+\n+    To check which action groups were triggered, check the alert history tab. You would see there both action groups defined in the alert rule, and action groups added to the alert by action rules: \n+\n+    ![Action repeated in multiple action groups](media/alerts-troubleshoot/action-repeated-multi-action-groups.png)\n+\n+## Action or notification has an unexpected content\n+\n+If you have received the alert, but believe some of its fields are missing or incorrect, follow these steps: \n+\n+1. **Did you pick the correct format for the action?** \n+\n+    Each action type (email, webhook, etc.) has two formats – the default, legacy format, and the [newer common schema format](alerts-common-schema.md). When you create an action group, you specify the format you want per action – different actions in the action groups may have different formats. \n+\n+    For example, for webhook action: \n+\n+    ![webhook action schema option](media/alerts-troubleshoot/webhook.png)\n+\n+    Check if the format specified at the action level is what you expect. For example, you may have developed code that responds to alerts (webhook, function, logic app, etc.), expecting one format, but later in the action you or another person specified a different format.  \n+\n+    Also, check the payload format (JSON) for [activity log alerts](activity-log-alerts-webhook.md), for [log search alerts](alerts-log-webhook.md) (both Application Insights and log analytics), for [metric alerts](alerts-metric-near-real-time.md#payload-schema), for the [common alert schema](alerts-common-schema-definitions.md), and for the deprecated [classic metric alerts](alerts-webhooks.md).\n+\n+ \n+1. **Activity log alerts: Is the information available in the activity log?** \n+\n+    [Activity log alerts](activity-log-alerts.md) are alerts that are based on events written to the Azure Activity Log, such as events about creating, updating or deleting Azure resources, service health and resource health events, or findings from Azure Advisor and Azure Policy. If you have received an alert based on the activity log but some fields that you need are missing or incorrect, first check the events in the activity log itself. If the Azure resource did not write the fields you are looking for in its activity log event, those fields will not be included in the corresponding alert. \n+\n+## Action rule is not working as expected \n+\n+If you can see a fired alert in the portal, but a related action rule did not work as expected, follow these steps: \n+\n+1. **Is the action rule enabled?** \n+\n+    Check the action rule status column to verify that the related action role is enabled. \n+\n+    ![graphic](media/alerts-troubleshoot/action-rule-status.png) \n+\n+    If it is not enabled, you can enable the action rule by selecting it and clicking Enable. \n+\n+1. **Is it a service health alert?** \n+\n+    Service health alerts (monitor service = \"Service Health\") are not affected by action rules. \n+\n+1. **Did the action rule act on your alert?** \n+\n+    Check if the action rule has processed your alert by clicking on the fired alert in the portal, and look at the history tab.\n+\n+    Here is an example of action rule suppressing all action groups: \n+ \n+     ![Alert action rule suppression history](media/alerts-troubleshoot/history-action-rule.png)\n+\n+    Here is an example of an action rule adding another action group:\n+\n+    ![Action repeated in multiple action groups](media/alerts-troubleshoot/action-repeated-multi-action-groups.png)\n+ \n+\n+1. **Does the action rule scope and filter match the fired alert?** \n+\n+    If you think the action rule should have fired but didn't, or that it shouldn't have fired but it did, carefully examine the action rule scope and filter conditions versus the properties of the fired alert. \n+\n+\n+## How to find the alert id of a fired alert\n+\n+When opening a case about a specific fired alert (such as – if you did not receive its email notification), you will need to provide the alert id. \n+\n+To locate it, follow these steps:\n+\n+1. In the Azure portal, navigate to the list of fired alerts, and find that specific alert. You can use the filters to help you locate it. \n+\n+1. Click on the alert to open the alert details. \n+\n+1. Scroll down in the alert fields of the first tab (the summary tab) until you locate it, and copy it. That field also includes a \"Copy to clipboard\" helper button you can use.  \n+\n+    ![find alert id](media/alerts-troubleshoot/get-alert-id.png)\n+\n+## Problem creating, updating, or deleting action rules in the Azure portal\n+\n+If you received an error while trying to create, update or delete an [action rule](alerts-action-rules.md), follow these steps: \n+\n+1. **Did you receive a permission error?**  \n+\n+    You should either have the [Monitoring Contributor built-in role](https://docs.microsoft.com/azure/role-based-access-control/built-in-roles#monitoring-contributor), or the specific permissions related to action rules and alerts.\n+\n+1. **Did you verify the action rule parameters?**  \n+\n+    Check the [action rule documentation](alerts-action-rules.md), or the [action rule PowerShell Set-AzActionRule](https://docs.microsoft.com/powershell/module/az.alertsmanagement/Set-AzActionRule?view=azps-3.5.0) command. \n+\n+\n+## Next steps\n+\n+Go back to the [Azure portal](https://portal.azure.com) to check if you've solved your issue with any of the advice above"
  },
  {
    "Number": 108330,
    "Title": "3/19 AM Publish",
    "ClosedAt": "2020-03-19T17:03:35Z",
    "User": "huypub",
    "FileName": "articles/active-directory/app-provisioning/export-import-provisioning-configuration.md",
    "Addition": 12,
    "Delections": 4,
    "Changes": 16,
    "Patch": "@@ -18,10 +18,18 @@ ms.author: chmutali\n \n ms.collection: M365-identity-device-management\n ---\n-# Export or import your provisioning configuration by using the Microsoft Graph API\n-\n-You can use the Microsoft Graph API and the Microsoft Graph Explorer to export your User Provisioning attribute mappings and schema to a JSON file and import it back into Azure AD. You can also use the steps captured here to create a backup of your provisioning configuration. \n-\n+# Export your provisioning configuration and roll back to a known good state\n+\n+## Export and import your provisioning configuration from the Azure portal\n+### How can I export my provisioning configuration?\n+To export your configuration:\n+1. In the [Azure portal](https://portal.azure.com/), on the left navigation panel, select **Azure Active Directory**.\n+2. In the **Azure Active Directory** pane, select **Enterprise applications** and choose your application.\n+3. In the left navigation pane, select **provisioning**. From the provisioning configuration page, click on **attribute mappings**, then **show advanced options**, and finally **review your schema**. This will take you to the schema editor. \n+5. Click on download in the command bar at the top of the page to download your schema.\n+\n+### Disaster recovery - roll back to a known good state\n+Exporting and saving your configuration allows you to roll back to a previous version of your configuration. We recommend exporting your provisioning configuration and saving it for later use anytime you make a change to your attribute mappings or scoping filters. All you need to do is open up the JSON file that you downloaded in the steps above, copy the entire contents of the JSON file, replace the entire contents of the JSON payload in the schema editor, and then save. If there is an active provisioning cycle, it will complete and the next cycle will use the updated schema. The next cycle will also be an initial cycle, which reevaluates every user and group based on the new configuration. \n ## Step 1: Retrieve your Provisioning App Service Principal ID (Object ID)\n \n 1. Launch the [Azure portal](https://portal.azure.com), and navigate to the Properties section of your  provisioning application. For e.g. if you want to export your *Workday to AD User Provisioning application* mapping navigate to the Properties section of that app. "
  },
  {
    "Number": 108330,
    "Title": "3/19 AM Publish",
    "ClosedAt": "2020-03-19T17:03:35Z",
    "User": "huypub",
    "FileName": "articles/active-directory/devices/hybrid-azuread-join-control.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -40,7 +40,7 @@ Use the Active Directory Services Interfaces Editor (ADSI Edit) to modify the SC\n 1. Launch the **ADSI Edit** desktop application from and administrative workstation or a domain controller as an Enterprise Administrator.\n 1. Connect to the **Configuration Naming Context** of your domain.\n 1. Browse to **CN=Configuration,DC=contoso,DC=com** > **CN=Services** > **CN=Device Registration Configuration**\n-1. Right click on the leaf object under **CN=Device Registration Configuration** and select **Properties**\n+1. Right click on the leaf object **CN=62a0ff2e-97b9-4513-943f-0d221bd30080** and select **Properties**\n    1. Select **keywords** from the **Attribute Editor** window and click **Edit**\n    1. Select the values of **azureADId** and **azureADName** (one at a time) and click **Remove**\n 1. Close **ADSI Edit**"
  },
  {
    "Number": 108330,
    "Title": "3/19 AM Publish",
    "ClosedAt": "2020-03-19T17:03:35Z",
    "User": "huypub",
    "FileName": "articles/active-directory/saas-apps/planview-enterprise-one-tutorial.md",
    "Addition": 3,
    "Delections": 3,
    "Changes": 6,
    "Patch": "@@ -88,7 +88,7 @@ Follow these steps to enable Azure AD SSO in the Azure portal.\n     `https://<SUBDOMAIN>.pvcloud.com/planview`\n \n \t> [!NOTE]\n-\t> These values are not real. Update these values with the actual Sign on URL and Identifier. Contact [Planview Enterprise One Client support team](mailto:hostingsupport@planview.com) to get these values. You can also refer to the patterns shown in the **Basic SAML Configuration** section in the Azure portal.\n+\t> These values are not real. Update these values with the actual Sign on URL and Identifier. Contact [Planview Enterprise One Client support team](mailto:customercare@planview.com) to get these values. You can also refer to the patterns shown in the **Basic SAML Configuration** section in the Azure portal.\n \n 1. On the **Set up single sign-on with SAML** page, in the **SAML Signing Certificate** section,  find **Federation Metadata XML** and select **Download** to download the certificate and save it on your computer.\n \n@@ -130,11 +130,11 @@ In this section, you'll enable B.Simon to use Azure single sign-on by granting a\n \n ## Configure Planview Enterprise One SSO\n \n-To configure single sign-on on **Planview Enterprise One** side, you need to send the downloaded **Federation Metadata XML** and appropriate copied URLs from Azure portal to [Planview Enterprise One support team](mailto:hostingsupport@planview.com). They set this setting to have the SAML SSO connection set properly on both sides.\n+To configure single sign-on on **Planview Enterprise One** side, you need to send the downloaded **Federation Metadata XML** and appropriate copied URLs from Azure portal to [Planview Enterprise One support team](mailto:customercare@planview.com). They set this setting to have the SAML SSO connection set properly on both sides.\n \n ### Create Planview Enterprise One test user\n \n-In this section, you create a user called B.Simon in Planview Enterprise One. Work with [Planview Enterprise One support team](mailto:hostingsupport@planview.com) to add the users in the Planview Enterprise One platform.Users must be created and activated before you use single sign-on.\n+In this section, you create a user called B.Simon in Planview Enterprise One. Work with [Planview Enterprise One support team](mailto:customercare@planview.com) to add the users in the Planview Enterprise One platform.Users must be created and activated before you use single sign-on.\n \n ## Test SSO \n "
  },
  {
    "Number": 108330,
    "Title": "3/19 AM Publish",
    "ClosedAt": "2020-03-19T17:03:35Z",
    "User": "huypub",
    "FileName": "articles/application-gateway/tutorial-url-redirect-powershell.md",
    "Addition": 7,
    "Delections": 7,
    "Changes": 14,
    "Patch": "@@ -4,17 +4,17 @@ description: Learn how to create an application gateway with URL path-based redi\n services: application-gateway\n author: vhorne\n ms.service: application-gateway\n-ms.date: 11/14/2019\n+ms.date: 03/19/2020\n ms.author: victorh\n-ms.topic: tutorial\n+ms.topic: conceptual\n #Customer intent: As an IT administrator, I want to use Azure PowerShell to set up URL path redirection of web traffic to specific pools of servers so I can ensure my customers have access to the information they need.\n ---\n \n # Create an application gateway with URL path-based redirection using Azure PowerShell\n \n-You can use Azure PowerShell to configure [URL-based routing rules](application-gateway-url-route-overview.md) when you create an [application gateway](application-gateway-introduction.md). In this tutorial, you create backend pools using  [virtual machine scale sets](../virtual-machine-scale-sets/virtual-machine-scale-sets-overview.md). You then create URL routing rules that make sure web traffic is redirected to the appropriate backend pool.\n+You can use Azure PowerShell to configure [URL-based routing rules](application-gateway-url-route-overview.md) when you create an [application gateway](application-gateway-introduction.md). In this article, you create backend pools using  [virtual machine scale sets](../virtual-machine-scale-sets/virtual-machine-scale-sets-overview.md). You then create URL routing rules that make sure web traffic is redirected to the appropriate backend pool.\n \n-In this tutorial, you learn how to:\n+In this article, you learn how to:\n \n > [!div class=\"checklist\"]\n > * Set up the network\n@@ -26,15 +26,15 @@ The following example shows site traffic coming from both ports 8080 and 8081 an\n \n ![URL routing example](./media/tutorial-url-redirect-powershell/scenario.png)\n \n-If you prefer, you can complete this tutorial using [Azure CLI](tutorial-url-redirect-cli.md).\n+If you prefer, you can complete this procedure using [Azure CLI](tutorial-url-redirect-cli.md).\n \n If you don't have an Azure subscription, create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.\n \n [!INCLUDE [updated-for-az](../../includes/updated-for-az.md)]\n \n [!INCLUDE [cloud-shell-try-it.md](../../includes/cloud-shell-try-it.md)]\n \n-If you choose to install and use the PowerShell locally, this tutorial requires the Azure PowerShell module version 1.0.0 or later. To find the version, run `Get-Module -ListAvailable Az` . If you need to upgrade, see [Install Azure PowerShell module](/powershell/azure/install-az-ps). If you are running PowerShell locally, you also need to run `Connect-AzAccount` to create a connection with Azure.\n+If you choose to install and use the PowerShell locally, this procedure requires the Azure PowerShell module version 1.0.0 or later. To find the version, run `Get-Module -ListAvailable Az` . If you need to upgrade, see [Install Azure PowerShell module](/powershell/azure/install-az-ps). If you are running PowerShell locally, you also need to run `Connect-AzAccount` to create a connection with Azure.\n \n ## Create a resource group\n \n@@ -125,7 +125,7 @@ $poolSettings = New-AzApplicationGatewayBackendHttpSettings `\n \n ### Create the default listener and rule\n \n-A listener is required to enable the application gateway to route traffic appropriately to a backend pool. In this tutorial, you create multiple listeners. The first basic listener expects traffic at the root URL. The other listeners expect traffic at specific URLs, such as `http://52.168.55.24:8080/images/` or `http://52.168.55.24:8081/video/`.\n+A listener is required to enable the application gateway to route traffic appropriately to a backend pool. In this article, you create multiple listeners. The first basic listener expects traffic at the root URL. The other listeners expect traffic at specific URLs, such as `http://52.168.55.24:8080/images/` or `http://52.168.55.24:8081/video/`.\n \n Create a listener named *defaultListener* using [New-AzApplicationGatewayHttpListener](/powershell/module/az.network/new-azapplicationgatewayhttplistener) with the frontend configuration and frontend port that you previously created. A rule is required for the listener to know which backend pool to use for incoming traffic. Create a basic rule named *rule1* using [New-AzApplicationGatewayRequestRoutingRule](/powershell/module/az.network/new-azapplicationgatewayrequestroutingrule).\n "
  },
  {
    "Number": 108330,
    "Title": "3/19 AM Publish",
    "ClosedAt": "2020-03-19T17:03:35Z",
    "User": "huypub",
    "FileName": "articles/azure-maps/tutorial-geofence.md",
    "Addition": 4,
    "Delections": 3,
    "Changes": 7,
    "Patch": "@@ -145,13 +145,13 @@ Once the Postman app is installed, follow these steps to upload the construction\n    }\n    ```\n \n-5. Click send and review the response header. Upon a successful request, the **Location** header will contain the status URI. The status URI is of the following format. \n+5. Click send and review the response header. Upon a successful request, the **Location** header will contain the status URI. The status URI is of the following format. The uploadStatusId value isn't between { }. It's a common practice to use { } to show values that the user must enter, or values that are different for different user.\n \n    ```HTTP\n    https://atlas.microsoft.com/mapData/{uploadStatusId}/status?api-version=1.0\n    ```\n \n-6. Copy your status URI and append the subscription-key. The status URI format should be like the one below. Notice that in the format below, you would change the {subscription-key}, including the { }, with your subscription key.\n+6. Copy your status URI and append the subscription-key. The status URI format should be like the one below. Notice that in the format below, you would change the {subscription-key}, don't including the { }, with your subscription key.\n \n    ```HTTP\n    https://atlas.microsoft.com/mapData/{uploadStatusId}/status?api-version=1.0&subscription-key={Subscription-key}\n@@ -161,7 +161,8 @@ Once the Postman app is installed, follow these steps to upload the construction\n \n    ```JSON\n    {\n-    \"udid\" : \"{udId}\"\n+    \"status\": \"Succeeded\",\n+    \"resourceLocation\": \"https://atlas.microsoft.com/mapData/metadata/{udId}?api-version=1.0\"\n    }\n    ```\n "
  },
  {
    "Number": 108330,
    "Title": "3/19 AM Publish",
    "ClosedAt": "2020-03-19T17:03:35Z",
    "User": "huypub",
    "FileName": "articles/azure-monitor/insights/ad-replication-status.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Monitor Active Directory replication status with Azure Monitor | Microsoft Docs\n+title: Monitor Active Directory replication status\n description: The Active Directory Replication Status solution pack regularly monitors your Active Directory environment for any replication failures.\n ms.subservice: logs\n ms.topic: conceptual"
  },
  {
    "Number": 108330,
    "Title": "3/19 AM Publish",
    "ClosedAt": "2020-03-19T17:03:35Z",
    "User": "huypub",
    "FileName": "articles/azure-monitor/insights/container-insights-health-monitors-config.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -57,7 +57,7 @@ Azure Monitor for containers includes a number of key monitoring scenarios that\n |Container memory utilization |Unit monitor |This monitor reports combined health status of the Memory utilization(RSS) of the instances of the container.<br> It performs a simple comparison that compares each sample to a single threshold, and specified by the configuration parameter **ConsecutiveSamplesForStateTransition**.<br> Its state is calculated as the worst state of 90% of the container (StateThresholdPercentage) instances, sorted in descending order of severity of container health state (that is, Critical, Warning, Healthy).<br> If no record is received from a container instance, then the health state of the container instance is reported as **Unknown**, and has higher precedence in the sorting order over the **Critical** state.<br> Each individual container instance's state is calculated using the thresholds specified in the configuration. If the usage is over critical threshold (90%), then the instance is in a **Critical** state, if it is less than critical threshold (90%) but greater than warning threshold (80%), then the instance is in a **Warning** state. Otherwise, it is in **Healthy** state. |ConsecutiveSamplesForStateTransition<br> FailIfLessThanPercentage<br> StateThresholdPercentage<br> WarnIfGreaterThanPercentage| 3<br> 90<br> 90<br> 80 ||\n |Container CPU utilization |Unit monitor |This monitor reports combined health status of the CPU utilization of the instances of the container.<br> It performs a simple comparison that compares each sample to a single threshold, and specified by the configuration parameter **ConsecutiveSamplesForStateTransition**.<br> Its state is calculated as the worst state of 90% of the container (StateThresholdPercentage) instances, sorted in descending order of severity of container health state (that is, Critical, Warning, Healthy).<br> If no record is received from a container instance, then the health state of the container instance is reported as **Unknown**, and has higher precedence in the sorting order over the **Critical** state.<br> Each individual container instance's state is calculated using the thresholds specified in the configuration. If the usage is over critical threshold (90%), then the instance is in a **Critical** state, if it is less than critical threshold (90%) but greater than warning threshold (80%), then the instance is in a **Warning** state. Otherwise, it is in **Healthy** state. |ConsecutiveSamplesForStateTransition<br> FailIfLessThanPercentage<br> StateThresholdPercentage<br> WarnIfGreaterThanPercentage| 3<br> 90<br> 90<br> 80 ||\n |System workload pods ready |Unit monitor |This monitor reports status based on percentage of pods in ready state in a given workload. Its state is set to **Critical** if less than 100% of the pods are in a **Healthy** state |ConsecutiveSamplesForStateTransition<br> FailIfLessThanPercentage |2<br> 100 ||\n-|Kube API status |Unit monitor |This monitor reports status of Kube Api service. Monitor is in critical state in case Kube Api endpoint is unavailable. For this particular monitor, the state is determined by making a query to the 'nodes' endpoint for the kube-api server. Anything other than an OK response code changes the monitor to a **Critical** state. | No configuration properties |||\n+|Kube API status |Unit monitor |This monitor reports status of Kube API service. Monitor is in critical state in case Kube API endpoint is unavailable. For this particular monitor, the state is determined by making a query to the 'nodes' endpoint for the kube-api server. Anything other than an OK response code changes the monitor to a **Critical** state. | No configuration properties |||\n \n ### Aggregate monitors\n "
  },
  {
    "Number": 108330,
    "Title": "3/19 AM Publish",
    "ClosedAt": "2020-03-19T17:03:35Z",
    "User": "huypub",
    "FileName": "articles/azure-monitor/insights/network-performance-monitor-performance-monitor.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Performance Monitor feature in Network Performance Monitor solution in Azure Log Analytics | Microsoft Docs\n+title: Performance Monitor in Network Performance Monitor\n description: The Performance Monitor capability in Network Performance Monitor helps you monitor network connectivity across various points in your network. You can monitor cloud deployments and on-premises locations, multiple data centers and branch offices, and mission-critical multitier applications or microservices.\n ms.subservice: logs\n ms.topic: conceptual"
  },
  {
    "Number": 108330,
    "Title": "3/19 AM Publish",
    "ClosedAt": "2020-03-19T17:03:35Z",
    "User": "huypub",
    "FileName": "articles/azure-monitor/insights/scom-assessment.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Optimize your System Center Operations Manager environment with Azure Log Analytics | Microsoft Docs\n+title: Assess System Center Operations Manager with Azure Monitor\n description: You can use the System Center Operations Manager Health Check solution to assess the risk and health of your environments on a regular interval.\n ms.subservice: logs\n ms.topic: conceptual"
  },
  {
    "Number": 108330,
    "Title": "3/19 AM Publish",
    "ClosedAt": "2020-03-19T17:03:35Z",
    "User": "huypub",
    "FileName": "articles/azure-monitor/log-query/get-started-portal.md",
    "Addition": 124,
    "Delections": 117,
    "Changes": 241,
    "Patch": "@@ -1,136 +1,136 @@\n ---\n-title: Get started with Azure Monitor Log Analytics | Microsoft Docs\n-description: This article provides a tutorial for using Log Analytics in the Azure portal to write queries.\n+title: \"Tutorial: Get started with Log Analytics queries\"\n+description: Learn from this tutorial how to write and manage Azure Monitor log queries using Log Analytics in the Azure portal.\n ms.subservice: logs\n ms.topic: tutorial\n author: bwren\n ms.author: bwren\n-ms.date: 07/19/2019\n+ms.date: 03/17/2020\n \n ---\n \n-# Get started with Log Analytics in Azure Monitor\n+# Tutorial: Get started with Log Analytics queries\n \n-> [!NOTE]\n-> You can work through this exercise in your own environment if you are collecting data from at least one virtual machine. If not then use our [Demo environment](https://portal.loganalytics.io/demo), which includes plenty of sample data.\n+This tutorial shows you how to use Log Analytics to write, execute, and manage Azure Monitor log queries in the Azure portal. You can use Log Analytics queries to search for terms, identify trends, analyze patterns, and provide many other insights from your data. \n \n-In this tutorial you will learn how to use Log Analytics in the Azure portal to write Azure Monitor log queries. It will teach you how to:\n+In this tutorial, you learn how to use Log Analytics to:\n \n-- Use Log Analytics to write a simple query\n-- Understand the schema of your data\n-- Filter, sort, and group results\n-- Apply a time range\n-- Create charts\n-- Save and load queries\n-- Export and share queries\n+> [!div class=\"checklist\"]\n+> * Understand the log data schema\n+> * Write and run simple queries, and modify the time range for queries\n+> * Filter, sort, and group query results\n+> * View, modify, and share visuals of query results\n+> * Save, load, export, and copy queries and results\n \n-For a tutorial on writing log queries, see [Get started with log queries in Azure Monitor](get-started-queries.md).<br>\n-For more details on log queries, see [Overview of log queries in Azure Monitor](log-query-overview.md).\n+For more information about log queries, see [Overview of log queries in Azure Monitor](log-query-overview.md).<br/>\n+For a detailed tutorial on writing log queries, see [Get started with log queries in Azure Monitor](get-started-queries.md).\n \n-## Meet Log Analytics\n-Log Analytics is a web tool used to write and execute Azure Monitor log queries. Open it by selecting **Logs** in the Azure Monitor menu. It starts with a new blank query.\n+## Open Log Analytics\n+To use Log Analytics, you need to be signed in to an Azure account. If you don't have an Azure account, [create one for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).\n \n-![Home page](media/get-started-portal/homepage.png)\n+To complete most of the steps in this tutorial, you can use [this demo environment](https://portal.loganalytics.io/demo), which includes plenty of sample data. With the demo environment, you won't be able to save queries or pin results to a dashboard.\n \n-## Firewall requirements\n-To use Log Analytics, your browser requires access to the following addresses. If your browser is accessing the Azure portal through a firewall, you must enable access to these addresses.\n+You can also use your own environment, if you're using Azure Monitor to collect log data on at least one Azure resource. To open a Log Analytics workspace, in your Azure Monitor left navigation, select **Logs**. \n \n-| Uri | IP | Ports |\n-|:---|:---|:---|\n-| portal.loganalytics.io | Dynamic | 80,443 |\n-| api.loganalytics.io | Dynamic | 80,443 |\n-| docs.loganalytics.io | Dynamic | 80,443 |\n-\n-## Basic queries\n-Queries can be used to search terms, identify trends, analyze patterns, and provide many other insights based on your data. Start with a basic query:\n-\n-```Kusto\n-Event | search \"error\"\n-```\n-\n-This query searches the _Event_ table for records that contain the term _error_ in any property.\n-\n-Queries can start with either a table name or a [search](/azure/kusto/query/searchoperator) command. The above example starts with the table name _Event_, which retrieves all records from the Event table. The pipe (|) character separates commands, so the output of the first one serves as the input of the following command. You can add any number of commands to a single query.\n-\n-Another way to write that same query would be:\n-\n-```Kusto\n-search in (Event) \"error\"\n-```\n-\n-In this example, **search** is scoped to the _Event_ table, and all records in that table are searched for the term _error_.\n+## Understand the schema\n+A *schema* is a collection of tables grouped under logical categories. The Demo schema has several categories from monitoring solutions. For example, the **LogManagement** category contains Windows and Syslog events, performance data, and agent heartbeats.\n \n-## Running a query\n-Run a query by clicking the **Run** button or pressing **Shift+Enter**. Consider the following details which determine the code that will be run and the data that's returned:\n+The schema tables appear on the **Tables** tab of the Log Analytics workspace. The tables contain columns, each with a data type shown by the icon next to the column name. For example, the **Event** table contains text columns like **Computer** and numerical columns like **EventCategory**.\n \n-- Line breaks: A single break makes your query easier to read. Multiple line breaks split it into separate queries.\n-- Cursor: Place your cursor somewhere inside the query to execute it. The current query is considered to be the code up until a blank line is found.\n-- Time range - A time range of _last 24 hours_ is set by default. To use a different range, use the time-picker or add an explicit time range filter to your query.\n+![Schema](media/get-started-portal/schema.png)\n \n+## Write and run basic queries\n \n-## Understand the schema\n-The schema is a collection of tables visually grouped under a logical category. Several of the categories are from monitoring solutions. The _LogManagement_ category contains common data such as Windows and Syslog events, performance data, and agent heartbeats.\n+Log Analytics opens with a new blank query in the **Query editor**.\n \n-![Schema](media/get-started-portal/schema.png)\n+![Log Analytics](media/get-started-portal/homepage.png)\n \n-In each table, data is organized in columns with different data types as indicated by icons next to the column name. For example, the _Event_ table shown in the screenshot contains columns such as _Computer_ which is text, _EventCategory_ which is a number, and _TimeGenerated_ which is date/time.\n+### Write a query\n+Azure Monitor log queries use a version of the Kusto query language. Queries can begin with either a table name or a [search](/azure/kusto/query/searchoperator) command. \n \n-## Filter the results\n-Start by getting everything in the _Event_ table.\n+The following query retrieves all records from the **Event** table:\n \n ```Kusto\n Event\n ```\n \n-Log Analytics automatically scopes results by:\n-\n-- Time range:  By default, queries are limited to the last 24 hours.\n-- Number of results: Results are limited to maximum of 10,000 records.\n+The pipe (|) character separates commands, so the output of the first command is the input of the next command. You can add any number of commands to a single query. The following query retrieves the records from the **Event** table, and then searches them for the term **error** in any property:\n \n-This query is very general, and it returns too many results to be useful. You can filter the results either through the table elements, or by explicitly adding a filter to the query. Filtering results through the table elements applies to the existing result set, while a filter to the query itself will return a new filtered result set and could therefore produce more accurate results.\n+```Kusto\n+Event \n+| search \"error\"\n+```\n \n-### Add a filter to the query\n-There is an arrow to the left of each record. Click this arrow to open the details for a specific record.\n+A single line break makes queries easier to read. More than one line break splits the query into separate queries.\n \n-Hover above a column name for the \"+\" and \"-\" icons to display. To add a filter that will return only records with the same value, click the \"+\" sign. Click \"-\" to exclude records with this value and then click **Run** to run the query again.\n+Another way to write the same query is:\n \n-![Add filter to query](media/get-started-portal/add-filter.png)\n+```Kusto\n+search in (Event) \"error\"\n+```\n \n-### Filter through the table elements\n-Now let's focus on events with a severity of _Error_. This is specified in a column named _EventLevelName_. You'll need to scroll to the right to see this column.\n+In the second example, the **search** command searches only records in the **Events** table for the term **error**.\n \n-Click the Filter icon next to the column title, and in the pop-up window select values that _Starts with_ the text _error_:\n+By default, Log Analytics limits queries to a time range of the past 24 hours. To set a different time range, you can add an explicit **TimeGenerated** filter to the query, or use the **Time range** control.\n \n-![Filter](media/get-started-portal/filter.png)\n+### Use the Time range control\n+To use the **Time range** control, select it in the top bar, and then select a value from the dropdown list, or select **Custom** to create a custom time range.\n \n+![Time picker](media/get-started-portal/time-picker.png)\n \n-## Sort and group results\n-The results are now narrowed down to include only error events from SQL Server, created in the last 24 hours. However, the results are not sorted in any way. To sort the results by a specific column, such as _timestamp_ for example, click the column title. One click sorts in ascending order while a second click will sort in descending.\n+- Time range values are in UTC, which could be different than your local time zone.\n+- If the query explicitly sets a filter for **TimeGenerated**, the time picker control shows **Set in query**, and is disabled to prevent a conflict.\n+\n+### Run a query\n+To run a query, place your cursor somewhere inside the query, and select **Run** in the top bar or press **Shift**+**Enter**. The query runs until it finds a blank line.\n+\n+## Filter results\n+Log Analytics limits results to a maximum of 10,000 records. A general query like `Event` returns too many results to be useful. You can filter query results either through restricting the table elements in the query, or by explicitly adding a filter to the results. Filtering through the table elements returns a new result set, while an explicit filter applies to the existing result set.\n+\n+### Filter by restricting table elements\n+To filter `Event` query results to **Error** events by restricting table elements in the query:\n+\n+1. In the query results, select the dropdown arrow next to any record that has **Error** in the **EventLevelName** column. \n+   \n+1. In the expanded details, hover over and select the **...** next to **EventLevelName**, and then select **Include \"Error\"**. \n+   \n+   ![Add filter to query](media/get-started-portal/add-filter.png)\n+   \n+1. Notice that the query in the **Query editor** has now changed to:\n+   \n+   ```Kusto\n+   Event\n+   | where EventLevelName == \"Error\"\n+   ```\n+   \n+1. Select **Run** to run the new query.\n+\n+### Filter by explicitly filtering results\n+To filter the `Event` query results to **Error** events by filtering the query results:\n+\n+1. In the query results, select the **Filter** icon next to the column heading **EventLevelName**. \n+   \n+1. In the first field of the pop-up window, select **Is equal to**, and in the next field, enter *error*. \n+   \n+1. Select **Filter**.\n+   \n+   ![Filter](media/get-started-portal/filter.png)\n+\n+## Sort, group, and select columns\n+To sort query results by a specific column, such as **TimeGenerated [UTC]**, select the column heading. Select the heading again to toggle between ascending and descending order.\n \n ![Sort column](media/get-started-portal/sort-column.png)\n \n-Another way to organize results is by groups. To group results by a specific column, simply drag the column header above the other columns. To create subgroups, drag other columns the upper bar as well.\n+Another way to organize results is by groups. To group results by a specific column, drag the column header to the bar above the results table labeled **Drag a column header and drop it here to group by that column**. To create subgroups, drag other columns to the upper bar. You can rearrange the hierarchy and sorting of the groups and subgroups in the bar.\n \n ![Groups](media/get-started-portal/groups.png)\n \n-## Select columns to display\n-The results table often includes a lot of columns. You might find that some of the returned columns are not displayed by default, or you may want to remove some the columns that are displayed. To select the columns to show, click the Columns button:\n+To hide or show columns in the results, select **Columns** above the table, and then select or deselect the columns you want from the dropdown list.\n \n ![Select columns](media/get-started-portal/select-columns.png)\n \n-\n-## Select a time range\n-By default, Log Analytics applies the _last 24 hours_ time range. To use a different range, select another value through the time picker and click **Run**. In addition to the preset values, you can use the _Custom time range_ option to select an absolute range for your query.\n-\n-![Time picker](media/get-started-portal/time-picker.png)\n-\n-When selecting a custom time range, the selected values are in UTC, which could be different than your local time zone.\n-\n-If the query explicitly contains a filter for _TimeGenerated_, the time picker title will show _Set in query_. Manual selection will be disabled to prevent a conflict.\n-\n-\n-## Charts\n-In addition to returning results in a table, query results can be presented in visual formats. Use the following query as an example:\n+## View and modify charts\n+You can also see query results in visual formats. Enter the following query as an example:\n \n ```Kusto\n Event \n@@ -139,58 +139,65 @@ Event\n | summarize count() by Source \n ```\n \n-By default, results are displayed in a table. Click _Chart_ to see the results in a graphic view:\n+By default, results appear in a table. Select **Chart** above the table to see the results in a graphic view.\n \n ![Bar chart](media/get-started-portal/bar-chart.png)\n \n-The results are shown in a stacked bar chart. Click _Stacked Column_ and select _Pie_ to show another view of the results:\n+The results appear in a stacked bar chart. Select other options like **Stacked Column** or **Pie** to show other views of the results.\n \n ![Pie chart](media/get-started-portal/pie-chart.png)\n \n-Different properties of the view, such as x and y axes, or grouping and splitting preferences, can be changed manually from the control bar.\n-\n-You can also set the preferred view in the query itself, using the render operator.\n-\n-### Smart diagnostics\n-On a timechart, if there is a sudden spike or step in your data, you may see a highlighted point on the line. This indicates that _Smart Diagnostics_ has identified a combination of properties that filter out the sudden change. Click the point to get more detail on the filter, and to see the filtered version. This may help you identify what caused the change:\n+You can change properties of the view, such as x and y axes, or grouping and splitting preferences, manually from the control bar.\n \n-![Smart diagnostics](media/get-started-portal/smart-diagnostics.png)\n+You can also set the preferred view in the query itself, using the [render](/azure/kusto/query/renderoperator) operator.\n \n-## Pin to dashboard\n-To pin a diagram or table to one of your shared Azure dashboards, click the pin icon. Note that this icon has moved to the top of the Log Analytics window, different from the screenshot below.\n+## Pin results to a dashboard\n+To pin a results table or chart from Log Analytics to a shared Azure dashboard, select **Pin to dashboard** on the top bar. \n \n ![Pin to dashboard](media/get-started-portal/pin-dashboard.png)\n \n-Certain simplifications are applied to a chart when you pin it to a dashboard:\n+In the **Pin to another dashboard** pane, select or create a shared dashboard to pin to, and select **Apply**. The table or chart appears on the selected Azure dashboard.\n \n-- Table columns and rows: In order to pin a table to the dashboard, it must have four or fewer columns. Only the top seven rows are displayed.\n-- Time restriction: Queries are automatically limited to the past 14 days.\n-- Bin count restriction: If you display a chart that has a lot of discrete bins, less populated bins are automatically grouped into a single _others_ bin.\n+![Chart pinned to dashboard](media/get-started-portal/pin-dashboard2.png)\n \n-## Save queries\n-Once you've created a useful query, you might want to save it or share with others. The **Save** icon is on the top bar.\n+A table or chart that you pin to a shared dashboard has the following simplifications: \n \n-You can save either the entire query page, or a single query as a function. Functions are queries that can also be referenced by other queries. In order to save a query as a function, you must provide a function alias, which is the name used to call this query when referenced by other queries.\n+- Data is limited to the past 14 days.\n+- A table shows only up to four columns and the top seven rows.\n+- Charts with many discrete categories automatically group less populated categories into a single **others** bin.\n \n-![Save function](media/get-started-portal/save-function.png)\n+## Save, load, or export queries\n+Once you create a query, you can save or share the query or results with others. \n \n->[!NOTE]\n->The following characters are supported - `a–z, A–Z, 0-9, -, _, ., <space>, (, ), |` in the **Name** field when saving or editing the saved query.\n+### Save queries\n+To save a query:\n \n-Log Analytics queries are always saved to a selected workspace, and shared with other users of that workspace.\n+1. Select **Save** on the top bar.\n+   \n+1. In the **Save** dialog, give the query a **Name**, using the characters a–z, A–Z, 0-9, space, hyphen, underscore, period, parenthesis, or pipe. \n+   \n+1. Select whether to save the query as a **Query** or a **Function**. Functions are queries that other queries can reference. \n+   \n+   To save a query as a function, provide a **Function Alias**, which is a short name for other queries to use to call this query.\n+   \n+1. Provide a **Category** for **Query explorer** to use for the query.\n+   \n+1. Select **Save**.\n+   \n+   ![Save function](media/get-started-portal/save-function.png)\n \n-## Load queries\n-The Query Explorer icon is at the top-right area. This lists all saved queries by category. It also enables you to mark specific queries as Favorites to quickly find them in the future. Double-click a saved query to add it to the current window.\n+### Load queries\n+To load a saved query, select **Query explorer** at upper right. The **Query explorer** pane opens, listing all queries by category. Expand the categories or enter a query name in the search bar, then select a query to load it into the **Query editor**. You can mark a query as a **Favorite** by selecting the star next to the query name.\n \n ![Query explorer](media/get-started-portal/query-explorer.png)\n \n-## Export and share as link\n-Log Analytics supports several exporting methods:\n+### Export and share queries\n+To export a query, select **Export** on the top bar, and then select **Export to CSV - all columns**, **Export to CSV - displayed columns**, or **Export to Power BI (M query)** from the dropdown list.\n \n-- Excel: Save the results as a CSV file.\n-- Power BI: Export the results to Power BI. See [Import Azure Monitor log data into Power BI](../../azure-monitor/platform/powerbi.md) for details.\n-- Share a link: The query itself can be shared as a link which can then be sent and executed by other users that have access to the same workspace.\n+To share a link to a query, select **Copy link** on the top bar, and then select **Copy link to query**, **Copy query text**, or **Copy query results** to copy to the clipboard. You can send the query link to others who have access to the same workspace.\n \n ## Next steps\n \n-- Learn more about [writing Azure Monitor log queries](get-started-queries.md).\n+Advance to the next tutorial to learn more about writing Azure Monitor log queries.\n+> [!div class=\"nextstepaction\"]\n+> [Write Azure Monitor log queries](get-started-queries.md)"
  },
  {
    "Number": 108330,
    "Title": "3/19 AM Publish",
    "ClosedAt": "2020-03-19T17:03:35Z",
    "User": "huypub",
    "FileName": "articles/azure-monitor/platform/activity-log-collect-tenants.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Collect Azure Activity logs into a Log Analytics workspace across Azure tenants | Microsoft Docs\n+title: Cross-tenant Azure activity logs in Azure Monitor\n description: Use Event Hubs and Logic Apps to collect data from the Azure Activity Log and send it to a Log Analytics workspace in Azure Monitor in a different tenant.\n ms.subservice: logs\n ms.topic: conceptual"
  },
  {
    "Number": 108330,
    "Title": "3/19 AM Publish",
    "ClosedAt": "2020-03-19T17:03:35Z",
    "User": "huypub",
    "FileName": "articles/azure-monitor/platform/activity-log-collect.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Collect and analyze Azure activity logs in Log Analytics workspace | Microsoft Docs\n+title: Collect Azure activity log in Log Analytics workspace\n description: Collect the Azure Activity Log in Azure Monitor Logs and use the monitoring solution to analyze and search the Azure activity log across all your Azure subscriptions.\n ms.subservice: logs\n ms.topic: conceptual"
  },
  {
    "Number": 108330,
    "Title": "3/19 AM Publish",
    "ClosedAt": "2020-03-19T17:03:35Z",
    "User": "huypub",
    "FileName": "articles/azure-monitor/platform/create-pipeline-datacollector-api.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Create a data pipeline with the Azure Monitor Data Collector API | Microsoft Docs\n+title: Use Data Collector API to create a data pipeline\n description: You can use the Azure Monitor HTTP Data Collector API to add POST JSON data to the Log Analytics workspace from any client that can call the REST API. This article describes how to upload data stored in files in an automated way.\n ms.subservice: logs\n ms.topic: conceptual"
  },
  {
    "Number": 108330,
    "Title": "3/19 AM Publish",
    "ClosedAt": "2020-03-19T17:03:35Z",
    "User": "huypub",
    "FileName": "articles/azure-monitor/platform/itsmc-connections.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Supported connections with IT Service Management Connector in Azure Log Analytics | Microsoft Docs\n+title: IT Service Management Connector in Azure Monitor\n description: This article provides information about how to connect your ITSM products/services with the IT Service Management Connector (ITSMC) in Azure Monitor to centrally monitor and manage the ITSM work items.\n ms.subservice: logs\n ms.topic: conceptual"
  },
  {
    "Number": 108330,
    "Title": "3/19 AM Publish",
    "ClosedAt": "2020-03-19T17:03:35Z",
    "User": "huypub",
    "FileName": "articles/azure-monitor/platform/itsmc-service-manager-script.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Automated script to create Service Manager Web app to connect with IT Service Management Connector in Azure | Microsoft Docs\n+title: Create web app for Service Management Connector\n description: Create a Service Manager Web app using an automated script to connect with IT Service Management Connector in Azure, and centrally monitor and manage the ITSM work items.\n ms.subservice: logs\n ms.topic: conceptual"
  },
  {
    "Number": 106405,
    "Title": "Refresh log-query get-started-portal",
    "ClosedAt": "2020-03-19T16:38:37Z",
    "User": "v-thepet",
    "FileName": "articles/azure-monitor/log-query/get-started-portal.md",
    "Addition": 124,
    "Delections": 117,
    "Changes": 241,
    "Patch": "@@ -1,136 +1,136 @@\n ---\n-title: Get started with Azure Monitor Log Analytics | Microsoft Docs\n-description: This article provides a tutorial for using Log Analytics in the Azure portal to write queries.\n+title: \"Tutorial: Get started with Log Analytics queries\"\n+description: Learn from this tutorial how to write and manage Azure Monitor log queries using Log Analytics in the Azure portal.\n ms.subservice: logs\n ms.topic: tutorial\n author: bwren\n ms.author: bwren\n-ms.date: 07/19/2019\n+ms.date: 03/17/2020\n \n ---\n \n-# Get started with Log Analytics in Azure Monitor\n+# Tutorial: Get started with Log Analytics queries\n \n-> [!NOTE]\n-> You can work through this exercise in your own environment if you are collecting data from at least one virtual machine. If not then use our [Demo environment](https://portal.loganalytics.io/demo), which includes plenty of sample data.\n+This tutorial shows you how to use Log Analytics to write, execute, and manage Azure Monitor log queries in the Azure portal. You can use Log Analytics queries to search for terms, identify trends, analyze patterns, and provide many other insights from your data. \n \n-In this tutorial you will learn how to use Log Analytics in the Azure portal to write Azure Monitor log queries. It will teach you how to:\n+In this tutorial, you learn how to use Log Analytics to:\n \n-- Use Log Analytics to write a simple query\n-- Understand the schema of your data\n-- Filter, sort, and group results\n-- Apply a time range\n-- Create charts\n-- Save and load queries\n-- Export and share queries\n+> [!div class=\"checklist\"]\n+> * Understand the log data schema\n+> * Write and run simple queries, and modify the time range for queries\n+> * Filter, sort, and group query results\n+> * View, modify, and share visuals of query results\n+> * Save, load, export, and copy queries and results\n \n-For a tutorial on writing log queries, see [Get started with log queries in Azure Monitor](get-started-queries.md).<br>\n-For more details on log queries, see [Overview of log queries in Azure Monitor](log-query-overview.md).\n+For more information about log queries, see [Overview of log queries in Azure Monitor](log-query-overview.md).<br/>\n+For a detailed tutorial on writing log queries, see [Get started with log queries in Azure Monitor](get-started-queries.md).\n \n-## Meet Log Analytics\n-Log Analytics is a web tool used to write and execute Azure Monitor log queries. Open it by selecting **Logs** in the Azure Monitor menu. It starts with a new blank query.\n+## Open Log Analytics\n+To use Log Analytics, you need to be signed in to an Azure account. If you don't have an Azure account, [create one for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).\n \n-![Home page](media/get-started-portal/homepage.png)\n+To complete most of the steps in this tutorial, you can use [this demo environment](https://portal.loganalytics.io/demo), which includes plenty of sample data. With the demo environment, you won't be able to save queries or pin results to a dashboard.\n \n-## Firewall requirements\n-To use Log Analytics, your browser requires access to the following addresses. If your browser is accessing the Azure portal through a firewall, you must enable access to these addresses.\n+You can also use your own environment, if you're using Azure Monitor to collect log data on at least one Azure resource. To open a Log Analytics workspace, in your Azure Monitor left navigation, select **Logs**. \n \n-| Uri | IP | Ports |\n-|:---|:---|:---|\n-| portal.loganalytics.io | Dynamic | 80,443 |\n-| api.loganalytics.io | Dynamic | 80,443 |\n-| docs.loganalytics.io | Dynamic | 80,443 |\n-\n-## Basic queries\n-Queries can be used to search terms, identify trends, analyze patterns, and provide many other insights based on your data. Start with a basic query:\n-\n-```Kusto\n-Event | search \"error\"\n-```\n-\n-This query searches the _Event_ table for records that contain the term _error_ in any property.\n-\n-Queries can start with either a table name or a [search](/azure/kusto/query/searchoperator) command. The above example starts with the table name _Event_, which retrieves all records from the Event table. The pipe (|) character separates commands, so the output of the first one serves as the input of the following command. You can add any number of commands to a single query.\n-\n-Another way to write that same query would be:\n-\n-```Kusto\n-search in (Event) \"error\"\n-```\n-\n-In this example, **search** is scoped to the _Event_ table, and all records in that table are searched for the term _error_.\n+## Understand the schema\n+A *schema* is a collection of tables grouped under logical categories. The Demo schema has several categories from monitoring solutions. For example, the **LogManagement** category contains Windows and Syslog events, performance data, and agent heartbeats.\n \n-## Running a query\n-Run a query by clicking the **Run** button or pressing **Shift+Enter**. Consider the following details which determine the code that will be run and the data that's returned:\n+The schema tables appear on the **Tables** tab of the Log Analytics workspace. The tables contain columns, each with a data type shown by the icon next to the column name. For example, the **Event** table contains text columns like **Computer** and numerical columns like **EventCategory**.\n \n-- Line breaks: A single break makes your query easier to read. Multiple line breaks split it into separate queries.\n-- Cursor: Place your cursor somewhere inside the query to execute it. The current query is considered to be the code up until a blank line is found.\n-- Time range - A time range of _last 24 hours_ is set by default. To use a different range, use the time-picker or add an explicit time range filter to your query.\n+![Schema](media/get-started-portal/schema.png)\n \n+## Write and run basic queries\n \n-## Understand the schema\n-The schema is a collection of tables visually grouped under a logical category. Several of the categories are from monitoring solutions. The _LogManagement_ category contains common data such as Windows and Syslog events, performance data, and agent heartbeats.\n+Log Analytics opens with a new blank query in the **Query editor**.\n \n-![Schema](media/get-started-portal/schema.png)\n+![Log Analytics](media/get-started-portal/homepage.png)\n \n-In each table, data is organized in columns with different data types as indicated by icons next to the column name. For example, the _Event_ table shown in the screenshot contains columns such as _Computer_ which is text, _EventCategory_ which is a number, and _TimeGenerated_ which is date/time.\n+### Write a query\n+Azure Monitor log queries use a version of the Kusto query language. Queries can begin with either a table name or a [search](/azure/kusto/query/searchoperator) command. \n \n-## Filter the results\n-Start by getting everything in the _Event_ table.\n+The following query retrieves all records from the **Event** table:\n \n ```Kusto\n Event\n ```\n \n-Log Analytics automatically scopes results by:\n-\n-- Time range:  By default, queries are limited to the last 24 hours.\n-- Number of results: Results are limited to maximum of 10,000 records.\n+The pipe (|) character separates commands, so the output of the first command is the input of the next command. You can add any number of commands to a single query. The following query retrieves the records from the **Event** table, and then searches them for the term **error** in any property:\n \n-This query is very general, and it returns too many results to be useful. You can filter the results either through the table elements, or by explicitly adding a filter to the query. Filtering results through the table elements applies to the existing result set, while a filter to the query itself will return a new filtered result set and could therefore produce more accurate results.\n+```Kusto\n+Event \n+| search \"error\"\n+```\n \n-### Add a filter to the query\n-There is an arrow to the left of each record. Click this arrow to open the details for a specific record.\n+A single line break makes queries easier to read. More than one line break splits the query into separate queries.\n \n-Hover above a column name for the \"+\" and \"-\" icons to display. To add a filter that will return only records with the same value, click the \"+\" sign. Click \"-\" to exclude records with this value and then click **Run** to run the query again.\n+Another way to write the same query is:\n \n-![Add filter to query](media/get-started-portal/add-filter.png)\n+```Kusto\n+search in (Event) \"error\"\n+```\n \n-### Filter through the table elements\n-Now let's focus on events with a severity of _Error_. This is specified in a column named _EventLevelName_. You'll need to scroll to the right to see this column.\n+In the second example, the **search** command searches only records in the **Events** table for the term **error**.\n \n-Click the Filter icon next to the column title, and in the pop-up window select values that _Starts with_ the text _error_:\n+By default, Log Analytics limits queries to a time range of the past 24 hours. To set a different time range, you can add an explicit **TimeGenerated** filter to the query, or use the **Time range** control.\n \n-![Filter](media/get-started-portal/filter.png)\n+### Use the Time range control\n+To use the **Time range** control, select it in the top bar, and then select a value from the dropdown list, or select **Custom** to create a custom time range.\n \n+![Time picker](media/get-started-portal/time-picker.png)\n \n-## Sort and group results\n-The results are now narrowed down to include only error events from SQL Server, created in the last 24 hours. However, the results are not sorted in any way. To sort the results by a specific column, such as _timestamp_ for example, click the column title. One click sorts in ascending order while a second click will sort in descending.\n+- Time range values are in UTC, which could be different than your local time zone.\n+- If the query explicitly sets a filter for **TimeGenerated**, the time picker control shows **Set in query**, and is disabled to prevent a conflict.\n+\n+### Run a query\n+To run a query, place your cursor somewhere inside the query, and select **Run** in the top bar or press **Shift**+**Enter**. The query runs until it finds a blank line.\n+\n+## Filter results\n+Log Analytics limits results to a maximum of 10,000 records. A general query like `Event` returns too many results to be useful. You can filter query results either through restricting the table elements in the query, or by explicitly adding a filter to the results. Filtering through the table elements returns a new result set, while an explicit filter applies to the existing result set.\n+\n+### Filter by restricting table elements\n+To filter `Event` query results to **Error** events by restricting table elements in the query:\n+\n+1. In the query results, select the dropdown arrow next to any record that has **Error** in the **EventLevelName** column. \n+   \n+1. In the expanded details, hover over and select the **...** next to **EventLevelName**, and then select **Include \"Error\"**. \n+   \n+   ![Add filter to query](media/get-started-portal/add-filter.png)\n+   \n+1. Notice that the query in the **Query editor** has now changed to:\n+   \n+   ```Kusto\n+   Event\n+   | where EventLevelName == \"Error\"\n+   ```\n+   \n+1. Select **Run** to run the new query.\n+\n+### Filter by explicitly filtering results\n+To filter the `Event` query results to **Error** events by filtering the query results:\n+\n+1. In the query results, select the **Filter** icon next to the column heading **EventLevelName**. \n+   \n+1. In the first field of the pop-up window, select **Is equal to**, and in the next field, enter *error*. \n+   \n+1. Select **Filter**.\n+   \n+   ![Filter](media/get-started-portal/filter.png)\n+\n+## Sort, group, and select columns\n+To sort query results by a specific column, such as **TimeGenerated [UTC]**, select the column heading. Select the heading again to toggle between ascending and descending order.\n \n ![Sort column](media/get-started-portal/sort-column.png)\n \n-Another way to organize results is by groups. To group results by a specific column, simply drag the column header above the other columns. To create subgroups, drag other columns the upper bar as well.\n+Another way to organize results is by groups. To group results by a specific column, drag the column header to the bar above the results table labeled **Drag a column header and drop it here to group by that column**. To create subgroups, drag other columns to the upper bar. You can rearrange the hierarchy and sorting of the groups and subgroups in the bar.\n \n ![Groups](media/get-started-portal/groups.png)\n \n-## Select columns to display\n-The results table often includes a lot of columns. You might find that some of the returned columns are not displayed by default, or you may want to remove some the columns that are displayed. To select the columns to show, click the Columns button:\n+To hide or show columns in the results, select **Columns** above the table, and then select or deselect the columns you want from the dropdown list.\n \n ![Select columns](media/get-started-portal/select-columns.png)\n \n-\n-## Select a time range\n-By default, Log Analytics applies the _last 24 hours_ time range. To use a different range, select another value through the time picker and click **Run**. In addition to the preset values, you can use the _Custom time range_ option to select an absolute range for your query.\n-\n-![Time picker](media/get-started-portal/time-picker.png)\n-\n-When selecting a custom time range, the selected values are in UTC, which could be different than your local time zone.\n-\n-If the query explicitly contains a filter for _TimeGenerated_, the time picker title will show _Set in query_. Manual selection will be disabled to prevent a conflict.\n-\n-\n-## Charts\n-In addition to returning results in a table, query results can be presented in visual formats. Use the following query as an example:\n+## View and modify charts\n+You can also see query results in visual formats. Enter the following query as an example:\n \n ```Kusto\n Event \n@@ -139,58 +139,65 @@ Event\n | summarize count() by Source \n ```\n \n-By default, results are displayed in a table. Click _Chart_ to see the results in a graphic view:\n+By default, results appear in a table. Select **Chart** above the table to see the results in a graphic view.\n \n ![Bar chart](media/get-started-portal/bar-chart.png)\n \n-The results are shown in a stacked bar chart. Click _Stacked Column_ and select _Pie_ to show another view of the results:\n+The results appear in a stacked bar chart. Select other options like **Stacked Column** or **Pie** to show other views of the results.\n \n ![Pie chart](media/get-started-portal/pie-chart.png)\n \n-Different properties of the view, such as x and y axes, or grouping and splitting preferences, can be changed manually from the control bar.\n-\n-You can also set the preferred view in the query itself, using the render operator.\n-\n-### Smart diagnostics\n-On a timechart, if there is a sudden spike or step in your data, you may see a highlighted point on the line. This indicates that _Smart Diagnostics_ has identified a combination of properties that filter out the sudden change. Click the point to get more detail on the filter, and to see the filtered version. This may help you identify what caused the change:\n+You can change properties of the view, such as x and y axes, or grouping and splitting preferences, manually from the control bar.\n \n-![Smart diagnostics](media/get-started-portal/smart-diagnostics.png)\n+You can also set the preferred view in the query itself, using the [render](/azure/kusto/query/renderoperator) operator.\n \n-## Pin to dashboard\n-To pin a diagram or table to one of your shared Azure dashboards, click the pin icon. Note that this icon has moved to the top of the Log Analytics window, different from the screenshot below.\n+## Pin results to a dashboard\n+To pin a results table or chart from Log Analytics to a shared Azure dashboard, select **Pin to dashboard** on the top bar. \n \n ![Pin to dashboard](media/get-started-portal/pin-dashboard.png)\n \n-Certain simplifications are applied to a chart when you pin it to a dashboard:\n+In the **Pin to another dashboard** pane, select or create a shared dashboard to pin to, and select **Apply**. The table or chart appears on the selected Azure dashboard.\n \n-- Table columns and rows: In order to pin a table to the dashboard, it must have four or fewer columns. Only the top seven rows are displayed.\n-- Time restriction: Queries are automatically limited to the past 14 days.\n-- Bin count restriction: If you display a chart that has a lot of discrete bins, less populated bins are automatically grouped into a single _others_ bin.\n+![Chart pinned to dashboard](media/get-started-portal/pin-dashboard2.png)\n \n-## Save queries\n-Once you've created a useful query, you might want to save it or share with others. The **Save** icon is on the top bar.\n+A table or chart that you pin to a shared dashboard has the following simplifications: \n \n-You can save either the entire query page, or a single query as a function. Functions are queries that can also be referenced by other queries. In order to save a query as a function, you must provide a function alias, which is the name used to call this query when referenced by other queries.\n+- Data is limited to the past 14 days.\n+- A table shows only up to four columns and the top seven rows.\n+- Charts with many discrete categories automatically group less populated categories into a single **others** bin.\n \n-![Save function](media/get-started-portal/save-function.png)\n+## Save, load, or export queries\n+Once you create a query, you can save or share the query or results with others. \n \n->[!NOTE]\n->The following characters are supported - `a–z, A–Z, 0-9, -, _, ., <space>, (, ), |` in the **Name** field when saving or editing the saved query.\n+### Save queries\n+To save a query:\n \n-Log Analytics queries are always saved to a selected workspace, and shared with other users of that workspace.\n+1. Select **Save** on the top bar.\n+   \n+1. In the **Save** dialog, give the query a **Name**, using the characters a–z, A–Z, 0-9, space, hyphen, underscore, period, parenthesis, or pipe. \n+   \n+1. Select whether to save the query as a **Query** or a **Function**. Functions are queries that other queries can reference. \n+   \n+   To save a query as a function, provide a **Function Alias**, which is a short name for other queries to use to call this query.\n+   \n+1. Provide a **Category** for **Query explorer** to use for the query.\n+   \n+1. Select **Save**.\n+   \n+   ![Save function](media/get-started-portal/save-function.png)\n \n-## Load queries\n-The Query Explorer icon is at the top-right area. This lists all saved queries by category. It also enables you to mark specific queries as Favorites to quickly find them in the future. Double-click a saved query to add it to the current window.\n+### Load queries\n+To load a saved query, select **Query explorer** at upper right. The **Query explorer** pane opens, listing all queries by category. Expand the categories or enter a query name in the search bar, then select a query to load it into the **Query editor**. You can mark a query as a **Favorite** by selecting the star next to the query name.\n \n ![Query explorer](media/get-started-portal/query-explorer.png)\n \n-## Export and share as link\n-Log Analytics supports several exporting methods:\n+### Export and share queries\n+To export a query, select **Export** on the top bar, and then select **Export to CSV - all columns**, **Export to CSV - displayed columns**, or **Export to Power BI (M query)** from the dropdown list.\n \n-- Excel: Save the results as a CSV file.\n-- Power BI: Export the results to Power BI. See [Import Azure Monitor log data into Power BI](../../azure-monitor/platform/powerbi.md) for details.\n-- Share a link: The query itself can be shared as a link which can then be sent and executed by other users that have access to the same workspace.\n+To share a link to a query, select **Copy link** on the top bar, and then select **Copy link to query**, **Copy query text**, or **Copy query results** to copy to the clipboard. You can send the query link to others who have access to the same workspace.\n \n ## Next steps\n \n-- Learn more about [writing Azure Monitor log queries](get-started-queries.md).\n+Advance to the next tutorial to learn more about writing Azure Monitor log queries.\n+> [!div class=\"nextstepaction\"]\n+> [Write Azure Monitor log queries](get-started-queries.md)"
  },
  {
    "Number": 105567,
    "Title": "SEO OKR: [log-analytics] Lengthy title issue (13)",
    "ClosedAt": "2020-03-19T16:18:35Z",
    "User": "v-thepet",
    "FileName": "articles/azure-monitor/insights/ad-replication-status.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Monitor Active Directory replication status with Azure Monitor | Microsoft Docs\n+title: Monitor Active Directory replication status\n description: The Active Directory Replication Status solution pack regularly monitors your Active Directory environment for any replication failures.\n ms.subservice: logs\n ms.topic: conceptual"
  },
  {
    "Number": 105567,
    "Title": "SEO OKR: [log-analytics] Lengthy title issue (13)",
    "ClosedAt": "2020-03-19T16:18:35Z",
    "User": "v-thepet",
    "FileName": "articles/azure-monitor/insights/network-performance-monitor-performance-monitor.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Performance Monitor feature in Network Performance Monitor solution in Azure Log Analytics | Microsoft Docs\n+title: Performance Monitor in Network Performance Monitor\n description: The Performance Monitor capability in Network Performance Monitor helps you monitor network connectivity across various points in your network. You can monitor cloud deployments and on-premises locations, multiple data centers and branch offices, and mission-critical multitier applications or microservices.\n ms.subservice: logs\n ms.topic: conceptual"
  },
  {
    "Number": 105567,
    "Title": "SEO OKR: [log-analytics] Lengthy title issue (13)",
    "ClosedAt": "2020-03-19T16:18:35Z",
    "User": "v-thepet",
    "FileName": "articles/azure-monitor/insights/scom-assessment.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Optimize your System Center Operations Manager environment with Azure Log Analytics | Microsoft Docs\n+title: Assess System Center Operations Manager with Azure Monitor\n description: You can use the System Center Operations Manager Health Check solution to assess the risk and health of your environments on a regular interval.\n ms.subservice: logs\n ms.topic: conceptual"
  },
  {
    "Number": 105567,
    "Title": "SEO OKR: [log-analytics] Lengthy title issue (13)",
    "ClosedAt": "2020-03-19T16:18:35Z",
    "User": "v-thepet",
    "FileName": "articles/azure-monitor/platform/activity-log-collect-tenants.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Collect Azure Activity logs into a Log Analytics workspace across Azure tenants | Microsoft Docs\n+title: Cross-tenant Azure activity logs in Azure Monitor\n description: Use Event Hubs and Logic Apps to collect data from the Azure Activity Log and send it to a Log Analytics workspace in Azure Monitor in a different tenant.\n ms.subservice: logs\n ms.topic: conceptual"
  },
  {
    "Number": 105567,
    "Title": "SEO OKR: [log-analytics] Lengthy title issue (13)",
    "ClosedAt": "2020-03-19T16:18:35Z",
    "User": "v-thepet",
    "FileName": "articles/azure-monitor/platform/activity-log-collect.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Collect and analyze Azure activity logs in Log Analytics workspace | Microsoft Docs\n+title: Collect Azure activity log in Log Analytics workspace\n description: Collect the Azure Activity Log in Azure Monitor Logs and use the monitoring solution to analyze and search the Azure activity log across all your Azure subscriptions.\n ms.subservice: logs\n ms.topic: conceptual"
  },
  {
    "Number": 105567,
    "Title": "SEO OKR: [log-analytics] Lengthy title issue (13)",
    "ClosedAt": "2020-03-19T16:18:35Z",
    "User": "v-thepet",
    "FileName": "articles/azure-monitor/platform/create-pipeline-datacollector-api.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Create a data pipeline with the Azure Monitor Data Collector API | Microsoft Docs\n+title: Use Data Collector API to create a data pipeline\n description: You can use the Azure Monitor HTTP Data Collector API to add POST JSON data to the Log Analytics workspace from any client that can call the REST API. This article describes how to upload data stored in files in an automated way.\n ms.subservice: logs\n ms.topic: conceptual"
  },
  {
    "Number": 105567,
    "Title": "SEO OKR: [log-analytics] Lengthy title issue (13)",
    "ClosedAt": "2020-03-19T16:18:35Z",
    "User": "v-thepet",
    "FileName": "articles/azure-monitor/platform/itsmc-connections.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Supported connections with IT Service Management Connector in Azure Log Analytics | Microsoft Docs\n+title: IT Service Management Connector in Azure Monitor\n description: This article provides information about how to connect your ITSM products/services with the IT Service Management Connector (ITSMC) in Azure Monitor to centrally monitor and manage the ITSM work items.\n ms.subservice: logs\n ms.topic: conceptual"
  },
  {
    "Number": 105567,
    "Title": "SEO OKR: [log-analytics] Lengthy title issue (13)",
    "ClosedAt": "2020-03-19T16:18:35Z",
    "User": "v-thepet",
    "FileName": "articles/azure-monitor/platform/itsmc-service-manager-script.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Automated script to create Service Manager Web app to connect with IT Service Management Connector in Azure | Microsoft Docs\n+title: Create web app for Service Management Connector\n description: Create a Service Manager Web app using an automated script to connect with IT Service Management Connector in Azure, and centrally monitor and manage the ITSM work items.\n ms.subservice: logs\n ms.topic: conceptual"
  },
  {
    "Number": 105567,
    "Title": "SEO OKR: [log-analytics] Lengthy title issue (13)",
    "ClosedAt": "2020-03-19T16:18:35Z",
    "User": "v-thepet",
    "FileName": "articles/azure-monitor/platform/powershell-workspace-configuration.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Use PowerShell to Create and Configure a Log Analytics Workspace | Microsoft Docs\n+title: Create & configure Log Analytics with PowerShell\n description: Log Analytics workspaces in Azure Monitor store data from servers in your on-premises or cloud infrastructure. You can collect machine data from Azure storage when generated by Azure diagnostics.\n ms.subservice: logs\n ms.topic: conceptual"
  },
  {
    "Number": 105567,
    "Title": "SEO OKR: [log-analytics] Lengthy title issue (13)",
    "ClosedAt": "2020-03-19T16:18:35Z",
    "User": "v-thepet",
    "FileName": "articles/azure-monitor/platform/template-workspace-configuration.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Use Azure Resource Manager templates to Create and Configure a Log Analytics Workspace | Microsoft Docs\n+title: Azure Resource Manager template for Log Analytics workspace\n description: You can use Azure Resource Manager templates to create and configure Log Analytics workspaces.\n ms.subservice: logs\n ms.topic: conceptual"
  },
  {
    "Number": 105567,
    "Title": "SEO OKR: [log-analytics] Lengthy title issue (13)",
    "ClosedAt": "2020-03-19T16:18:35Z",
    "User": "v-thepet",
    "FileName": "articles/azure-monitor/platform/vmext-troubleshoot.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Troubleshoot Azure Log Analytics VM Extension in Azure Monitor | Microsoft Docs\n+title: Troubleshoot Azure Log Analytics VM Extension\n description: Describe the symptoms, causes, and resolution for the most common issues with the Log Analytics VM extension for Windows and Linux Azure VMs.\n ms.subservice: logs\n ms.topic: conceptual"
  },
  {
    "Number": 105567,
    "Title": "SEO OKR: [log-analytics] Lengthy title issue (13)",
    "ClosedAt": "2020-03-19T16:18:35Z",
    "User": "v-thepet",
    "FileName": "articles/azure-monitor/scripts/powershell-sample-create-workspace.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Azure PowerShell Script Sample -  Create a Log Analytics workspace| Microsoft Docs\n+title: Create Log Analytics workspace - Azure PowerShell\n description: Azure PowerShell Script Sample -  Create a Log Analytics workspace to\n ms.subservice: logs\n ms.topic: sample"
  },
  {
    "Number": 108295,
    "Title": "added alert rule",
    "ClosedAt": "2020-03-19T10:57:43Z",
    "User": "nolavime",
    "FileName": "articles/azure-monitor/platform/alerts-activity-log.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -123,7 +123,7 @@ A simple analogy for understanding conditions on which alert rules can be create\n \n \n ## Azure Resource Manager template\n-To create an activity log rule by using an Azure Resource Manager template, you create a resource of the type `microsoft.insights/activityLogAlerts`. Then you fill in all related properties. Here's a template that creates an activity log rule:\n+To create an activity log alert rule by using an Azure Resource Manager template, you create a resource of the type `microsoft.insights/activityLogAlerts`. Then you fill in all related properties. Here's a template that creates an activity log alert  rule:\n \n ```json\n {"
  },
  {
    "Number": 107372,
    "Title": "[AzureADDS] Security audit events with Azure Monitor Workbooks",
    "ClosedAt": "2020-03-18T23:44:30Z",
    "User": "iainfoulds",
    "FileName": "articles/active-directory-domain-services/use-azure-monitor-workbooks.md",
    "Addition": 116,
    "Delections": 0,
    "Changes": 116,
    "Patch": "@@ -0,0 +1,116 @@\n+---\n+title: Use Azure Monitor Workbooks with Azure AD Domain Services | Microsoft Docs\n+description: Learn how to use Azure Monitor Workbooks to review security audits and understand issues in an Azure Active Directory Domain Services managed domain.\n+author: iainfoulds\n+manager: daveba\n+\n+ms.service: active-directory\n+ms.subservice: domain-services\n+ms.workload: identity\n+ms.topic: conceptual\n+ms.date: 03/18/2020\n+ms.author: iainfou\n+\n+---\n+# Review security audit events in Azure AD Domain Services using Azure Monitor Workbooks\n+\n+To help you understand the state of your Azure Active Directory Domain Services (Azure AD DS) managed domain, you can enable security audit events. These security audit events can then be reviewed using Azure Monitor Workbooks that combine text, analytics queries, and parameters into rich interactive reports. Azure AD DS includes workbook templates for security overview and account activity that let you dig into audit events and manage your environment.\n+\n+This article shows you how to use Azure Monitor Workbooks to review security audit events in Azure AD DS.\n+\n+## Before you begin\n+\n+To complete this article, you need the following resources and privileges:\n+\n+* An active Azure subscription.\n+    * If you don't have an Azure subscription, [create an account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).\n+* An Azure Active Directory tenant associated with your subscription, either synchronized with an on-premises directory or a cloud-only directory.\n+    * If needed, [create an Azure Active Directory tenant][create-azure-ad-tenant] or [associate an Azure subscription with your account][associate-azure-ad-tenant].\n+* An Azure Active Directory Domain Services managed domain enabled and configured in your Azure AD tenant.\n+    * If needed, complete the tutorial to [create and configure an Azure Active Directory Domain Services instance][create-azure-ad-ds-instance].\n+* Security audit events enabled for your Azure Active Directory Domain Services managed domain that stream data to a Log Analytics workspace.\n+    * If needed, [enable security audits for Azure Active Directory Domain Services][enable-security-audits].\n+\n+## Azure Monitor Workbooks overview\n+\n+When security audit events are turned on in Azure AD DS, it can be hard to analyze and identify issues in the managed domain. Azure Monitor lets you aggregate these security audit events and query the data. With Azure Monitor Workbooks, you can visualize this data to make it quicker and easier to identify issues.\n+\n+Workbook templates are curated reports that are designed for flexible reuse by multiple users and teams. When you open a workbook template, the data from your Azure Monitor environment is loaded. You can use templates without an impact on other users in your organization, and can save your own workbooks based on the template.\n+\n+Azure AD DS includes the following two workbook templates:\n+\n+* Security overview report\n+* Account activity report\n+\n+For more information about how to edit and manage workbooks, see [Azure Monitor Workbooks overview](../azure-monitor/platform/workbooks-overview.md).\n+\n+## Use the security overview report workbook\n+\n+To help you better understand usage and identify potential security threats, the security overview report summarizes sign-in data and identifies accounts you might want to check on. You can view events in a particular date range, and drill down into specific sign-in events, such as bad password attempts or where the account was disabled.\n+\n+To access the workbook template for the security overview report, complete the following steps:\n+\n+1. Search for and select **Azure Active Directory Domain Services** in the Azure portal.\n+1. Select your managed domain, such as *aaddscontoso.com*\n+1. From the menu on the left-hand side, choose **Monitoring > Workbooks**\n+\n+    ![Select the Workbooks menu option in the Azure portal](./media/use-azure-monitor-workbooks/select-workbooks-in-azure-portal.png)\n+\n+1. Choose the **Security Overview Report**.\n+1. From the drop-down menus at the top of the workbook, select your Azure subscription and then Azure Monitor workspace. Choose a **Time range**, such as *Last 7 days*.\n+\n+    ![Select the Workbooks menu option in the Azure portal](./media/use-azure-monitor-workbooks/select-query-filters.png)\n+\n+    The **Tile view** and **Chart view** options can also be changed to analyze and visualize the data as desired\n+\n+1. To drill down into a specific event type, select the one of the **Sign-in result** cards such as *Account Locked Out*, as shown in the following example:\n+\n+    ![Example Security Overview Report data visualized in Azure Monitor Workbooks](./media/use-azure-monitor-workbooks/example-security-overview-report.png)\n+\n+1. The lower part of the security overview report below the chart then breaks down the activity type selected. You can filter by usernames involved on the right-hand side, as shown in the following example report:\n+\n+    [![](./media/use-azure-monitor-workbooks/account-lockout-details-cropped.png \"Details of account lockouts in Azure Monitor Workbooks\")](./media/use-azure-monitor-workbooks/account-lockout-details.png#lightbox)\n+\n+## Use the account activity report workbook\n+\n+To help you troubleshoot issues for a specific user account, the account activity report breaks down detailed audit event log information. You can review when a bad username or password was provided during sign-in, and the source of the sign-in attempt.\n+\n+To access the workbook template for the account activity report, complete the following steps:\n+\n+1. Search for and select **Azure Active Directory Domain Services** in the Azure portal.\n+1. Select your managed domain, such as *aaddscontoso.com*\n+1. From the menu on the left-hand side, choose **Monitoring > Workbooks**\n+1. Choose the **Account Activity Report**.\n+1. From the drop-down menus at the top of the workbook, select your Azure subscription and then Azure Monitor workspace. Choose a **Time range**, such as *Last 30 days*, then how you want the **Tile view** to represent the data. You can filter by **Account username**, such as *felix*, as shown in the following example report:\n+\n+    [![](./media/use-azure-monitor-workbooks/account-activity-report-cropped.png \"Account activity report in Azure Monitor Workbooks\")](./media/use-azure-monitor-workbooks/account-activity-report.png#lightbox)\n+\n+    The area below the chart shows individual sign-in events along with information such as the activity result and source workstation. This information can help determine repeated sources of sign-in events that may cause account lockouts or indicate a potential attack.\n+\n+As with the security overview report, you can drill down into the different tiles at the top of the report to visualize and analyze the data as needed.\n+\n+## Save and edit workbooks\n+\n+The two template workbooks provided by Azure AD DS are a good place to start with your own data analysis. If you need to get more granular in the data queries and investigations, you can save your own workbooks and edit the queries.\n+\n+1. To save a copy of one of the workbook templates, select **Edit > Save as > Shared reports**, then provide a name and save it.\n+1. From your own copy of the template, select **Edit** to enter the edit mode. You can choose the blue **Edit** button next to any part of the report and change it.\n+\n+All of the charts and tables in Azure Monitor Workbooks are generated using Kusto queries. For more information on creating your own queries, see [Azure Monitor log queries][azure-monitor-queries] and [Kusto queries tutorial][kusto-queries].\n+\n+## Next steps\n+\n+If you need to adjust password and lockout policies, see [Password and account lockout policies on managed domains][password-policy].\n+\n+For problems with users, learn how to troubleshoot [account sign-in problems][troubleshoot-sign-in] or [account lockout problems][troubleshoot-account-lockout].\n+\n+<!-- INTERNAL LINKS -->\n+[create-azure-ad-tenant]: ../active-directory/fundamentals/sign-up-organization.md\n+[associate-azure-ad-tenant]: ../active-directory/fundamentals/active-directory-how-subscriptions-associated-directory.md\n+[create-azure-ad-ds-instance]: tutorial-create-instance.md\n+[enable-security-audits]: security-audit-events.md\n+[password-policy]: password-policy.md\n+[troubleshoot-sign-in]: troubleshoot-sign-in.md\n+[troubleshoot-account-lockout]: troubleshoot-account-lockout.md\n+[azure-monitor-queries]: ../azure-monitor/log-query/query-language.md\n+[kusto-queries]: https://docs.microsoft.com/azure/kusto/query/tutorial?pivots=azuredataexplorer"
  },
  {
    "Number": 108219,
    "Title": "Azure Monitor overview video",
    "ClosedAt": "2020-03-18T21:46:18Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/overview.md",
    "Addition": 3,
    "Delections": 0,
    "Changes": 3,
    "Patch": "@@ -21,6 +21,9 @@ Just a few examples of what you can do with Azure Monitor include:\n - Support operations at scale with [smart alerts](platform/alerts-smartgroups-overview.md) and [automated actions](platform/alerts-action-rules.md).\n - Create visualizations with Azure [dashboards](learn/tutorial-logs-dashboards.md) and [workbooks](app/usage-workbooks.md).\n \n+> [!VIDEO https://www.microsoft.com/en-us/videoplayer/embed/RE4qXeL]\n+\n+\n [!INCLUDE [azure-lighthouse-supported-service](../../includes/azure-lighthouse-supported-service.md)]\n \n ## Overview"
  },
  {
    "Number": 107971,
    "Title": "Azure Monitor visualizations deprecate views",
    "ClosedAt": "2020-03-18T19:55:22Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/visualizations.md",
    "Addition": 43,
    "Delections": 40,
    "Changes": 83,
    "Patch": "@@ -5,7 +5,7 @@ ms.subservice:\n ms.topic: conceptual\n author: bwren\n ms.author: bwren\n-ms.date: 11/24/2018\n+ms.date: 03/17/2020\n \n ---\n \n@@ -14,7 +14,23 @@ This article provides a summary of the available methods to visualize log and me\n \n Visualizations such as charts and graphs can help you analyze your monitoring data to drill-down on issues and identify patterns. Depending on the tool you use, you may also have the option to share visualizations with other users inside and outside of your organization.\n \n-[!INCLUDE [azure-monitor-log-analytics-rebrand](../../includes/azure-monitor-log-analytics-rebrand.md)]\n+## Workbooks\n+[Workbooks](../azure-monitor/app/usage-workbooks.md) are interactive documents that provide deep insights into your data, investigation, and collaboration inside the team. Specific examples where workbooks are useful are troubleshooting guides and incident postmortem.\n+\n+![Workbook](media/visualizations/workbook.png)\n+\n+### Advantages\n+- Supports both metrics and logs.\n+- Supports parameters  enabling interactive reports where selecting an element in a table will dynamically update associated charts and visualizations.\n+- Document-like flow.\n+- Option for personal or shared workbooks.\n+- Easy, collaborative-friendly authoring experience.\n+- Templates support public GitHub-based template gallery.\n+\n+### Limitations\n+- No automatic refresh.\n+- No dense layout like dashboards, which make workbooks less useful as a single pane of glass. Intended more for providing deeper insights.\n+\n \n ## Azure Dashboards\n [Azure dashboards](../azure-portal/azure-portal-dashboards.md) are the primary dashboarding technology for Azure. They're particularly useful in providing single pane of glass over your Azure infrastructure and services allowing you to quickly identify important issues.\n@@ -40,44 +56,6 @@ Visualizations such as charts and graphs can help you analyze your monitoring da\n - No interactivity with dashboard data.\n - Limited contextual drill-down.\n \n-## Azure Monitor Views\n-[Views in Azure Monitor](platform/view-designer.md)  allow you to create custom visualizations with log data. They are used by [monitoring solutions](insights/solutions.md) to present the data they collect.\n-\n-![View](media/visualizations/view.png)\n-\n-### Advantages\n-- Rich visualizations for log data.\n-- Export and import views to transfer them to other resource groups and subscriptions.\n-- Integrates into Azure Monitor management model with workspaces and monitoring solutions.\n-- [Filters](platform/view-designer-filters.md) for custom parameters.\n-- Interactive, supports multi-level drill-in (view that drills into another view)\n-\n-### Limitations\n-- Supports logs but not metrics.\n-- No personal views. Available to all users with access to the workspace.\n-- No automatic refresh.\n-- Limited layout options.\n-- No support for querying across multiple workspaces or Application Insights applications.\n-- Queries are limited in response size to 8MB and query execution time of 110 seconds.\n-\n-\n-## Workbooks\n-[Workbooks](../azure-monitor/app/usage-workbooks.md) are interactive documents that provide deep insights into your data, investigation, and collaboration inside the team. Specific examples where workbooks are useful are troubleshooting guides and incident postmortem.\n-\n-![Workbook](media/visualizations/workbook.png)\n-\n-### Advantages\n-- Supports both metrics and logs.\n-- Supports parameters  enabling interactive reports where selecting an element in a table will dynamically update associated charts and visualizations.\n-- Document-like flow.\n-- Option for personal or shared workbooks.\n-- Easy, collaborative-friendly authoring experience.\n-- Templates support public GitHub-based template gallery.\n-\n-### Limitations\n-- No automatic refresh.\n-- No dense layout like dashboards, which make workbooks less useful as a single pane of glass. Intended more for providing deeper insights.\n-\n \n ## Power BI\n [Power BI](https://powerbi.microsoft.com/documentation/powerbi-service-get-started/) is particularly useful for creating business-centric dashboards and reports, as well as reports analyzing long-term KPI trends. You can [import the results of a log query](platform/powerbi.md) into a Power BI dataset so you can take advantage of its features such as combining data from different sources and sharing reports on the web and mobile devices.\n@@ -126,6 +104,31 @@ You can access data in log and metric data in Azure Monitor through their API us\n - Significant engineering effort required.\n \n \n+## Azure Monitor Views\n+\n+> [!IMPORTANT]\n+> Views are in the process of being deprecated. See [Azure Monitor view designer to workbooks transition guide](platform/view-designer-conversion-overview.md) for guidance on converting views to workbooks.\n+\n+[Views in Azure Monitor](platform/view-designer.md)  allow you to create custom visualizations with log data. They are used by [monitoring solutions](insights/solutions.md) to present the data they collect.\n+\n+\n+![View](media/visualizations/view.png)\n+\n+### Advantages\n+- Rich visualizations for log data.\n+- Export and import views to transfer them to other resource groups and subscriptions.\n+- Integrates into Azure Monitor management model with workspaces and monitoring solutions.\n+- [Filters](platform/view-designer-filters.md) for custom parameters.\n+- Interactive, supports multi-level drill-in (view that drills into another view)\n+\n+### Limitations\n+- Supports logs but not metrics.\n+- No personal views. Available to all users with access to the workspace.\n+- No automatic refresh.\n+- Limited layout options.\n+- No support for querying across multiple workspaces or Application Insights applications.\n+- Queries are limited in response size to 8MB and query execution time of 110 seconds.\n+\n ## Next steps\n - Learn about the [data collected by Azure Monitor](platform/data-platform.md).\n - Learn about [Azure dashboards](../azure-portal/azure-portal-dashboards.md)."
  },
  {
    "Number": 108169,
    "Title": "Update OpenCensus Python doc to include exception info",
    "ClosedAt": "2020-03-18T17:31:56Z",
    "User": "lzchen",
    "FileName": "articles/azure-monitor/app/opencensus-python.md",
    "Addition": 23,
    "Delections": 2,
    "Changes": 25,
    "Patch": "@@ -268,7 +268,8 @@ Below is a list of standard metrics that are currently sent:\n - Process CPU Usage (percentage)\n - Process Private Bytes (bytes)\n \n-You should be able to see these metrics in `performanceCounters`. Incoming request rate would be under `customMetrics`.\n+You should be able to see these metrics in `performanceCounters`. Incoming request rate would be under `customMetrics`. For more information, see [performance counters](https://docs.microsoft.com/azure/azure-monitor/app/performance-counters).\n+\n #### Modify telemetry\n \n For details on how to modify tracked telemetry before it is sent to Azure Monitor, see OpenCensus Python [telemetry processors](https://docs.microsoft.com/azure/azure-monitor/app/api-filtering-sampling#opencensus-python-telemetry-processors).\n@@ -385,13 +386,33 @@ For details on how to modify tracked telemetry before it is sent to Azure Monito\n \n     # Use properties in logging statements\n     logger.warning('action', extra=properties)\n+    ```\n+\n+#### Sending exceptions\n+\n+OpenCensus Python does not automatically track and send `exception` telemetry. They are sent through the `AzureLogHandler` by using exceptions through the Python logging library. You can add custom properties just like with normal logging.\n+\n+    ```python\n+    import logging\n+    \n+    from opencensus.ext.azure.log_exporter import AzureLogHandler\n+    \n+    logger = logging.getLogger(__name__)\n+    # TODO: replace the all-zero GUID with your instrumentation key.\n+    logger.addHandler(AzureLogHandler(\n+        connection_string='InstrumentationKey=00000000-0000-0000-0000-000000000000')\n+    )\n+\n+    properties = {'custom_dimensions': {'key_1': 'value_1', 'key_2': 'value_2'}}\n \n     # Use properties in exception logs\n     try:\n         result = 1 / 0  # generate a ZeroDivisionError\n     except Exception:\n-    logger.exception('Captured an exception.', extra=properties)\n+        logger.exception('Captured an exception.', extra=properties)\n     ```\n+Since you must log exceptions explicitly, it is up to the user in how they want to log unhandled exceptions. OpenCensus does not place restrictions in how a user wants to do this, as long as they explicitly log an exception telemetry.\n+\n #### Sampling\n \n For information on sampling in OpenCensus, take a look at [sampling in OpenCensus](sampling.md#configuring-fixed-rate-sampling-for-opencensus-python-applications)."
  },
  {
    "Number": 108135,
    "Title": "updated changes in the terms",
    "ClosedAt": "2020-03-18T13:44:13Z",
    "User": "nolavime",
    "FileName": "articles/azure-monitor/platform/alerts-activity-log.md",
    "Addition": 9,
    "Delections": 9,
    "Changes": 18,
    "Patch": "@@ -123,7 +123,7 @@ A simple analogy for understanding conditions on which alert rules can be create\n \n \n ## Azure Resource Manager template\n-To create an activity log alert by using an Azure Resource Manager template, you create a resource of the type `microsoft.insights/activityLogAlerts`. Then you fill in all related properties. Here's a template that creates an activity log alert:\n+To create an activity log rule by using an Azure Resource Manager template, you create a resource of the type `microsoft.insights/activityLogAlerts`. Then you fill in all related properties. Here's a template that creates an activity log rule:\n \n ```json\n {\n@@ -192,16 +192,16 @@ To create an activity log alert by using an Azure Resource Manager template, you\n ```\n The previous sample JSON can be saved as, for example, sampleActivityLogAlert.json for the purpose of this walk-through and can be deployed by using [Azure Resource Manager in the Azure portal](../../azure-resource-manager/templates/deploy-portal.md).\n \n-The following fields are the fields that you can use in the ARM template for the conditions fields:\n+The following fields are the fields that you can use in the Azure Resource Manager template for the conditions fields:\n Notice that “Resource Health”, “Advisor” and “Service Health” have extra properties fields for their special fields. In the beginning.\n-1. resourceId:\tThe resource ID of the impacted resource that the alert should be generated on.\n-2. category: The category of the event in the activity log.\tFor example: Administrative, ServiceHealth, ResourceHealth, Autoscale, Security, Recommendation, Policy.\n-3. caller: The email address or Azure Active Directory identifier of the user who performed the operation.\n-4. level: Level of the event in the activity log that the alert should be generated on. For example: Critical, Error, Warning, Informational, Verbose.\n-5. operationName: The name of the operation of the event in the activity log. For example: Microsoft.Resources/deployments/write\n-6. resourceGroup: Name of the resource group for the impacted resource in the activity log.\n+1. resourceId:\tThe resource ID of the impacted resource in the activity log event that the alert should be generated on.\n+2. category: The category of in the activity log event. For example: Administrative, ServiceHealth, ResourceHealth, Autoscale, Security, Recommendation, Policy.\n+3. caller: The email address or Azure Active Directory identifier of the user who performed the operation of the activity log event.\n+4. level: Level of the activity in the activity log event that the alert should be generated on. For example: Critical, Error, Warning, Informational, Verbose.\n+5. operationName: The name of the operation in the activity log event. For example: Microsoft.Resources/deployments/write\n+6. resourceGroup: Name of the resource group for the impacted resource in the activity log event.\n 7. resourceProvider: [Azure resource providers and types explanation](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Fazure-resource-manager%2Fmanagement%2Fresource-providers-and-types&data=02%7C01%7CNoga.Lavi%40microsoft.com%7C90b7c2308c0647c0347908d7c9a2918d%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637199572373543634&sdata=4RjpTkO5jsdOgPdt%2F%2FDOlYjIFE2%2B%2BuoHq5%2F7lHpCwQw%3D&reserved=0). For a list that maps resource providers to Azure services, see [Resource providers for Azure services](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Fazure-resource-manager%2Fmanagement%2Fazure-services-resource-providers&data=02%7C01%7CNoga.Lavi%40microsoft.com%7C90b7c2308c0647c0347908d7c9a2918d%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637199572373553639&sdata=0ZgJPK7BYuJsRifBKFytqphMOxMrkfkEwDqgVH1g8lw%3D&reserved=0).\n-8. status: String describing the status of the operation in the activity log. For example: Started, In Progress, Succeeded, Failed, Active, Resolved\n+8. status: String describing the status of the operation in the activity event. For example: Started, In Progress, Succeeded, Failed, Active, Resolved\n 9. subStatus: Usually the HTTP status code of the corresponding REST call, but can also include other strings describing a substatus.\tFor example: OK (HTTP Status Code: 200), Created (HTTP Status Code: 201), Accepted (HTTP Status Code: 202), No Content (HTTP Status Code: 204), Bad Request (HTTP Status Code: 400), Not Found (HTTP Status Code: 404), Conflict (HTTP Status Code: 409), Internal Server Error (HTTP Status Code: 500), Service Unavailable (HTTP Status Code: 503), Gateway Timeout (HTTP Status Code: 504).\n 10. resourceType: The type of the resource that was affected by the event. For example: Microsoft.Resources/deployments\n "
  },
  {
    "Number": 108119,
    "Title": "(AzureCXP) Update typo of workspace",
    "ClosedAt": "2020-03-18T05:28:27Z",
    "User": "BharathNimmala-MSFT",
    "FileName": "articles/azure-monitor/platform/delete-workspace.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -55,7 +55,7 @@ PS C:\\>Remove-AzOperationalInsightsWorkspace -ResourceGroupName \"resource-group-\n ```\n \n ## Permanent workspace delete\n-The soft-delete method may not fit in some scenarios such as development and testing, where you need to repeat a deployment with the same settings and workspace name. In such cases you can permanently delete your workspace and “override” the soft-delete period. The permanent workspace delete operation releases the workplace name and you can create a new workspace using the same name.\n+The soft-delete method may not fit in some scenarios such as development and testing, where you need to repeat a deployment with the same settings and workspace name. In such cases you can permanently delete your workspace and “override” the soft-delete period. The permanent workspace delete operation releases the workspace name and you can create a new workspace using the same name.\n \n \n > [!IMPORTANT]"
  },
  {
    "Number": 104438,
    "Title": "Azure Monitor virtual machine scenario",
    "ClosedAt": "2020-03-17T23:25:29Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/monitor-vm-azure.md",
    "Addition": 243,
    "Delections": 0,
    "Changes": 243,
    "Patch": "@@ -0,0 +1,243 @@\n+---\n+title: Monitor Azure virtual machines with Azure Monitor\n+description: Describes how to collect and analyze monitoring data from virtual machines in Azure using Azure Monitor.\n+ms.service:  azure-monitor\n+ms.subservice: logs\n+ms.topic: conceptual\n+author: bwren\n+ms.author: bwren\n+ms.date: 03/17/2020\n+\n+---\n+\n+# Monitoring Azure virtual machines with Azure Monitor\n+This article describes how to use Azure Monitor to collect and analyze monitoring data from Azure virtual machines to maintain their health. Virtual machines can be monitored for availability and performance with Azure Monitor like any [other Azure resource](monitor-azure-resource.md), but they're unique from other resources since you also need to monitor the guest operating and system and the workloads that run in it. \n+\n+> [!NOTE]\n+> This article provides a complete overview of the concepts and options for monitoring virtual machines in Azure Monitor. To start monitoring your virtual machines quickly without focusing on the underlying concepts, see [Quickstart: Monitor an Azure virtual machine with Azure Monitor](../learn/quick-monitor-azure-vm.md).\n+\n+\n+## Differences from other Azure resources\n+[Monitoring Azure resources with Azure Monitor](monitor-azure-resource.md) describes the monitoring data generated by Azure resources and how you can use the features of Azure Monitor to analyze and alert on this data. You can collect and act on the same monitoring data from Azure virtual machines with the following differences:\n+\n+- [Platform metrics](../platform/data-platform-metrics.md) are collected automatically for virtual machines but only for the [virtual machine host](#monitoring-data). You need an agent to collect performance data from the guest operating system. \n+- Virtual machines don't generate [resource logs](../platform/platform-logs-overview.md) which provide insight into operations that were performed within an Azure resource. You use an agent to collect log data from the guest operating system.\n+- You can create [diagnostic settings](../platform/diagnostic-settings.md) for a virtual machine to send platform metrics to other destinations such as storage and event hubs, but you can't configure these diagnostic settings in the Azure portal. \n+\n+## Monitoring data\n+Virtual machines in Azure in Azure generate [logs](../platform/data-platform-logs.md) and [metrics](../platform/data-platform-metrics.md) shown the following diagram.\n+\n+![Overview](media/monitor-vm-azure/logs-metrics.png)\n+\n+\n+### Virtual machine host\n+Virtual machines in Azure generate the following data for the virtual machine host the same as other Azure resources as described in [Monitoring data](monitor-azure-resource.md#monitoring-data).\n+\n+- [Platform metrics](../platform/data-platform-metrics.md) - Numerical values that are automatically collected at regular intervals and describe some aspect of a resource at a particular time. Platform metrics are collected for the virtual machine host, but you require the diagnostics extension to collect metrics for the guest operating system.\n+- [Activity log](../platform/platform-logs-overview.md) - Provides insight into the operations on each Azure resource in the subscription from the outside (the management plane). For a virtual machine, this includes such information as when it was started and any configuration changes.\n+\n+\n+### Guest operating system\n+To collect data from the guest operating system of a virtual machine, you require an agent, which runs locally on each virtual machine and sends data to Azure Monitor. Multiple agents are available for Azure Monitor with each collecting different data and writing data to different locations. Get a detailed comparison of the different agents at [Overview of the Azure Monitor agents](../platform/agents-overview.md). \n+\n+- [Log Analytics agent](../platform/agents-overview.md#log-analytics-agent) - Available for virtual machines in Azure, other cloud environments, and on-premises. Collects data to Azure Monitor Logs. Supports Azure Monitor for VMs and monitoring solutions. This is the same agent used for System Center Operations Manager.\n+- [Dependency agent](../platform/agents-overview.md#dependency-agent) - Collects data about the processes running on the virtual machine and their dependencies. Relies on the Log Analytics agent to transmit data into Azure and supports Azure Monitor for VMs, Service Map, and Wire Data 2.0 solutions.\n+- [Azure Diagnostic extension](../platform/agents-overview.md#azure-diagnostics-extension) - Available for Azure Monitor virtual machines only. Can collect data to multiple locations but primarily used to collect guest performance data into Azure Monitor Metrics for Windows virtual machines.\n+- [Telegraf agent](../platform/collect-custom-metrics-linux-telegraf.md) - Collect performance data from Linux VMs into Azure Monitor Metrics.\n+\n+\n+## Configuration requirements\n+To enable all features of Azure Monitor for monitoring a virtual machine, you need to collect monitoring data from the virtual machine host and guest operating system to both [Azure Monitor Metrics](../platform/data-platform-logs.md) and [Azure Monitor Logs](../platform/data-platform-logs.md). The following table lists the configuration that must be performed to enable this collection. You may choose to not perform all of these steps depending on your particular requirements.\n+\n+| Configuration step | Actions completed | Features enabled |\n+|:---|:---|:---|\n+| No configuration | - Host platform metrics collected to Metrics.<br>- Activity log collected. | - Metrics explorer for host.<br>- Metrics alerts for host.<br>- Activity log alerts. |\n+| [Enable Azure Monitor for VMs](#enable-azure-monitor-for-vms) | - Log Analytics agent installed.<br>- Dependency agent installed.<br>- Guest performance data collected to Logs.<br>- Process and dependency details collected to Logs. | - Performance charts and workbooks for guest performance data.<br>- Log queries for guest performance data.<br>- Log alerts for guest performance data.<br>- Dependency map. |\n+| [Install the diagnostics extension and telegraf agent](#enable-diagnostics-extension-and-telegraf-agent) | - Guest performance data collected to Metrics. | - Metrics explorer for guest.<br>- Metrics alerts for guest.  |\n+| [Configure Log Analytics workspace](#configure-log-analytics-workspace) | - Events collected from guest. | - Log queries for guest events.<br>- Log alerts for guest events. |\n+| [Create diagnostic setting for virtual machine](#collect-platform-metrics-and-activity-log) | - Platform metrics collected to Logs.<br>- Activity log collected to Logs. | - Loq queries for host metrics.<br>- Log alerts for host metrics.<br>- Log queries for Activity log.\n+\n+Each of these configuration steps is described in the following sections.\n+\n+### Enable Azure Monitor for VMs\n+[Azure Monitor for VMs](vminsights-overview.md) is an [insight](insights-overview.md) in Azure Monitor that is the primary tool for monitoring virtual machines in Azure Monitor. It provides the following additional value over standard Azure Monitor features.\n+\n+- Simplified onboarding of Log Analytics agent and Dependency agent to enable monitoring of a virtual machine guest operating system and workloads. \n+- Pre-defined trending performance charts and workbooks that allow you to analyze core performance metrics from the virtual machine's guest operating system.\n+- Dependency map that displays processes running on each virtual machine and the interconnected components with other machines and external sources.\n+\n+![Azure Monitor for VMs](media/monitor-vm-azure/vminsights-01.png)\n+\n+![Azure Monitor for VMs](media/monitor-vm-azure/vminsights-02.png)\n+\n+\n+Enable Azure Monitor for VMs from the **Insights** option in the virtual machine menu of the Azure portal. See [Enable Azure Monitor for VMs overview](vminsights-enable-overview.md) for details and other configuration methods.\n+\n+![Enable Azure Monitor for VMs](media/monitor-vm-azure/enable-vminsights.png)\n+\n+### Configure Log Analytics workspace\n+The Log Analytics agent used by Azure Monitor for VMs sends data to a [Log Analytics workspace](../platform/data-platform-logs.md#how-is-data-in-azure-monitor-logs-structured). You can enable the collection of additional performance data, events, and other monitoring data from the agent by configuring the Log Analytics workspace. It only needs to be configured once, since any agent connecting to the workspace will automatically download the configuration and immediately start collecting the defined data. \n+\n+You can access the configuration for the workspace directly from Azure Monitor for VMs by selecting **Workspace configuration** from the **Get Started**. Click on the workspace name to open its menu.\n+\n+![Workspace configuration](media/monitor-vm-azure/workspace-configuration.png)\n+\n+Select **Advanced Settings** from the workspace menu and then **Data** to configure data sources. For Windows agents, select **Windows Event Logs** and add common event logs such as *System* and *Application*. For Linux agents, select **Syslog** and add common facilities such as *kern* and *daemon*. See [Agent data sources in Azure Monitor](../platform/agent-data-sources.md) for a list of the data sources available and details on configuring them. \n+\n+![Configure events](media/monitor-vm-azure/configure-events.png)\n+\n+\n+> [!NOTE]\n+> You can configure performance counters to be collected from the workspace configuration, but this may be redundant with the performance counters collected by Azure Monitor for VMs. Azure Monitor for VMs collects the most common set of counters at a frequency of once per minute. Only configure performance counters to be collected by the workspace if you want to collect counters not already collected by Azure Monitor for VMs or if you have existing queries using performance data.\n+\n+\n+### Enable diagnostics extension and Telegraf agent\n+Azure Monitor for VMs is based on the Log Analytics agent that collects data into a Log Analytics workspace. This supports [multiple features of Azure Monitor](../platform/data-platform-logs.md#what-can-you-do-with-azure-monitor-logs) such as [log queries](../log-query/log-query-overview.md), [log alerts](../platform/alerts-log.md), and [workbooks](../platform/workbooks-overview.md). The [diagnostics extension](../platform/diagnostics-extension-overview.md) collects performance data from the guest operating system of Windows virtual machines to Azure Storage and optionally sends performance data to [Azure Monitor Metrics](../platform/data-platform-metrics.md). For Linux virtual machines, the [Telegraf agent](../platform/collect-custom-metrics-linux-telegraf.md) is required to send data to Azure Metrics.  This enables other features of Azure Monitor such as [metrics explorer](../platform/metrics-getting-started.md) and [metrics alerts](../platform/alerts-metric.md). You can also configure the diagnostics extension to send events and performance data outside of Azure Monitor using Azure Event Hubs.\n+\n+Install the diagnostics extension for a single Windows virtual machine in the Azure portal from the **Diagnostics setting** option in the VM menu. Select the option to enable **Azure Monitor** in the **Sinks** tab. To enable the extension from a template or command line for multiple virtual machines, see [Installation and configuration](../platform/diagnostics-extension-overview.md#installation-and-configuration). Unlike the Log Analytics agent, the data to collect is defined in the configuration for the extension on each virtual machine.\n+\n+![Diagnostic setting](media/monitor-vm-azure/diagnostic-setting.png)\n+\n+See [Install and configure Telegraf](../platform/collect-custom-metrics-linux-telegraf.md#install-and-configure-telegraf) for details on configuring the Telegraf agents on Linux virtual machines. The **Diagnostic setting** menu option is available for Linux, but it will only allow you to send data to Azure storage.\n+\n+### Collect platform metrics and Activity log\n+You can view the platform metrics and Activity log collected for each virtual machine host in the Azure portal. Collect this data into the same Log Analytics workspace as Azure Monitor for VMs to analyze it with the other monitoring data collected for the virtual machine. This collection is configured with a [diagnostic setting](../platform/diagnostic-settings.md). Collect the Activity log with a [diagnostic setting for the subscription](../platform/diagnostic-settings.md#create-diagnostic-settings-in-azure-portal).\n+\n+Collect platform metrics with a diagnostic setting for the virtual machine. Unlike other Azure resources, you cannot create a diagnostic setting for a virtual machine in the Azure portal but must use [another method](../platform/diagnostic-settings.md#create-diagnostic-settings-using-powershell). The following examples show how to collect metrics for a virtual machine using both PowerShell and CLI.\n+\n+```powershell\n+Set-AzDiagnosticSetting -Name vm-diagnostics -ResourceId \"/subscriptions/monitor diagnostic-settings create \\\n+xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/my-resource-group/providers/Microsoft.Compute/virtualMachines/my-vm\" -Enabled $true -MetricCategory AllMetrics -workspaceId \"/subscriptions/monitor diagnostic-settings create \\\n+xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourcegroups/my-resource-group/providers/microsoft.operationalinsights/workspaces/my-workspace\"\n+```\n+\n+```CLI\n+az monitor diagnostic-settings create \\\n+--name VM-Diagnostics \n+--resource /subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/my-resource-group/providers/Microsoft.Compute/virtualMachines/my-vm \\\n+--metrics '[{\"category\": \"AllMetrics\",\"enabled\": true}]' \\\n+--workspace /subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourcegroups/my-resource-group/providers/microsoft.operationalinsights/workspaces/my-workspace\n+\n+```\n+\n+## Monitoring in the Azure portal \n+Once you configure collection of monitoring data for a virtual machine, you have multiple options for accessing it in the Azure portal:\n+\n+- Use the **Azure Monitor** menu to access data from all monitored resources. \n+- Use Azure Monitor for VMs for monitoring sets of virtual machines at scale.\n+- Analyze data for a single virtual machine from its menu in the Azure portal. The table below lists different options for monitoring the virtual machines menu.\n+\n+![Monitoring in the Azure portal](media/monitor-vm-azure/monitor-menu.png)\n+\n+| Menu option | Description |\n+|:---|:---|\n+| Overview | Displays [platform metrics](../platform/data-platform-metrics.md) for the virtual machine host. Click on a graph to work with this data in [metrics explorer](../platform/metrics-getting-started.md). |\n+| Activity log | [Activity log](../platform/activity-log-view.md) entries filtered for the current virtual machine. |\n+| Insights | Opens [Azure Monitor for VMs](../insights/vminsights-overview.md) with the map for the current virtual machine selected. |\n+| Alerts | Views [alerts](../platform/alerts-overview.md) for the current virtual machine.  |\n+| Metrics | Open [metrics explorer](../platform/metrics-getting-started.md) with the scope set to the current virtual machine. |\n+| Diagnostic settings | Enable and configure [diagnostics extension](../platform/diagnostics-extension-overview.md) for the current virtual machine. |\n+| Advisor recommendations | Recommendations for the current virtual machine from [Azure Advisor](/azure/advisor/). |\n+| Logs | Open [Log Analytics](../log-query/log-query-overview.md#what-is-log-analytics) with the [scope](../log-query/scope.md) set to the current virtual machine. |\n+| Connection monitor | Open [Network Watcher Connection Monitor](../../network-watcher/connection-monitor-preview.md) to monitor connections between the current virtual machine and other virtual machines. |\n+\n+\n+## Analyzing metric data\n+You can analyze metrics for virtual machines using metrics explorer by opening **Metrics** from the virtual machine's menu. See [Getting started with Azure Metrics Explorer](../platform/metrics-getting-started.md) for details on using this tool. \n+\n+There are two namespaces used by virtual machines for metrics:\n+\n+| Namespace | Description |\n+|:---|:---|\n+| Virtual Machine Host | Host metrics automatically collected for all Azure virtual machines. Detailed list of metrics at [Microsoft.Compute/virtualMachines](../platform/metrics-supported.md#microsoftcomputevirtualmachines). |\n+| Virtual Machine Guest | Guest operating system metrics collected from virtual machines with diagnostics extension installed and configured to send to Azure Monitor sink. |\n+\n+![Metrics](media/monitor-vm-azure/metrics.png)\n+\n+## Analyzing log data\n+Azure virtual machines will collect the following data to Azure Monitor Logs. \n+\n+Azure Monitor for VMs enables the collection of a predetermined set of performance counters that are written to the *InsightsMetrics* table. This is the same table used by [Azure Monitor for Containers](container-insights-overview.md). \n+\n+| Data source | Requirements | Tables |\n+|:---|:---|:---|\n+| Azure Monitor for VMs | Enable on each virtual machine. | InsightsMetrics<br>VMBoundPort<br>VMComputer<br>VMConnection<br>VMProcess<br>See [How to query logs from Azure Monitor for VMs](vminsights-log-search.md#sample-log-searches) for details. |\n+| Activity log | Diagnostic setting for the subscription. | AzureActivity |\n+| Host metrics | Diagnostic setting for the virtual machine. | AzureMetrics |\n+| Data sources from the guest operating system | Enable Log Analytics agent and configure data sources. | See documentation for each data source. |\n+\n+\n+> [!NOTE]\n+> Performance data collected by the Log Analytics agent writes to the *Perf* table while Azure Monitor for VMs will collect it to the *InsightsMetrics* table. This is the same data, but the tables have a different structure. If you have existing queries based on *Perf*, the will need to be rewritten to use *InsightsMetrics*.\n+\n+\n+## Alerts\n+[Alerts](../platform/alerts-overview.md) in Azure Monitor proactively notify you when important conditions are found in your monitoring data and potentially launch an action such as starting a Logic App or calling a webhook. Alert rules define the logic used to determine when an alert should be created. Azure Monitor collects the data used by alert rules, but you need to create rules to define alerting conditions in your Azure subscription.\n+\n+The following sections describe the types of alert rules and recommendations on when you should use each. This recommendation is based on the functionality and cost of the alert rule type. For details pricing of alerts, see [Azure Monitor pricing](https://azure.microsoft.com/pricing/details/monitor/).\n+\n+\n+### Activity log alert rules\n+[Activity log alert rules](../platform/alerts-activity-log.md) fire when an entry matching particular criteria is created in the activity log. They have no cost so they should be your first choice if the logic you require is in the activity log. \n+\n+The target resource for activity log alerts can be a specific virtual machine, all virtual machines in a resource group, or all virtual machines in a subscription.\n+\n+For example, create an alert if a critical virtual machine is stopped by selecting the *Power Off Virtual Machine* for the signal name.\n+\n+![Activity log alert](media/monitor-vm-azure/activity-log-alert.png)\n+\n+\n+### Metric alert rules\n+[Metric alert rules](../platform/alerts-metric.md) fire when a metric value exceeds a threshold. You can define a specific threshold value or allow Azure Monitor to dynamically determine a threshold based on historical data.  Use metric alerts whenever possible with metric data since they cost less and are more responsive than log alert rules. They are also stateful meaning they will resolve themselves when the metric drops below the threshold.\n+\n+The target resource for activity log alerts can be a specific virtual machine or all virtual machines in a resource group.\n+\n+For example, to create an alert when the processor of a virtual machine exceeds a particular value, create a metric alert rule using *Percentage CPU* as the signal type. Set either a specific threshold value or allow Azure Monitor to set a dynamic threshold. \n+\n+![Metric alert](media/monitor-vm-azure/metric-alert.png)\n+\n+### Log alerts\n+[Log alert rules](../platform/alerts-log.md) fire when the results of a scheduled log query match certain criteria. Log query alerts are the most expensive and least responsive of the alert rules, but they have access to the most diverse data and can perform complex logic that can't be performed by the other alert rules. \n+\n+The target resource for a log query is a Log Analytics workspace. Filter for specific computers in the query.\n+\n+For example, to create an alert that checks if any virtual machines in a particular resource group are offline, use the following query which returns a record for each computer that's missed a heartbeat in the last ten minutes. Use a threshold of 1 which fires if at least one computer has a missed heartbeat.\n+\n+```kusto\n+Heartbeat\n+| where TimeGenerated < ago(10m)\n+| where ResourceGroup == \"my-resource-group\"\n+| summarize max(TimeGenerated) by Computer\n+```\n+\n+![Log alert](media/monitor-vm-azure/log-alert-01.png)\n+\n+To create an alert if an excessive number of failed logons have occurred on any Windows virtual machines in the subscription, use the following query which returns a record for each failed logon event in the past hour. Use a threshold set to the number of failed logons that you'll allow. \n+\n+```kusto\n+Event\n+| where TimeGenerated < ago(1hr)\n+| where EventID == 4625\n+```\n+\n+![Log alert](media/monitor-vm-azure/log-alert-02.png)\n+\n+\n+## System Center Operations Manager\n+System Center Operations Manager (SCOM) provides granular monitoring of workloads on virtual machines. See the [Cloud Monitoring Guide](https://docs.microsoft.com/azure/cloud-adoption-framework/manage/monitor/) for a comparison of monitoring platforms and different strategies for implementation.\n+\n+If you have an existing SCOM environment that you intend to keep using, you can integrate it with Azure Monitor to provide additional functionality. The Log Analytics agent used by Azure Monitor is the same one used for SCOM so that you have monitored virtual machines send data to both. You still need to add the agent to Azure Monitor for VMs and configure the workspace to collect additional data as specified above, but the virtual machines can continue to run their existing management packs in a SCOM environment without modification.\n+\n+Features of Azure Monitor that augment an existing SCOM features include the following:\n+\n+- Use Log Analytics to interactively analyze your log and performance data.\n+- Use log alerts to define alerting conditions across multiple virtual machines and using long term trends that aren't possible using alerts in SCOM.   \n+\n+See [Connect Operations Manager to Azure Monitor](../platform/om-agents.md) for details on connecting your existing SCOM management group to your Log Analytics workspace.\n+\n+\n+## Next steps\n+\n+* [Learn how to analyze data in Azure Monitor logs using log queries.](../log-query/get-started-queries.md)\n+* [Learn about alerts using metrics and logs in Azure Monitor.](../platform/alerts-overview.md)\n+"
  },
  {
    "Number": 104438,
    "Title": "Azure Monitor virtual machine scenario",
    "ClosedAt": "2020-03-17T23:25:29Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/learn/quick-monitor-azure-vm.md",
    "Addition": 105,
    "Delections": 0,
    "Changes": 105,
    "Patch": "@@ -0,0 +1,105 @@\n+---\n+title: Monitor an Azure resource with Azure Monitor\n+description: Learn how to collect and analyze data for an Azure resource in Azure Monitor.\n+ms.service:  azure-monitor\n+ms. subservice: logs\n+ms.topic: quickstart\n+author: bwren\n+ms.author: bwren\n+ms.date: 03/10/2020\n+---\n+\n+# Quickstart: Monitor an Azure virtual machine with Azure Monitor\n+[Azure Monitor](../overview.md) starts collecting data from Azure virtual machines the moment that they're created. In this quickstart, you'll take a brief walkthrough the data that's automatically collected for an Azure VM and how to view it in the Azure portal. You'll then enable [Azure Monitor for VMs](../insights/vminsights-overview.md) for your VM which will enable agents on the VM to collect and analyze data from the guest operating system including processes and their dependencies.\n+\n+This quickstart assumes you have an existing Azure virtual machine. If not you can create a [Windows VM](../../virtual-machines/windows/quick-create-portal.md) or create a [Linux VM](../../virtual-machines/linux/quick-create-cli.md) following our VM quickstarts.\n+\n+For more detailed descriptions of monitoring data collected from Azure resources  see [Monitoring Azure virtual machines with Azure Monitor](../insights/monitor-vm-azure.md).\n+\n+\n+## Complete the Monitor an Azure resource quickstart.\n+Complete [Monitor an Azure resource with Azure Monitor](quick-monitor-azure-resource.md) to view the overview page, activity log, and metrics for a VM in your subscription. Azure VMs collect the same monitoring data as any other Azure resource, but this is only for the host VM. The rest of this quickstart will focus on monitoring the guest operating system and its workloads.\n+\n+\n+## Enable Azure Monitor for VMs\n+While metrics and activity logs will be collected for the host VM, you need an agent and some configuration to collect and analyze monitoring data from the guest operating system and its workloads. Azure Monitor for VMs installs these agents and provides additional powerful features for monitoring your virtual machines.\n+\n+1. Go to the menu for your virtual machine.\n+2. Either click **Go to Insights** from the tile in the **Overview** page, or click on **Insights** from the **Monitoring** menu.\n+\n+    ![Overview page](media/quick-monitor-azure-vm/overview-insights.png)\n+\n+3. If Azure Monitor for VMs has not yet been enabled for the virtual machine, click **Enable**. \n+\n+    ![Enable insights](media/quick-monitor-azure-vm/enable-insights.png)\n+\n+4. If the virtual machine isn't already attached to a Log Analytics workspace, you will be prompted to select an existing workspace or create a new one. Select the default which is a workspace with a unique name in the same region as your virtual machine.\n+\n+    ![Select workspace](media/quick-monitor-azure-vm/select-workspace.png)\n+\n+5. Onboarding will take a few minutes as extensions are enabled and agents are installed on your virtual machine. When it's complete, you get a message that insights have been successfully deployed. Click **Azure Monitor** to open Azure Monitor for VMs.\n+\n+    ![Open Azure Monitor](media/quick-monitor-azure-vm/azure-monitor.png)\n+\n+6. You'll see your VM with any other VMs in your subscription that are onboarded. Select the **Not monitored** tab if you want to view virtual machines in your subscription that aren't onboarded.\n+\n+    ![Get started](media/quick-monitor-azure-vm/get-started.png)\n+\n+\n+## Configure workspace\n+When you create a new Log Analytics workspace, it needs to be configured to collect logs. This configuration only needs to be performed once since configuration is sent to any virtual machines that connect to it.\n+\n+1. Select **Workspace configuration** and then select your workspace.\n+\n+2. Select **Advanced settings**\n+\n+    ![Log Analytics Advance Settings](media/quick-collect-azurevm/log-analytics-advanced-settings-azure-portal.png)\n+\n+### Data collection from Windows VM\n+\n+\n+2. Select **Data**, and then select **Windows Event Logs**.\n+\n+3. Add an event log by typing in the name of the log.  Type **System** and then select the plus sign **+**.\n+\n+4. In the table, check the severities **Error** and **Warning**.\n+\n+5. Select **Save** at the top of the page to save the configuration.\n+\n+### Data collection from Linux VM\n+\n+1. Select **Syslog**.  \n+\n+2. Add an event log by typing in the name of the log.  Type **Syslog** and then select the plus sign **+**.  \n+\n+3. In the table, deselect the severities **Info**, **Notice** and **Debug**. \n+\n+4. Select **Save** at the top of the page to save the configuration.\n+\n+## View data collected\n+\n+7. Click on your virtual machine and then select the **Performance** tab. This shows a select group of performance counters collected from the guest operating system of your VM. Scroll down to view more counters, and move the mouse over a graph to view average and percentiles at different times.\n+\n+    ![Performance](media/quick-monitor-azure-vm/performance.png)\n+\n+9. Select **Map** to open the maps feature which shows the processes running on the virtual machine and their dependencies. Select **Properties** to open the property pane if it isn't already open.\n+\n+    ![Map](media/quick-monitor-azure-vm/map.png)\n+\n+11. Expand the processes for your virtual machine. Select one of the processes to view its details and to highlight its dependencies.\n+\n+    ![Processes](media/quick-monitor-azure-vm/processes.png)\n+\n+12. Select your virtual machine again and then select **Log Events**. \n+\n+    ![Log events](media/quick-monitor-azure-vm/log-events.png)\n+\n+13. You see a list of tables that are stored in the Log Analytics workspace for the virtual machine. This list will be different depending whether you're using a Windows or Linux virtual machine. Click the **Event** table. This includes all events from the Windows event log. Log Analytics opens with a simple query to retrieve event log entries.\n+\n+    ![Log analytics](media/quick-monitor-azure-vm/log-analytics.png)\n+\n+## Next steps\n+In this quickstart, you enabled Azure Monitor for VMs for a virtual machine and configured the Log Analytics workspace to collect events for the guest operating system. To learn how to view and analyze the data, continue to the tutorial.\n+\n+> [!div class=\"nextstepaction\"]\n+> [View or analyze data in Log Analytics](../../azure-monitor/learn/tutorial-viewdata.md)"
  },
  {
    "Number": 108096,
    "Title": "Update ApplicationInsights with recent change",
    "ClosedAt": "2020-03-18T00:23:47Z",
    "User": "cijothomas",
    "FileName": "articles/azure-monitor/app/asp-net-core.md",
    "Addition": 10,
    "Delections": 1,
    "Changes": 11,
    "Patch": "@@ -58,7 +58,7 @@ The [Application Insights SDK for ASP.NET Core](https://nuget.org/packages/Micro\n \n     ```xml\n         <ItemGroup>\n-          <PackageReference Include=\"Microsoft.ApplicationInsights.AspNetCore\" Version=\"2.12.0\" />\n+          <PackageReference Include=\"Microsoft.ApplicationInsights.AspNetCore\" Version=\"2.13.1\" />\n         </ItemGroup>\n     ```\n \n@@ -197,6 +197,12 @@ Full List of settings in `ApplicationInsightsServiceOptions`\n \n |Setting | Description | Default\n |---------------|-------|-------\n+|EnablePerformanceCounterCollectionModule  | Enable/Disable `PerformanceCounterCollectionModule` | true\n+|EnableRequestTrackingTelemetryModule   | Enable/Disable `RequestTrackingTelemetryModule` | true\n+|EnableEventCounterCollectionModule   | Enable/Disable `EventCounterCollectionModule` | true\n+|EnableDependencyTrackingTelemetryModule   | Enable/Disable `DependencyTrackingTelemetryModule` | true\n+|EnableAppServicesHeartbeatTelemetryModule  |  Enable/Disable `AppServicesHeartbeatTelemetryModule` | true\n+|EnableAzureInstanceMetadataTelemetryModule   |  Enable/Disable `AzureInstanceMetadataTelemetryModule` | true\n |EnableQuickPulseMetricStream | Enable/Disable LiveMetrics feature | true\n |EnableAdaptiveSampling | Enable/Disable Adaptive Sampling | true\n |EnableHeartbeat | Enable/Disable Heartbeats feature, which periodically (15-min default) sends a custom metric named 'HeartBeatState' with information about the runtime like .NET Version, Azure Environment information, if applicable, etc. | true\n@@ -313,6 +319,9 @@ public void ConfigureServices(IServiceCollection services)\n }\n ```\n \n+Starting with 2.12.2 version, [`ApplicationInsightsServiceOptions`](#using-applicationinsightsserviceoptions) contains easy\n+option to disable any of the default modules.\n+\n ### Configuring a telemetry channel\n \n The default channel is `ServerTelemetryChannel`. You can override it as the following example shows."
  },
  {
    "Number": 108096,
    "Title": "Update ApplicationInsights with recent change",
    "ClosedAt": "2020-03-18T00:23:47Z",
    "User": "cijothomas",
    "FileName": "articles/azure-monitor/app/ilogger.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -14,9 +14,9 @@ To learn more, see [Logging in ASP.NET Core](https://docs.microsoft.com/aspnet/c\n \n ## ASP.NET Core applications\n \n-ApplicationInsightsLoggerProvider is enabled by default in [Microsoft.ApplicationInsights.AspNet SDK](https://www.nuget.org/packages/Microsoft.ApplicationInsights.AspNetCore) version 2.7.1 (and later) when you turn on regular Application Insights monitoring through either of the standard methods:\n+ApplicationInsightsLoggerProvider is enabled by default in [Microsoft.ApplicationInsights.AspNet SDK](https://www.nuget.org/packages/Microsoft.ApplicationInsights.AspNetCore) version 2.7.1 (and later) when you turn on regular Application Insights monitoring through either of the methods:\n \n-- By calling the **UseApplicationInsights** extension method on IWebHostBuilder\n+- By calling the **UseApplicationInsights** extension method on IWebHostBuilder (Now obsolete)\n - By calling the **AddApplicationInsightsTelemetry** extension method on IServiceCollection\n \n ILogger logs that ApplicationInsightsLoggerProvider captures are subject to the same configuration as any other telemetry that's collected. They have the same set of TelemetryInitializers and TelemetryProcessors, use the same TelemetryChannel, and are correlated and sampled in the same way as other telemetry. If you use version 2.7.1 or later, no action is required to capture ILogger logs."
  },
  {
    "Number": 108096,
    "Title": "Update ApplicationInsights with recent change",
    "ClosedAt": "2020-03-18T00:23:47Z",
    "User": "cijothomas",
    "FileName": "articles/azure-monitor/app/worker-service.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -27,7 +27,7 @@ A valid Application Insights instrumentation key. This key is required to send a\n \n ```xml\n     <ItemGroup>\n-        <PackageReference Include=\"Microsoft.ApplicationInsights.WorkerService\" Version=\"2.12.0\" />\n+        <PackageReference Include=\"Microsoft.ApplicationInsights.WorkerService\" Version=\"2.13.1\" />\n     </ItemGroup>\n ```\n "
  },
  {
    "Number": 108082,
    "Title": "App Service Metrics update",
    "ClosedAt": "2020-03-17T23:14:47Z",
    "User": "btardif",
    "FileName": "articles/app-service/web-sites-monitor.md",
    "Addition": 6,
    "Delections": 2,
    "Changes": 8,
    "Patch": "@@ -14,7 +14,7 @@ ms.custom: seodec18\n [Azure App Service](https://go.microsoft.com/fwlink/?LinkId=529714) provides\n built-in monitoring functionality for web apps, mobile, and API apps in the [Azure portal](https://portal.azure.com).\n \n-In the Azure portal, you can review *quotas* and *metrics* for an app and App Service plan, and set up *alerts* and *autoscaling* that are based metrics.\n+In the Azure portal, you can review *quotas* and *metrics* for an app and App Service plan, and set up *alerts* and *autoscaling* rules based metrics.\n \n ## Understand quotas\n \n@@ -60,13 +60,17 @@ You can increase or remove quotas from your app by upgrading your App Service pl\n > **File System Usage** is a new metric being rolled out globally, no data is expected unless you have been whitelisted for private preview.\n > \n \n+> [!IMPORTANT]\n+> **Average Response Time** will be deprecated to avoid confusion with metric aggregations. Use **Response Time** as a replacement.\n+\n Metrics provide information about the app or the App Service plan's behavior.\n \n For an app, the available metrics are:\n \n | Metric | Description |\n | --- | --- |\n-| **Average Response Time** | The average time taken for the app to serve requests, in seconds. |\n+| **Response Time** | The time taken for the app to serve requests, in seconds. |\n+| **Average Response Time (deprecated)** | The average time taken for the app to serve requests, in seconds. |\n | **Average memory working set** | The average amount of memory used by the app, in megabytes (MiB). |\n | **Connections** | The number of bound sockets existing in the sandbox (w3wp.exe and its child processes).  A bound socket is created by calling bind()/connect() APIs and remains until said socket is closed with CloseHandle()/closesocket(). |\n | **CPU Time** | The amount of CPU consumed by the app, in seconds. For more information about this metric, see [CPU time vs CPU percentage](#cpu-time-vs-cpu-percentage). |"
  },
  {
    "Number": 108082,
    "Title": "App Service Metrics update",
    "ClosedAt": "2020-03-17T23:14:47Z",
    "User": "btardif",
    "FileName": "articles/azure-monitor/platform/metrics-supported.md",
    "Addition": 8,
    "Delections": 1,
    "Changes": 9,
    "Patch": "@@ -2168,6 +2168,12 @@ Azure Monitor provides several ways to interact with metrics, including charting\n \r\n ## Microsoft.Web/sites (excluding functions)\r\n \r\n+> [!NOTE]\r\n+> **File System Usage** is a new metric being rolled out globally, no data is expected unless you have been whitelisted for private preview.\r\n+\r\n+> [!IMPORTANT]\r\n+> **Average Response Time** will be deprecated to avoid confusion with metric aggregations. Use **Response Time** as a replacement.\r\n+\r\n |Metric|Metric Display Name|Unit|Aggregation Type|Description|Dimensions|\r\n |---|---|---|---|---|---|\r\n |CpuTime|CPU Time|Seconds|Total|CPU Time|Instance|\r\n@@ -2185,7 +2191,8 @@ Azure Monitor provides several ways to interact with metrics, including charting\n |Http5xx|Http Server Errors|Count|Total|Http Server Errors|Instance|\r\n |MemoryWorkingSet|Memory working set|Bytes|Average|Memory working set|Instance|\r\n |AverageMemoryWorkingSet|Average memory working set|Bytes|Average|Average memory working set|Instance|\r\n-|AverageResponseTime|Average Response Time|Seconds|Average|Average Response Time|Instance|\r\n+|ResponseTime|Response Time|Seconds|Total|Response Time|Instance|\r\n+|AverageResponseTime|Average Response Time (deprecated)|Seconds|Average|Average Response Time|Instance|\r\n |AppConnections|Connections|Count|Average|Connections|Instance|\r\n |Handles|Handle Count|Count|Average|Handle Count|Instance|\r\n |Threads|Thread Count|Count|Average|Thread Count|Instance|\r"
  },
  {
    "Number": 107956,
    "Title": "update for government support",
    "ClosedAt": "2020-03-17T21:06:39Z",
    "User": "rboucher",
    "FileName": "articles/azure-monitor/platform/alerts-metric-overview.md",
    "Addition": 13,
    "Delections": 4,
    "Changes": 17,
    "Patch": "@@ -1,7 +1,7 @@\n ---\n title: Understand how metric alerts work in Azure Monitor.\n description: Get an overview of what you can do with metric alerts and how they work in Azure Monitor.\n-ms.date: 12/5/2019\n+ms.date: 03/17/2020\n ms.topic: conceptual\n ms.subservice: alerts\n \n@@ -118,11 +118,20 @@ Increasing look-back periods and number of violations can also allow filtering a\n \n ## Monitoring at scale using metric alerts in Azure Monitor\n \n-So far, you have seen how a single metric alert could be used to monitor one or many metric time-series related to a single Azure resource. Many times, you might want the same alert rule applied to many resources. Azure Monitor also supports monitoring multiple resources (of the same type) with one metric alert rule, for resources that exist in the same Azure region. This feature is currently supported only in Azure public cloud and only for virtual machines, SQL server databases, SQL server elastic pools and Data box edge devices. Also, this feature is only available for platform metrics, and isn't supported for custom metrics.\n+So far, you have seen how a single metric alert could be used to monitor one or many metric time-series related to a single Azure resource. Many times, you might want the same alert rule applied to many resources. Azure Monitor also supports monitoring multiple resources (of the same type) with one metric alert rule, for resources that exist in the same Azure region. \n \n-You can specify the scope of monitoring by a single metric alert rule in one of three ways:\n+This feature is currently supported for platform metrics (not custom metrics) for the following services in the following Azure clouds:\n \n-- as a list of virtual machines in one Azure region within a subscription\n+| Service | Public Azure | Government | China |\n+|:--------|:--------|:--------|:--------|\n+| Virtual machines  | **Yes** | No | No |\n+| SQL server databases | **Yes** | **Yes** | No |\n+| SQL server elastic pools | **Yes** | **Yes** | No |\n+| Data box edge devices | **Yes** | **Yes** | No |\n+\n+You can specify the scope of monitoring by a single metric alert rule in one of three ways. For example, with virtual machines you can specify the scope as:  \n+\n+- a list of virtual machines in one Azure region within a subscription\n - all virtual machines (in one Azure region) in one or more resource groups in a subscription\n - all virtual machines (in one Azure region) in one subscription\n "
  },
  {
    "Number": 107810,
    "Title": "added explation in the ARM template fields",
    "ClosedAt": "2020-03-17T19:33:13Z",
    "User": "nolavime",
    "FileName": "articles/azure-monitor/platform/alerts-activity-log.md",
    "Addition": 34,
    "Delections": 0,
    "Changes": 34,
    "Patch": "@@ -192,6 +192,40 @@ To create an activity log alert by using an Azure Resource Manager template, you\n ```\n The previous sample JSON can be saved as, for example, sampleActivityLogAlert.json for the purpose of this walk-through and can be deployed by using [Azure Resource Manager in the Azure portal](../../azure-resource-manager/templates/deploy-portal.md).\n \n+The following fields are the fields that you can use in the ARM template for the conditions fields:\n+Notice that “Resource Health”, “Advisor” and “Service Health” have extra properties fields for their special fields. In the beginning.\n+1. resourceId:\tThe resource ID of the impacted resource that the alert should be generated on.\n+2. category: The category of the event in the activity log.\tFor example: Administrative, ServiceHealth, ResourceHealth, Autoscale, Security, Recommendation, Policy.\n+3. caller: The email address or Azure Active Directory identifier of the user who performed the operation.\n+4. level: Level of the event in the activity log that the alert should be generated on. For example: Critical, Error, Warning, Informational, Verbose.\n+5. operationName: The name of the operation of the event in the activity log. For example: Microsoft.Resources/deployments/write\n+6. resourceGroup: Name of the resource group for the impacted resource in the activity log.\n+7. resourceProvider: [Azure resource providers and types explanation](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Fazure-resource-manager%2Fmanagement%2Fresource-providers-and-types&data=02%7C01%7CNoga.Lavi%40microsoft.com%7C90b7c2308c0647c0347908d7c9a2918d%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637199572373543634&sdata=4RjpTkO5jsdOgPdt%2F%2FDOlYjIFE2%2B%2BuoHq5%2F7lHpCwQw%3D&reserved=0). For a list that maps resource providers to Azure services, see [Resource providers for Azure services](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Fazure-resource-manager%2Fmanagement%2Fazure-services-resource-providers&data=02%7C01%7CNoga.Lavi%40microsoft.com%7C90b7c2308c0647c0347908d7c9a2918d%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637199572373553639&sdata=0ZgJPK7BYuJsRifBKFytqphMOxMrkfkEwDqgVH1g8lw%3D&reserved=0).\n+8. status: String describing the status of the operation in the activity log. For example: Started, In Progress, Succeeded, Failed, Active, Resolved\n+9. subStatus: Usually the HTTP status code of the corresponding REST call, but can also include other strings describing a substatus.\tFor example: OK (HTTP Status Code: 200), Created (HTTP Status Code: 201), Accepted (HTTP Status Code: 202), No Content (HTTP Status Code: 204), Bad Request (HTTP Status Code: 400), Not Found (HTTP Status Code: 404), Conflict (HTTP Status Code: 409), Internal Server Error (HTTP Status Code: 500), Service Unavailable (HTTP Status Code: 503), Gateway Timeout (HTTP Status Code: 504).\n+10. resourceType: The type of the resource that was affected by the event. For example: Microsoft.Resources/deployments\n+\n+For example:\n+\n+```json\n+\"condition\": {\n+          \"allOf\": [\n+            {\n+              \"field\": \"category\",\n+              \"equals\": \"Administrative\"\n+            },\n+            {\n+              \"field\": \"resourceType\",\n+              \"equals\": \"Microsoft.Resources/deployments\"\n+            }\n+          ]\n+        }\n+\n+```\n+More details on the activity log fields you can find [here](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Fazure-monitor%2Fplatform%2Factivity-log-schema&data=02%7C01%7CNoga.Lavi%40microsoft.com%7C90b7c2308c0647c0347908d7c9a2918d%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637199572373563632&sdata=6QXLswwZgUHFXCuF%2FgOSowLzA8iOALVgvL3GMVhkYJY%3D&reserved=0).\n+\n+\n+\n > [!NOTE]\n > It might take up to 5 minutes for the new activity log alert rule to become active.\n "
  },
  {
    "Number": 107984,
    "Title": "cluster per subscription limit update",
    "ClosedAt": "2020-03-17T16:18:15Z",
    "User": "yossi-y",
    "FileName": "articles/azure-monitor/platform/customer-managed-keys.md",
    "Addition": 3,
    "Delections": 3,
    "Changes": 6,
    "Patch": "@@ -405,7 +405,7 @@ All your data is accessible after the key rotation operation including data inge\n - The CMK feature is supported at ADX cluster level and requires a\n     dedicated Azure Monitor ADX cluster\n \n-- The max number of *Cluster* resources per subscription is limited to 5\n+- The max number of *Cluster* resources per subscription is limited to 2\n \n - *Cluster* resource association to workspace should be carried ONLY\n     after you received a confirmation from the product group that the\n@@ -414,8 +414,8 @@ All your data is accessible after the key rotation operation including data inge\n \n - CMK encryption applies to newly ingested data after the CMK\n     configuration. Data that was ingested prior to the CMK\n-    configuration, remained encrypted with Microsoft key. You can query\n-    data before and after the configuration seamlessly.\n+    configuration, remaines encrypted with Microsoft key. You can query\n+    data before and after the CMK configuration seamlessly.\n \n - Once workspace is associated to a *Cluster* resource, it cannot be\n     de-associated from the *Cluster* resource, since data is encrypted"
  },
  {
    "Number": 107974,
    "Title": "clarify app gateway WAF metrics are for v1",
    "ClosedAt": "2020-03-17T14:11:27Z",
    "User": "vhorne",
    "FileName": "articles/azure-monitor/platform/metrics-supported.md",
    "Addition": 4,
    "Delections": 4,
    "Changes": 8,
    "Patch": "@@ -5,7 +5,7 @@ author: anirudhcavale\n services: azure-monitor\r\n \r\n ms.topic: reference\r\n-ms.date: 12/18/2019\r\n+ms.date: 03/17/2020\r\n ms.author: ancav\r\n ms.subservice: metrics\r\n ---\r\n@@ -1465,9 +1465,9 @@ Azure Monitor provides several ways to interact with metrics, including charting\n |BackendConnectTime|Backend Connect Time|MilliSeconds|Average|Time spent establishing a connection with a backend server|Listener,BackendServer,BackendPool,BackendHttpSetting|\r\n |BackendFirstByteResponseTime|Backend First Byte Response Time|MilliSeconds|Average|Time interval between start of establishing a connection to backend server and receiving the first byte of the response header, approximating processing time of backend server|Listener,BackendServer,BackendPool,BackendHttpSetting|\r\n |BackendLastByteResponseTime|Backend Last Byte Response Time|MilliSeconds|Average|Time interval between start of establishing a connection to backend server and receiving the last byte of the response body|Listener,BackendServer,BackendPool,BackendHttpSetting|\r\n-|MatchedCount|Web Application Firewall Total Rule Distribution|Count|Total|Web Application Firewall Total Rule Distribution for the incoming traffic|RuleGroup,RuleId|\r\n-|BlockedCount|Web Application Firewall Blocked Requests Rule Distribution|Count|Total|Web Application Firewall blocked requests rule distribution|RuleGroup,RuleId|\r\n-|BlockedReqCount|Web Application Firewall Blocked Requests Count|Count|Total|Web Application Firewall blocked requests count|None|\r\n+|MatchedCount|Web Application Firewall v1 Total Rule Distribution|Count|Total|Web Application Firewall v1 Total Rule Distribution for the incoming traffic|RuleGroup,RuleId|\r\n+|BlockedCount|Web Application Firewall v1 Blocked Requests Rule Distribution|Count|Total|Web Application Firewall v1 blocked requests rule distribution|RuleGroup,RuleId|\r\n+|BlockedReqCount|Web Application Firewall v1 Blocked Requests Count|Count|Total|Web Application Firewall v1 blocked requests count|None|\r\n \r\n ## Microsoft.Network/virtualNetworkGateways\r\n \r"
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/active-directory/app-provisioning/application-provisioning-configure-api.md",
    "Addition": 4,
    "Delections": 4,
    "Changes": 8,
    "Patch": "@@ -171,7 +171,7 @@ Content-type: application/json\n \n ### Retrieve the template for the provisioning connector\n \n-Applications in the gallery that are enabled for provisioning have templates to streamline configuration. Use the request below to [retrieve the template for the provisioning configuration](https://docs.microsoft.com/graph/api/synchronization-synchronizationtemplate-list?view=graph-rest-beta&tabs=http).\n+Applications in the gallery that are enabled for provisioning have templates to streamline configuration. Use the request below to [retrieve the template for the provisioning configuration](https://docs.microsoft.com/graph/api/synchronization-synchronizationtemplate-list?view=graph-rest-beta&tabs=http). Note that you will need to provide the ID. The ID refers to the preceding resource, which in this case is the ServicePrincipal. \n \n #### *Request*\n \n@@ -263,10 +263,10 @@ Content-type: application/json\n \n ### Test the connection to the application\n \n-Test the connection with the third-party application. The example below is for an application that requires clientSecret and secretToken. Each application has its on requirements. Review the [API documentation](https://docs.microsoft.com/graph/api/synchronization-synchronizationjob-validatecredentials?view=graph-rest-beta&tabs=http) to see the available options. \n+Test the connection with the third-party application. The example below is for an application that requires clientSecret and secretToken. Each application has its on requirements. Applications often use BaseAddress in place of ClientSecret. To determine what credentials your app requires, navigate to the provisioning configuration page for your application and in developer mode click test connection. The network traffic will show the parameters used for credentials. The full list of credentials can be found [here](https://docs.microsoft.com/graph/api/synchronization-synchronizationjob-validatecredentials?view=graph-rest-beta&tabs=http). \n \n #### *Request*\n-```http\n+```msgraph-interactive\n POST https://graph.microsoft.com/beta/servicePrincipals/{id}/synchronization/jobs/{id}/validateCredentials\n { \n     credentials: [ \n@@ -290,7 +290,7 @@ HTTP/1.1 204 No Content\n Configuring provisioning requires establishing a trust between Azure AD and the application. Authorize access to the third-party application. The example below is for an application that requires clientSecret and secretToken. Each application has its on requirements. Review the [API documentation](https://docs.microsoft.com/graph/api/synchronization-synchronizationjob-validatecredentials?view=graph-rest-beta&tabs=http) to see the available options. \n \n #### *Request*\n-```json\n+```msgraph-interactive\n PUT https://graph.microsoft.com/beta/servicePrincipals/{id}/synchronization/secrets \n  \n { "
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/active-directory/develop/howto-convert-app-to-be-multi-tenant.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -11,7 +11,7 @@ ms.service: active-directory\n ms.subservice: develop\n ms.topic: conceptual\n ms.workload: identity\n-ms.date: 02/19/2020\n+ms.date: 03/17/2020\n ms.author: ryanwi\n ms.reviewer: jmprieur, lenalepa, sureshja, kkrishna\n ms.custom: aaddev\n@@ -172,7 +172,7 @@ In this article, you learned how to build an application that can sign in a user\n \n ## Related content\n \n-* [Multi-tenant application sample](https://github.com/mspnp/multitenant-saas-guidance)\n+* [Multi-tenant application sample](https://github.com/Azure-Samples/active-directory-aspnetcore-webapp-openidconnect-v2/blob/master/2-WebApp-graph-user/2-3-Multi-Tenant/README.md)\n * [Branding guidelines for applications][AAD-App-Branding]\n * [Application objects and service principal objects][AAD-App-SP-Objects]\n * [Integrating applications with Azure Active Directory][AAD-Integrating-Apps]"
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/active-directory/fundamentals/active-directory-data-storage-australia-newzealand.md",
    "Addition": 4,
    "Delections": 6,
    "Changes": 10,
    "Patch": "@@ -15,22 +15,20 @@ ms.custom: \"it-pro, seodec18\"\n ms.collection: M365-identity-device-management\n ---\n \n-# Identity data storage for Australian customers in Azure Active Directory\n+# Identity data storage for Australian and New Zealand customers in Azure Active Directory\n \n Identity data is stored by Azure AD in a geographical location based on the address provided by your organization when subscribing for a Microsoft Online service such as Office 365 and Azure. For information on where your Identity Customer Data is stored, you can use the [Where is your data located?](https://www.microsoft.com/trustcenter/privacy/where-your-data-is-located) section of the Microsoft Trust Center.\n \n > [!NOTE]\n > Services and applications that integrate with Azure AD have access to Identity Customer Data. Evaluate each service and application you use to determine how Identity Customer Data is processed by that specific service and application, and whether they meet your company's data storage requirements. For more information about Microsoft services' data residency, see the Where is your data located? section of the Microsoft Trust Center.\n \n-For customers who provided an address in Australia, Azure AD keeps identity data for these services within Australian datacenters: \n-- Azure AD Directory Management \n-- Authentication\n+For customers who provided an address in Australia and New Zealand and uses Azure AD free edition, Azure AD keeps PII data at rest within Australian datacenters. \n \n-All other Azure AD services store customer data in global datacenters. To locate the datacenter for a service, see [Azure Active Directory – Where is your data located?](https://www.microsoft.com/trustcenter/privacy/where-your-data-is-located)\n+All other Azure AD premium services store customer data in global datacenters. To locate the datacenter for a service, see [Azure Active Directory – Where is your data located?](https://www.microsoft.com/trustcenter/privacy/where-your-data-is-located)\n \n ## Microsoft Azure multi-factor authentication (MFA)\n \n-MFA stores Identity Customer Data in global datacenters. To learn more about the user information collected and stored by cloud-based Azure MFA and Azure MFA Server, see [Azure Multi-Factor Authentication user data collection](https://docs.microsoft.com/azure/active-directory/authentication/concept-mfa-data-residency).\n+MFA service in Azure AD stores Identity Customer Data in global datacenters at rest. To learn more about the user information collected and stored by cloud-based Azure MFA and Azure MFA Server, see [Azure Multi-Factor Authentication user data collection](https://docs.microsoft.com/azure/active-directory/authentication/concept-mfa-data-residency). If customers use MFA their data will be stored outside of Australia datacenters at rest. \n \n ## Next steps\n For more information about any of the features and functionality described above, see these articles:"
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/active-directory/saas-apps/datadog-tutorial.md",
    "Addition": 157,
    "Delections": 0,
    "Changes": 157,
    "Patch": "@@ -0,0 +1,157 @@\n+---\n+title: 'Tutorial: Azure Active Directory single sign-on (SSO) integration with Datadog | Microsoft Docs'\n+description: Learn how to configure single sign-on between Azure Active Directory and Datadog.\n+services: active-directory\n+documentationCenter: na\n+author: jeevansd\n+manager: mtillman\n+ms.reviewer: barbkess\n+\n+ms.assetid: b7845bdd-7bcd-4888-84fd-2551345054ee\n+ms.service: active-directory\n+ms.subservice: saas-app-tutorial\n+ms.workload: identity\n+ms.tgt_pltfrm: na\n+ms.topic: tutorial\n+ms.date: 03/12/2020\n+ms.author: jeedes\n+\n+ms.collection: M365-identity-device-management\n+---\n+\n+# Tutorial: Azure Active Directory single sign-on (SSO) integration with Datadog\n+\n+In this tutorial, you'll learn how to integrate Datadog with Azure Active Directory (Azure AD). When you integrate Datadog with Azure AD, you can:\n+\n+* Control in Azure AD who has access to Datadog.\n+* Enable your users to be automatically signed-in to Datadog with their Azure AD accounts.\n+* Manage your accounts in one central location - the Azure portal.\n+\n+To learn more about SaaS app integration with Azure AD, see [What is application access and single sign-on with Azure Active Directory](https://docs.microsoft.com/azure/active-directory/manage-apps/what-is-single-sign-on).\n+\n+## Prerequisites\n+\n+To get started, you need the following items:\n+\n+* An Azure AD subscription. If you don't have a subscription, you can get a [free account](https://azure.microsoft.com/free/).\n+* Datadog single sign-on (SSO) enabled subscription.\n+\n+## Scenario description\n+\n+In this tutorial, you configure and test Azure AD SSO in a test environment.\n+\n+* Datadog supports **SP and IDP** initiated SSO\n+* Once you configure Datadog you can enforce Session Control, which protect exfiltration and infiltration of your organization’s sensitive data in real-time. Session Control extend from Conditional Access. [Learn how to enforce session control with Microsoft Cloud App Security](https://docs.microsoft.com/cloud-app-security/proxy-deployment-aad)\n+\n+\n+## Adding Datadog from the gallery\n+\n+To configure the integration of Datadog into Azure AD, you need to add Datadog from the gallery to your list of managed SaaS apps.\n+\n+1. Sign in to the [Azure portal](https://portal.azure.com) using either a work or school account, or a personal Microsoft account.\n+1. On the left navigation pane, select the **Azure Active Directory** service.\n+1. Navigate to **Enterprise Applications** and then select **All Applications**.\n+1. To add new application, select **New application**.\n+1. In the **Add from the gallery** section, type **Datadog** in the search box.\n+1. Select **Datadog** from results panel and then add the app. Wait a few seconds while the app is added to your tenant.\n+\n+\n+## Configure and test Azure AD single sign-on for Datadog\n+\n+Configure and test Azure AD SSO with Datadog using a test user called **B.Simon**. For SSO to work, you need to establish a link relationship between an Azure AD user and the related user in Datadog.\n+\n+To configure and test Azure AD SSO with Datadog, complete the following building blocks:\n+\n+1. **[Configure Azure AD SSO](#configure-azure-ad-sso)** - to enable your users to use this feature.\n+    1. **[Create an Azure AD test user](#create-an-azure-ad-test-user)** - to test Azure AD single sign-on with B.Simon.\n+    1. **[Assign the Azure AD test user](#assign-the-azure-ad-test-user)** - to enable B.Simon to use Azure AD single sign-on.\n+1. **[Configure Datadog SSO](#configure-datadog-sso)** - to configure the single sign-on settings on application side.\n+    1. **[Create Datadog test user](#create-datadog-test-user)** - to have a counterpart of B.Simon in Datadog that is linked to the Azure AD representation of user.\n+1. **[Test SSO](#test-sso)** - to verify whether the configuration works.\n+\n+## Configure Azure AD SSO\n+\n+Follow these steps to enable Azure AD SSO in the Azure portal.\n+\n+1. In the [Azure portal](https://portal.azure.com/), on the **Datadog** application integration page, find the **Manage** section and select **single sign-on**.\n+1. On the **Select a single sign-on method** page, select **SAML**.\n+1. On the **Set up single sign-on with SAML** page, click the edit/pen icon for **Basic SAML Configuration** to edit the settings.\n+\n+   ![Edit Basic SAML Configuration](common/edit-urls.png)\n+\n+1. On the **Basic SAML Configuration** section, the user does not have to perform any step as the app is already pre-integrated with Azure.\n+\n+1. Click **Set additional URLs** and perform the following step if you wish to configure the application in **SP** initiated mode:\n+\n+    In the **Sign-on URL** text box, type a URL using the following pattern:\n+    `https://app.datadoghq.com/account/login/id/<CUSTOM_IDENTIFIER>`\n+\n+    > [!NOTE]\n+\t> The value is not real. Update the value with the actual Sign-on URL. Contact [Datadog Client support team](mailto:xuefwu@microsoft.com) to get the value. You can also refer to the patterns shown in the **Basic SAML Configuration** section in the Azure portal.\n+\n+1. Click **Save**.\n+\n+1. On the **Set up single sign-on with SAML** page, in the **SAML Signing Certificate** section,  find **Federation Metadata XML** and select **Download** to download the certificate and save it on your computer.\n+\n+\t![The Certificate download link](common/metadataxml.png)\n+\n+1. On the **Set up Datadog** section, copy the appropriate URL(s) based on your requirement.\n+\n+\t![Copy configuration URLs](common/copy-configuration-urls.png)\n+\n+### Create an Azure AD test user\n+\n+In this section, you'll create a test user in the Azure portal called B.Simon.\n+\n+1. From the left pane in the Azure portal, select **Azure Active Directory**, select **Users**, and then select **All users**.\n+1. Select **New user** at the top of the screen.\n+1. In the **User** properties, follow these steps:\n+   1. In the **Name** field, enter `B.Simon`.  \n+   1. In the **User name** field, enter the username@companydomain.extension. For example, `B.Simon@contoso.com`.\n+   1. Select the **Show password** check box, and then write down the value that's displayed in the **Password** box.\n+   1. Click **Create**.\n+\n+### Assign the Azure AD test user\n+\n+In this section, you'll enable B.Simon to use Azure single sign-on by granting access to Datadog.\n+\n+1. In the Azure portal, select **Enterprise Applications**, and then select **All applications**.\n+1. In the applications list, select **Datadog**.\n+1. In the app's overview page, find the **Manage** section and select **Users and groups**.\n+\n+   ![The \"Users and groups\" link](common/users-groups-blade.png)\n+\n+1. Select **Add user**, then select **Users and groups** in the **Add Assignment** dialog.\n+\n+\t![The Add User link](common/add-assign-user.png)\n+\n+1. In the **Users and groups** dialog, select **B.Simon** from the Users list, then click the **Select** button at the bottom of the screen.\n+1. If you're expecting any role value in the SAML assertion, in the **Select Role** dialog, select the appropriate role for the user from the list and then click the **Select** button at the bottom of the screen.\n+1. In the **Add Assignment** dialog, click the **Assign** button.\n+\n+\n+## Configure Datadog SSO\n+\n+To configure single sign-on on **Datadog** side, you need to send the downloaded **Federation Metadata XML** and appropriate copied URLs from Azure portal to [Datadog support team](mailto:xuefwu@microsoft.com). They set this setting to have the SAML SSO connection set properly on both sides.\n+\n+### Create Datadog test user\n+\n+In this section, you create a user called B.Simon in Datadog. Work with [Datadog support team](mailto:xuefwu@microsoft.com) to add the users in the Datadog platform.\n+\n+## Test SSO \n+\n+In this section, you test your Azure AD single sign-on configuration using the Access Panel.\n+\n+When you click the Datadog tile in the Access Panel, you should be automatically signed in to the Datadog for which you set up SSO. For more information about the Access Panel, see [Introduction to the Access Panel](https://docs.microsoft.com/azure/active-directory/active-directory-saas-access-panel-introduction).\n+\n+## Additional resources\n+\n+- [ List of Tutorials on How to Integrate SaaS Apps with Azure Active Directory ](https://docs.microsoft.com/azure/active-directory/active-directory-saas-tutorial-list)\n+\n+- [What is application access and single sign-on with Azure Active Directory? ](https://docs.microsoft.com/azure/active-directory/manage-apps/what-is-single-sign-on)\n+\n+- [What is conditional access in Azure Active Directory?](https://docs.microsoft.com/azure/active-directory/conditional-access/overview)\n+\n+- [Try Datadog with Azure AD](https://aad.portal.azure.com/)\n+\n+- [What is session control in Microsoft Cloud App Security?](https://docs.microsoft.com/cloud-app-security/proxy-intro-aad)\n\\ No newline at end of file"
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/active-directory/saas-apps/ip-platform-tutorial.md",
    "Addition": 152,
    "Delections": 0,
    "Changes": 152,
    "Patch": "@@ -0,0 +1,152 @@\n+---\n+title: 'Tutorial: Azure Active Directory single sign-on (SSO) integration with IP Platform | Microsoft Docs'\n+description: Learn how to configure single sign-on between Azure Active Directory and IP Platform.\n+services: active-directory\n+documentationCenter: na\n+author: jeevansd\n+manager: mtillman\n+ms.reviewer: barbkess\n+\n+ms.assetid: 953efaf5-f1da-4660-8bb4-8a6805aecc43\n+ms.service: active-directory\n+ms.subservice: saas-app-tutorial\n+ms.workload: identity\n+ms.tgt_pltfrm: na\n+ms.topic: tutorial\n+ms.date: 03/12/2020\n+ms.author: jeedes\n+\n+ms.collection: M365-identity-device-management\n+---\n+\n+# Tutorial: Azure Active Directory single sign-on (SSO) integration with IP Platform\n+\n+In this tutorial, you'll learn how to integrate IP Platform with Azure Active Directory (Azure AD). When you integrate IP Platform with Azure AD, you can:\n+\n+* Control in Azure AD who has access to IP Platform.\n+* Enable your users to be automatically signed-in to IP Platform with their Azure AD accounts.\n+* Manage your accounts in one central location - the Azure portal.\n+\n+To learn more about SaaS app integration with Azure AD, see [What is application access and single sign-on with Azure Active Directory](https://docs.microsoft.com/azure/active-directory/manage-apps/what-is-single-sign-on).\n+\n+## Prerequisites\n+\n+To get started, you need the following items:\n+\n+* An Azure AD subscription. If you don't have a subscription, you can get a [free account](https://azure.microsoft.com/free/).\n+* IP Platform single sign-on (SSO) enabled subscription.\n+\n+## Scenario description\n+\n+In this tutorial, you configure and test Azure AD SSO in a test environment.\n+\n+* IP Platform supports **SP** initiated SSO\n+* IP Platform supports **Just In Time** user provisioning\n+* Once you configure IP Platform you can enforce Session Control, which protect exfiltration and infiltration of your organization’s sensitive data in real-time. Session Control extend from Conditional Access. [Learn how to enforce session control with Microsoft Cloud App Security](https://docs.microsoft.com/cloud-app-security/proxy-deployment-aad)\n+\n+## Adding IP Platform from the gallery\n+\n+To configure the integration of IP Platform into Azure AD, you need to add IP Platform from the gallery to your list of managed SaaS apps.\n+\n+1. Sign in to the [Azure portal](https://portal.azure.com) using either a work or school account, or a personal Microsoft account.\n+1. On the left navigation pane, select the **Azure Active Directory** service.\n+1. Navigate to **Enterprise Applications** and then select **All Applications**.\n+1. To add new application, select **New application**.\n+1. In the **Add from the gallery** section, type **IP Platform** in the search box.\n+1. Select **IP Platform** from results panel and then add the app. Wait a few seconds while the app is added to your tenant.\n+\n+\n+## Configure and test Azure AD single sign-on for IP Platform\n+\n+Configure and test Azure AD SSO with IP Platform using a test user called **B.Simon**. For SSO to work, you need to establish a link relationship between an Azure AD user and the related user in IP Platform.\n+\n+To configure and test Azure AD SSO with IP Platform, complete the following building blocks:\n+\n+1. **[Configure Azure AD SSO](#configure-azure-ad-sso)** - to enable your users to use this feature.\n+    1. **[Create an Azure AD test user](#create-an-azure-ad-test-user)** - to test Azure AD single sign-on with B.Simon.\n+    1. **[Assign the Azure AD test user](#assign-the-azure-ad-test-user)** - to enable B.Simon to use Azure AD single sign-on.\n+1. **[Configure IP Platform SSO](#configure-ip-platform-sso)** - to configure the single sign-on settings on application side.\n+    1. **[Create IP Platform test user](#create-ip-platform-test-user)** - to have a counterpart of B.Simon in IP Platform that is linked to the Azure AD representation of user.\n+1. **[Test SSO](#test-sso)** - to verify whether the configuration works.\n+\n+## Configure Azure AD SSO\n+\n+Follow these steps to enable Azure AD SSO in the Azure portal.\n+\n+1. In the [Azure portal](https://portal.azure.com/), on the **IP Platform** application integration page, find the **Manage** section and select **single sign-on**.\n+1. On the **Select a single sign-on method** page, select **SAML**.\n+1. On the **Set up single sign-on with SAML** page, click the edit/pen icon for **Basic SAML Configuration** to edit the settings.\n+\n+   ![Edit Basic SAML Configuration](common/edit-urls.png)\n+\n+1. On the **Basic SAML Configuration** section, enter the values for the following fields:\n+\n+    In the **Sign-on URL** text box, type a URL using the following pattern:\n+    `https://<SUBDOMAIN>.ipplatform.com`\n+\n+\t> [!NOTE]\n+\t> The value is not real. Update the value with the actual Sign-On URL. Contact [IP Platform Client support team](mailto:helpdesk@cpaglobal.com) to get the value. You can also refer to the patterns shown in the **Basic SAML Configuration** section in the Azure portal.\n+\n+1. On the **Set up single sign-on with SAML** page, in the **SAML Signing Certificate** section,  find **Federation Metadata XML** and select **Download** to download the certificate and save it on your computer.\n+\n+\t![The Certificate download link](common/metadataxml.png)\n+\n+1. On the **Set up IP Platform** section, copy the appropriate URL(s) based on your requirement.\n+\n+\t![Copy configuration URLs](common/copy-configuration-urls.png)\n+\n+### Create an Azure AD test user\n+\n+In this section, you'll create a test user in the Azure portal called B.Simon.\n+\n+1. From the left pane in the Azure portal, select **Azure Active Directory**, select **Users**, and then select **All users**.\n+1. Select **New user** at the top of the screen.\n+1. In the **User** properties, follow these steps:\n+   1. In the **Name** field, enter `B.Simon`.  \n+   1. In the **User name** field, enter the username@companydomain.extension. For example, `B.Simon@contoso.com`.\n+   1. Select the **Show password** check box, and then write down the value that's displayed in the **Password** box.\n+   1. Click **Create**.\n+\n+### Assign the Azure AD test user\n+\n+In this section, you'll enable B.Simon to use Azure single sign-on by granting access to IP Platform.\n+\n+1. In the Azure portal, select **Enterprise Applications**, and then select **All applications**.\n+1. In the applications list, select **IP Platform**.\n+1. In the app's overview page, find the **Manage** section and select **Users and groups**.\n+\n+   ![The \"Users and groups\" link](common/users-groups-blade.png)\n+\n+1. Select **Add user**, then select **Users and groups** in the **Add Assignment** dialog.\n+\n+\t![The Add User link](common/add-assign-user.png)\n+\n+1. In the **Users and groups** dialog, select **B.Simon** from the Users list, then click the **Select** button at the bottom of the screen.\n+1. If you're expecting any role value in the SAML assertion, in the **Select Role** dialog, select the appropriate role for the user from the list and then click the **Select** button at the bottom of the screen.\n+1. In the **Add Assignment** dialog, click the **Assign** button.\n+\n+## Configure IP Platform SSO\n+\n+To configure single sign-on on **IP Platform** side, you need to send the downloaded **Federation Metadata XML** and appropriate copied URLs from Azure portal to [IP Platform support team](mailto:helpdesk@cpaglobal.com). They set this setting to have the SAML SSO connection set properly on both sides.\n+\n+### Create IP Platform test user\n+\n+In this section, a user called Britta Simon is created in IP Platform. IP Platform supports just-in-time user provisioning, which is enabled by default. There is no action item for you in this section. If a user doesn't already exist in IP Platform, a new one is created after authentication.\n+\n+## Test SSO \n+\n+In this section, you test your Azure AD single sign-on configuration using the Access Panel.\n+\n+When you click the IP Platform tile in the Access Panel, you should be automatically signed in to the IP Platform for which you set up SSO. For more information about the Access Panel, see [Introduction to the Access Panel](https://docs.microsoft.com/azure/active-directory/active-directory-saas-access-panel-introduction).\n+\n+## Additional resources\n+\n+- [ List of Tutorials on How to Integrate SaaS Apps with Azure Active Directory ](https://docs.microsoft.com/azure/active-directory/active-directory-saas-tutorial-list)\n+\n+- [What is application access and single sign-on with Azure Active Directory? ](https://docs.microsoft.com/azure/active-directory/manage-apps/what-is-single-sign-on)\n+\n+- [What is conditional access in Azure Active Directory?](https://docs.microsoft.com/azure/active-directory/conditional-access/overview)\n+\n+- [Try IP Platform with Azure AD](https://aad.portal.azure.com/)\n+\n+- [What is session control in Microsoft Cloud App Security?](https://docs.microsoft.com/cloud-app-security/proxy-intro-aad)\n\\ No newline at end of file"
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/active-directory/saas-apps/lift-tutorial.md",
    "Addition": 152,
    "Delections": 0,
    "Changes": 152,
    "Patch": "@@ -0,0 +1,152 @@\n+---\n+title: 'Tutorial: Azure Active Directory single sign-on (SSO) integration with LIFT | Microsoft Docs'\n+description: Learn how to configure single sign-on between Azure Active Directory and LIFT.\n+services: active-directory\n+documentationCenter: na\n+author: jeevansd\n+manager: mtillman\n+ms.reviewer: barbkess\n+\n+ms.assetid: 42c9cfe0-3ebb-428d-91e4-822cbddf2aae\n+ms.service: active-directory\n+ms.subservice: saas-app-tutorial\n+ms.workload: identity\n+ms.tgt_pltfrm: na\n+ms.topic: tutorial\n+ms.date: 03/11/2020\n+ms.author: jeedes\n+\n+ms.collection: M365-identity-device-management\n+---\n+\n+# Tutorial: Azure Active Directory single sign-on (SSO) integration with LIFT\n+\n+In this tutorial, you'll learn how to integrate LIFT with Azure Active Directory (Azure AD). When you integrate LIFT with Azure AD, you can:\n+\n+* Control in Azure AD who has access to LIFT.\n+* Enable your users to be automatically signed-in to LIFT with their Azure AD accounts.\n+* Manage your accounts in one central location - the Azure portal.\n+\n+To learn more about SaaS app integration with Azure AD, see [What is application access and single sign-on with Azure Active Directory](https://docs.microsoft.com/azure/active-directory/manage-apps/what-is-single-sign-on).\n+\n+## Prerequisites\n+\n+To get started, you need the following items:\n+\n+* An Azure AD subscription. If you don't have a subscription, you can get a [free account](https://azure.microsoft.com/free/).\n+* LIFT single sign-on (SSO) enabled subscription.\n+\n+## Scenario description\n+\n+In this tutorial, you configure and test Azure AD SSO in a test environment.\n+\n+* LIFT supports **SP** initiated SSO\n+* Once you configure LIFT you can enforce Session Control, which protect exfiltration and infiltration of your organization’s sensitive data in real-time. Session Control extend from Conditional Access. [Learn how to enforce session control with Microsoft Cloud App Security](https://docs.microsoft.com/cloud-app-security/proxy-deployment-aad)\n+\n+## Adding LIFT from the gallery\n+\n+To configure the integration of LIFT into Azure AD, you need to add LIFT from the gallery to your list of managed SaaS apps.\n+\n+1. Sign in to the [Azure portal](https://portal.azure.com) using either a work or school account, or a personal Microsoft account.\n+1. On the left navigation pane, select the **Azure Active Directory** service.\n+1. Navigate to **Enterprise Applications** and then select **All Applications**.\n+1. To add new application, select **New application**.\n+1. In the **Add from the gallery** section, type **LIFT** in the search box.\n+1. Select **LIFT** from results panel and then add the app. Wait a few seconds while the app is added to your tenant.\n+\n+\n+## Configure and test Azure AD single sign-on for LIFT\n+\n+Configure and test Azure AD SSO with LIFT using a test user called **B.Simon**. For SSO to work, you need to establish a link relationship between an Azure AD user and the related user in LIFT.\n+\n+To configure and test Azure AD SSO with LIFT, complete the following building blocks:\n+\n+1. **[Configure Azure AD SSO](#configure-azure-ad-sso)** - to enable your users to use this feature.\n+    1. **[Create an Azure AD test user](#create-an-azure-ad-test-user)** - to test Azure AD single sign-on with B.Simon.\n+    1. **[Assign the Azure AD test user](#assign-the-azure-ad-test-user)** - to enable B.Simon to use Azure AD single sign-on.\n+1. **[Configure LIFT SSO](#configure-lift-sso)** - to configure the single sign-on settings on application side.\n+    1. **[Create LIFT test user](#create-lift-test-user)** - to have a counterpart of B.Simon in LIFT that is linked to the Azure AD representation of user.\n+1. **[Test SSO](#test-sso)** - to verify whether the configuration works.\n+\n+## Configure Azure AD SSO\n+\n+Follow these steps to enable Azure AD SSO in the Azure portal.\n+\n+1. In the [Azure portal](https://portal.azure.com/), on the **LIFT** application integration page, find the **Manage** section and select **single sign-on**.\n+1. On the **Select a single sign-on method** page, select **SAML**.\n+1. On the **Set up single sign-on with SAML** page, click the edit/pen icon for **Basic SAML Configuration** to edit the settings.\n+\n+   ![Edit Basic SAML Configuration](common/edit-urls.png)\n+\n+1. On the **Basic SAML Configuration** section, enter the values for the following fields:\n+\n+\ta. In the **Sign on URL** text box, type a URL using the following pattern:\n+    `https://<companyname>.portal.liftsoftware.nl/lift/secure`\n+\n+    b. In the **Identifier (Entity ID)** text box, type a URL using the following pattern:\n+    `https://<companyname>.portal.liftsoftware.nl/saml-metadata/<identifier>`\n+\n+\t> [!NOTE]\n+\t> These values are not real. Update these values with the actual Sign on URL and Identifier. Contact [LIFT Client support team](mailto:support@liftsoftware.nl) to get these values. You can also refer to the patterns shown in the **Basic SAML Configuration** section in the Azure portal.\n+\n+\n+1. On the **Set up single sign-on with SAML** page, In the **SAML Signing Certificate** section, click copy button to copy **App Federation Metadata Url** and save it on your computer.\n+\n+\t![The Certificate download link](common/copy-metadataurl.png)\n+\n+### Create an Azure AD test user\n+\n+In this section, you'll create a test user in the Azure portal called B.Simon.\n+\n+1. From the left pane in the Azure portal, select **Azure Active Directory**, select **Users**, and then select **All users**.\n+1. Select **New user** at the top of the screen.\n+1. In the **User** properties, follow these steps:\n+   1. In the **Name** field, enter `B.Simon`.  \n+   1. In the **User name** field, enter the username@companydomain.extension. For example, `B.Simon@contoso.com`.\n+   1. Select the **Show password** check box, and then write down the value that's displayed in the **Password** box.\n+   1. Click **Create**.\n+\n+### Assign the Azure AD test user\n+\n+In this section, you'll enable B.Simon to use Azure single sign-on by granting access to LIFT.\n+\n+1. In the Azure portal, select **Enterprise Applications**, and then select **All applications**.\n+1. In the applications list, select **LIFT**.\n+1. In the app's overview page, find the **Manage** section and select **Users and groups**.\n+\n+   ![The \"Users and groups\" link](common/users-groups-blade.png)\n+\n+1. Select **Add user**, then select **Users and groups** in the **Add Assignment** dialog.\n+\n+\t![The Add User link](common/add-assign-user.png)\n+\n+1. In the **Users and groups** dialog, select **B.Simon** from the Users list, then click the **Select** button at the bottom of the screen.\n+1. If you're expecting any role value in the SAML assertion, in the **Select Role** dialog, select the appropriate role for the user from the list and then click the **Select** button at the bottom of the screen.\n+1. In the **Add Assignment** dialog, click the **Assign** button.\n+\n+\n+## Configure LIFT SSO\n+\n+To configure single sign-on on **LIFT** side, you need to send the **App Federation Metadata Url** to [LIFT support team](mailto:support@liftsoftware.nl). They set this setting to have the SAML SSO connection set properly on both sides.\n+\n+### Create LIFT test user\n+\n+In this section, you create a user called B.Simon in LIFT. Work with [LIFT support team](mailto:support@liftsoftware.nl) to add the users in the LIFT platform.\n+\n+## Test SSO \n+\n+In this section, you test your Azure AD single sign-on configuration using the Access Panel.\n+\n+When you click the LIFT tile in the Access Panel, you should be automatically signed in to the LIFT for which you set up SSO. For more information about the Access Panel, see [Introduction to the Access Panel](https://docs.microsoft.com/azure/active-directory/active-directory-saas-access-panel-introduction).\n+\n+## Additional resources\n+\n+- [ List of Tutorials on How to Integrate SaaS Apps with Azure Active Directory ](https://docs.microsoft.com/azure/active-directory/active-directory-saas-tutorial-list)\n+\n+- [What is application access and single sign-on with Azure Active Directory? ](https://docs.microsoft.com/azure/active-directory/manage-apps/what-is-single-sign-on)\n+\n+- [What is conditional access in Azure Active Directory?](https://docs.microsoft.com/azure/active-directory/conditional-access/overview)\n+\n+- [Try LIFT with Azure AD](https://aad.portal.azure.com/)\n+\n+- [What is session control in Microsoft Cloud App Security?](https://docs.microsoft.com/cloud-app-security/proxy-intro-aad)\n\\ No newline at end of file"
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/active-directory/saas-apps/planview-enterprise-one-tutorial.md",
    "Addition": 155,
    "Delections": 0,
    "Changes": 155,
    "Patch": "@@ -0,0 +1,155 @@\n+---\n+title: 'Tutorial: Azure Active Directory single sign-on (SSO) integration with Planview Enterprise One | Microsoft Docs'\n+description: Learn how to configure single sign-on between Azure Active Directory and Planview Enterprise One.\n+services: active-directory\n+documentationCenter: na\n+author: jeevansd\n+manager: mtillman\n+ms.reviewer: barbkess\n+\n+ms.assetid: 933a5b89-e795-4f63-b310-9277d6a32729\n+ms.service: active-directory\n+ms.subservice: saas-app-tutorial\n+ms.workload: identity\n+ms.tgt_pltfrm: na\n+ms.topic: tutorial\n+ms.date: 03/12/2020\n+ms.author: jeedes\n+\n+ms.collection: M365-identity-device-management\n+---\n+\n+# Tutorial: Azure Active Directory single sign-on (SSO) integration with Planview Enterprise One\n+\n+In this tutorial, you'll learn how to integrate Planview Enterprise One with Azure Active Directory (Azure AD). When you integrate Planview Enterprise One with Azure AD, you can:\n+\n+* Control in Azure AD who has access to Planview Enterprise One.\n+* Enable your users to be automatically signed-in to Planview Enterprise One with their Azure AD accounts.\n+* Manage your accounts in one central location - the Azure portal.\n+\n+To learn more about SaaS app integration with Azure AD, see [What is application access and single sign-on with Azure Active Directory](https://docs.microsoft.com/azure/active-directory/manage-apps/what-is-single-sign-on).\n+\n+## Prerequisites\n+\n+To get started, you need the following items:\n+\n+* An Azure AD subscription. If you don't have a subscription, you can get a [free account](https://azure.microsoft.com/free/).\n+* Planview Enterprise One single sign-on (SSO) enabled subscription.\n+\n+## Scenario description\n+\n+In this tutorial, you configure and test Azure AD SSO in a test environment.\n+\n+* Planview Enterprise One supports **SP** initiated SSO\n+* Once you configure Planview Enterprise One you can enforce Session Control, which protect exfiltration and infiltration of your organization’s sensitive data in real-time. Session Control extend from Conditional Access. [Learn how to enforce session control with Microsoft Cloud App Security](https://docs.microsoft.com/cloud-app-security/proxy-deployment-aad)\n+\n+\n+## Adding Planview Enterprise One from the gallery\n+\n+To configure the integration of Planview Enterprise One into Azure AD, you need to add Planview Enterprise One from the gallery to your list of managed SaaS apps.\n+\n+1. Sign in to the [Azure portal](https://portal.azure.com) using either a work or school account, or a personal Microsoft account.\n+1. On the left navigation pane, select the **Azure Active Directory** service.\n+1. Navigate to **Enterprise Applications** and then select **All Applications**.\n+1. To add new application, select **New application**.\n+1. In the **Add from the gallery** section, type **Planview Enterprise One** in the search box.\n+1. Select **Planview Enterprise One** from results panel and then add the app. Wait a few seconds while the app is added to your tenant.\n+\n+\n+## Configure and test Azure AD single sign-on for Planview Enterprise One\n+\n+Configure and test Azure AD SSO with Planview Enterprise One using a test user called **B.Simon**. For SSO to work, you need to establish a link relationship between an Azure AD user and the related user in Planview Enterprise One.\n+\n+To configure and test Azure AD SSO with Planview Enterprise One, complete the following building blocks:\n+\n+1. **[Configure Azure AD SSO](#configure-azure-ad-sso)** - to enable your users to use this feature.\n+    1. **[Create an Azure AD test user](#create-an-azure-ad-test-user)** - to test Azure AD single sign-on with B.Simon.\n+    1. **[Assign the Azure AD test user](#assign-the-azure-ad-test-user)** - to enable B.Simon to use Azure AD single sign-on.\n+1. **[Configure Planview Enterprise One SSO](#configure-planview-enterprise-one-sso)** - to configure the single sign-on settings on application side.\n+    1. **[Create Planview Enterprise One test user](#create-planview-enterprise-one-test-user)** - to have a counterpart of B.Simon in Planview Enterprise One that is linked to the Azure AD representation of user.\n+1. **[Test SSO](#test-sso)** - to verify whether the configuration works.\n+\n+## Configure Azure AD SSO\n+\n+Follow these steps to enable Azure AD SSO in the Azure portal.\n+\n+1. In the [Azure portal](https://portal.azure.com/), on the **Planview Enterprise One** application integration page, find the **Manage** section and select **single sign-on**.\n+1. On the **Select a single sign-on method** page, select **SAML**.\n+1. On the **Set up single sign-on with SAML** page, click the edit/pen icon for **Basic SAML Configuration** to edit the settings.\n+\n+   ![Edit Basic SAML Configuration](common/edit-urls.png)\n+\n+1. On the **Basic SAML Configuration** section, enter the values for the following fields:\n+\n+\ta. In the **Sign on URL** text box, type a URL using the following pattern:\n+    `https://<SUBDOMAIN>.pvcloud.com/planview`\n+\n+    b. In the **Identifier (Entity ID)** text box, type a URL using the following pattern:\n+    `https://<SUBDOMAIN>.pvcloud.com/planview`\n+\n+\t> [!NOTE]\n+\t> These values are not real. Update these values with the actual Sign on URL and Identifier. Contact [Planview Enterprise One Client support team](mailto:hostingsupport@planview.com) to get these values. You can also refer to the patterns shown in the **Basic SAML Configuration** section in the Azure portal.\n+\n+1. On the **Set up single sign-on with SAML** page, in the **SAML Signing Certificate** section,  find **Federation Metadata XML** and select **Download** to download the certificate and save it on your computer.\n+\n+\t![The Certificate download link](common/metadataxml.png)\n+\n+1. On the **Set up Planview Enterprise One** section, copy the appropriate URL(s) based on your requirement.\n+\n+\t![Copy configuration URLs](common/copy-configuration-urls.png)\n+\n+### Create an Azure AD test user\n+\n+In this section, you'll create a test user in the Azure portal called B.Simon.\n+\n+1. From the left pane in the Azure portal, select **Azure Active Directory**, select **Users**, and then select **All users**.\n+1. Select **New user** at the top of the screen.\n+1. In the **User** properties, follow these steps:\n+   1. In the **Name** field, enter `B.Simon`.  \n+   1. In the **User name** field, enter the username@companydomain.extension. For example, `B.Simon@contoso.com`.\n+   1. Select the **Show password** check box, and then write down the value that's displayed in the **Password** box.\n+   1. Click **Create**.\n+\n+### Assign the Azure AD test user\n+\n+In this section, you'll enable B.Simon to use Azure single sign-on by granting access to Planview Enterprise One.\n+\n+1. In the Azure portal, select **Enterprise Applications**, and then select **All applications**.\n+1. In the applications list, select **Planview Enterprise One**.\n+1. In the app's overview page, find the **Manage** section and select **Users and groups**.\n+\n+   ![The \"Users and groups\" link](common/users-groups-blade.png)\n+\n+1. Select **Add user**, then select **Users and groups** in the **Add Assignment** dialog.\n+\n+\t![The Add User link](common/add-assign-user.png)\n+\n+1. In the **Users and groups** dialog, select **B.Simon** from the Users list, then click the **Select** button at the bottom of the screen.\n+1. If you're expecting any role value in the SAML assertion, in the **Select Role** dialog, select the appropriate role for the user from the list and then click the **Select** button at the bottom of the screen.\n+1. In the **Add Assignment** dialog, click the **Assign** button.\n+\n+## Configure Planview Enterprise One SSO\n+\n+To configure single sign-on on **Planview Enterprise One** side, you need to send the downloaded **Federation Metadata XML** and appropriate copied URLs from Azure portal to [Planview Enterprise One support team](mailto:hostingsupport@planview.com). They set this setting to have the SAML SSO connection set properly on both sides.\n+\n+### Create Planview Enterprise One test user\n+\n+In this section, you create a user called B.Simon in Planview Enterprise One. Work with [Planview Enterprise One support team](mailto:hostingsupport@planview.com) to add the users in the Planview Enterprise One platform.Users must be created and activated before you use single sign-on.\n+\n+## Test SSO \n+\n+In this section, you test your Azure AD single sign-on configuration using the Access Panel.\n+\n+When you click the Planview Enterprise One tile in the Access Panel, you should be automatically signed in to the Planview Enterprise One for which you set up SSO. For more information about the Access Panel, see [Introduction to the Access Panel](https://docs.microsoft.com/azure/active-directory/active-directory-saas-access-panel-introduction).\n+\n+## Additional resources\n+\n+- [ List of Tutorials on How to Integrate SaaS Apps with Azure Active Directory ](https://docs.microsoft.com/azure/active-directory/active-directory-saas-tutorial-list)\n+\n+- [What is application access and single sign-on with Azure Active Directory? ](https://docs.microsoft.com/azure/active-directory/active-directory-appssoaccess-whatis)\n+\n+- [What is conditional access in Azure Active Directory?](https://docs.microsoft.com/azure/active-directory/conditional-access/overview)\n+\n+- [Try Planview Enterprise One with Azure AD](https://aad.portal.azure.com/)\n+\n+- [What is session control in Microsoft Cloud App Security?](https://docs.microsoft.com/cloud-app-security/proxy-intro-aad)\n\\ No newline at end of file"
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/api-management/api-management-authentication-policies.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -113,7 +113,7 @@ In this example client certificate is identified by resource name.\n ### Example  \n #### Use managed identity to authenticate with a backend service\n ```xml  \n-<authentication-managed-identity resource=\"https://graph.windows.net\"/> \n+<authentication-managed-identity resource=\"https://graph.microsoft.com\"/> \n ```\n ```xml  \n <authentication-managed-identity resource=\"https://management.azure.com/\"/> <!--Azure Resource Manager-->"
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/api-management/api-management-howto-aad-b2c.md",
    "Addition": 0,
    "Delections": 1,
    "Changes": 1,
    "Patch": "@@ -190,7 +190,6 @@ The **Sign-up form: OAuth** widget represents a form used for signing up with OA\n \n [https://oauth.net/2/]: https://oauth.net/2/\n [WebApp-GraphAPI-DotNet]: https://github.com/AzureADSamples/WebApp-GraphAPI-DotNet\n-[Accessing the Graph API]: https://msdn.microsoft.com/library/azure/dn132599.aspx#BKMK_Graph\n [Azure Active Directory B2C overview]: https://docs.microsoft.com/azure/active-directory-b2c/active-directory-b2c-overview\n [How to authorize developer accounts using Azure Active Directory]: https://docs.microsoft.com/azure/api-management/api-management-howto-aad\n [Azure Active Directory B2C: Extensible policy framework]: https://docs.microsoft.com/azure/active-directory-b2c/active-directory-b2c-reference-policies"
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/api-management/api-management-howto-aad.md",
    "Addition": 5,
    "Delections": 6,
    "Changes": 11,
    "Patch": "@@ -81,12 +81,12 @@ After the changes are saved, users in the specified Azure AD instance can sign i\n \n After you enable access for users in an Azure AD tenant, you can add Azure AD groups into API Management. As a result, you can control product visibility using Azure AD groups.\n \n-To add an external Azure AD group into APIM, you must first complete the previous section. Additionally, the application you registered must be granted access to the Azure Active Directory Graph API with `Directory.ReadAll` permission by following below steps: \n+To add an external Azure AD group into APIM, you must first complete the previous section. Additionally, the application you registered must be granted access to the Microsoft Graph API with `Directory.Read.All` permission by following these steps: \n \n-1. Go back to your App Registration that was created in the previous section\n-2. Click on the **API Permissions** tab, then click **+Add a permission** button \n-3. In the **Request API Permissions** pane, select the **Microsoft APIs** tab, and scroll to the bottom to find the **Azure Active Directory Graph** tile under the Supported Legacy APIs section and click it. Then click **APPLICATION Permissions** button, and select **Directory.ReadAll** permission and then add that permission using button at the bottom. \n-4. Click the **Grant admin consent for {tenantname}** button so that you grant access for all users in this directory. \n+1. Go back to your App Registration that was created in the previous section.\n+2. Select **API Permissions**, and then click **+Add a permission**. \n+3. In the **Request API Permissions** pane, select the **Microsoft APIs** tab, and then select the **Microsoft Graph** tile. Select **Application permissions**, search for **Directory**, and then select the **Directory.Read.All** permission. \n+4. Click **Add permissions** at the bottom of the pane, and then click **Grant admin consent for {tenantname}** so that you grant access for all users in this directory. \n \n Now you can add external Azure AD groups from the **Groups** tab of your API Management instance.\n \n@@ -150,7 +150,6 @@ Your user is now signed in to the developer portal for your API Management servi\n \n [https://oauth.net/2/]: https://oauth.net/2/\n [WebApp-GraphAPI-DotNet]: https://github.com/AzureADSamples/WebApp-GraphAPI-DotNet\n-[Accessing the Graph API]: https://msdn.microsoft.com/library/azure/dn132599.aspx#BKMK_Graph\n \n [Prerequisites]: #prerequisites\n [Configure an OAuth 2.0 authorization server in API Management]: #step1"
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/api-management/howto-protect-backend-frontend-azure-ad-b2c.md",
    "Addition": 0,
    "Delections": 3,
    "Changes": 3,
    "Patch": "@@ -462,8 +462,5 @@ The steps above can be adapted and edited to allow many different uses of Azure\n * Learn more about [Azure Active Directory and OAuth2.0](../active-directory/develop/authentication-scenarios.md).\n * Check out more [videos](https://azure.microsoft.com/documentation/videos/index/?services=api-management) about API Management.\n * For other ways to secure your back-end service, see [Mutual Certificate authentication](api-management-howto-mutual-certificates.md).\n-* Consider using the Azure AD Graph API to assign custom claims and using an API Management policy to validate they're present in the token.\n-\n * [Create an API Management service instance](get-started-create-service-instance.md).\n-\n * [Manage your first API](import-and-publish.md)."
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/app-service/environment/firewall-integration.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -135,7 +135,7 @@ With an Azure Firewall, you automatically get everything below configured with t\n \n | Endpoint |\n |----------|\n-|graph.windows.net:443 |\n+|graph.microsoft.com:443 |\n |login.live.com:443 |\n |login.windows.com:443 |\n |login.windows.net:443 |"
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/app-service/networking/private-endpoint.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -44,7 +44,7 @@ From the security perspective:\n - The NIC of the Private Endpoint cannot have an NSG associated\n - The Subnet that hosts the Private Endpoint can have an NSG associated, but you must disable the network policies enforcement for the Private Endpoint see [this article][disablesecuritype]. As a result, you cannot filter by any NSG the access to your Private Endpoint\n - When you enable Private Endpoint to your Web App, the [access restrictions][accessrestrictions] configuration of the Web App is not evaluated.\n-- You can reduce data exfiltration risk from the vnet by removing all NSG rules where destination is tag Internet or Azure services. But adding a Web App Service Endpoint in your subnet, will let you reach any Web App hosted in the same stamp and exposed to Internet.\n+- You can reduce data exfiltration risk from the vnet by removing all NSG rules where destination is tag Internet or Azure services. But adding a Web App Service Endpoint in your subnet, will let you reach any Web App hosted in the same stamp and exposed to Internet. (This is only a limitation of the current preview.)\n \n Private Endpoint for Web App is available for tier PremiumV2, and Isolated with an external ASE.\n "
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/app-service/overview-authentication-authorization.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -54,7 +54,7 @@ For more information, see [Access user claims](app-service-authentication-how-to\n App Service provides a built-in token store, which is a repository of tokens that are associated with the users of your web apps, APIs, or native mobile apps. When you enable authentication with any provider, this token store is immediately available to your app. If your application code needs to access data from these providers on the user's behalf, such as: \n \n - post to the authenticated user's Facebook timeline\n-- read the user's corporate data from the Azure Active Directory Graph API or even the Microsoft Graph\n+- read the user's corporate data using the Microsoft Graph API\n \n You typically must write code to collect, store, and refresh these tokens in your application. With the token store, you just [retrieve the tokens](app-service-authentication-how-to.md#retrieve-tokens-in-app-code) when you need them and [tell App Service to refresh them](app-service-authentication-how-to.md#refresh-identity-provider-tokens) when they become invalid. \n "
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/app-service/overview-security.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -60,7 +60,7 @@ App Service authentication and authorization support multiple authentication pro\n When authenticating against a back-end service, App Service provides two different mechanisms depending on your need:\n \n - **Service identity** - Sign in to the remote resource using the identity of the app itself. App Service lets you easily create a [managed identity](overview-managed-identity.md), which you can use to authenticate with other services, such as [Azure SQL Database](/azure/sql-database/) or [Azure Key Vault](/azure/key-vault/). For an end-to-end tutorial of this approach, see [Secure Azure SQL Database connection from App Service using a managed identity](app-service-web-tutorial-connect-msi.md).\n-- **On-behalf-of (OBO)** - Make delegated access to remote resources on behalf of the user. With Azure Active Directory as the authentication provider, your App Service app can perform delegated sign-in to a remote service, such as [Azure Active Directory Graph API](../active-directory/develop/active-directory-graph-api.md) or a remote API app in App Service. For an end-to-end tutorial of this approach, see [Authenticate and authorize users end-to-end in Azure App Service](app-service-web-tutorial-auth-aad.md).\n+- **On-behalf-of (OBO)** - Make delegated access to remote resources on behalf of the user. With Azure Active Directory as the authentication provider, your App Service app can perform delegated sign-in to a remote service, such as [Microsoft Graph API](../active-directory/develop/microsoft-graph-intro.md) or a remote API app in App Service. For an end-to-end tutorial of this approach, see [Authenticate and authorize users end-to-end in Azure App Service](app-service-web-tutorial-auth-aad.md).\n \n ## Connectivity to remote resources\n "
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/azure-cache-for-redis/security-baseline.md",
    "Addition": 1230,
    "Delections": 0,
    "Changes": 1230,
    "Patch": "@@ -0,0 +1,1230 @@\n+---\n+title: Azure Security Baseline for Azure Cache for Redis\n+description: Azure Security Baseline for Azure Cache for Redis\n+author: msmbaldwin\n+ms.service: security\n+ms.topic: conceptual\n+ms.date: 03/16/2020\n+ms.author: mbaldwin\n+ms.custom: security-benchmark\n+\n+---\n+\n+# Azure Security Baseline for Azure Cache for Redis\n+\n+The Azure Security Baseline for Azure Cache for Redis contains recommendations that will help you improve the security posture of your deployment.\n+\n+The baseline for this service is drawn from the [Azure Security Benchmark version 1.0](https://docs.microsoft.com/azure/security/benchmarks/overview), which provides recommendations on how you can secure your cloud solutions on Azure with our best practices guidance.\n+\n+For more information, see [Azure Security Baselines overview](https://docs.microsoft.com/azure/security/benchmarks/security-baselines-overview).\n+\n+## Network Security\n+\n+*For more information, see [Security Control: Network Security](https://docs.microsoft.com/azure/security/benchmarks/security-control-network-security).*\n+\n+### 1.1: Protect resources using Network Security Groups or Azure Firewall on your Virtual Network\n+\n+**Guidance**: Deploy your Azure Cache for Redis instance within a virtual network (VNet). A VNet is a private network in the cloud. When an Azure Cache for Redis instance is configured with a VNet, it is not publicly addressable and can only be accessed from virtual machines and applications within the VNet.\n+\n+You may also specify firewall rules with a start and end IP address range. When firewall rules are configured, only client connections from the specified IP address ranges can connect to the cache.\n+\n+How to configure Virtual Network Support for a Premium Azure Cache for Redis:\n+\n+https://docs.microsoft.com/azure/azure-cache-for-redis/cache-how-to-premium-vnet\n+\n+How to configure Azure Cache for Redis firewall rules:\n+\n+https://docs.microsoft.com/azure/azure-cache-for-redis/cache-configure#firewall\n+\n+**Azure Security Center monitoring**: Currently not available\n+\n+**Responsibility**: Customer\n+\n+### 1.2: Monitor and log the configuration and traffic of Vnets, Subnets, and NICs\n+\n+**Guidance**: When Virtual Machines are deployed in the same virtual network as your Azure Cache for Redis instance, you can use network security groups (NSG) to reduce the risk of data exfiltration. Enable NSG flow logs and send logs into an Azure Storage Account for traffic audit. You may also send NSG flow logs to a Log Analytics workspace and use Traffic Analytics to provide insights into traffic flow in your Azure cloud. Some advantages of Traffic Analytics are the ability to visualize network activity and identify hot spots, identify security threats, understand traffic flow patterns, and pinpoint network misconfigurations.\n+\n+How to Enable NSG Flow Logs:\n+\n+https://docs.microsoft.com/azure/network-watcher/network-watcher-nsg-flow-logging-portal\n+\n+How to Enable and use Traffic Analytics:\n+\n+https://docs.microsoft.com/azure/network-watcher/traffic-analytics\n+\n+**Azure Security Center monitoring**: Yes\n+\n+**Responsibility**: Customer\n+\n+### 1.3: Protect critical web applications\n+\n+**Guidance**: Not applicable; this recommendation is intended for web applications running on Azure App Service or compute resources.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 1.4: Deny communications with known malicious IP addresses\n+\n+**Guidance**: Azure Virtual Network (VNet) deployment provides enhanced security and isolation for your Azure Cache for Redis, as well as subnets, access control policies, and other features to further restrict access. When deployed in a VNet, Azure Cache for Redis is not publicly addressable and can only be accessed from virtual machines and applications within the VNet.\n+\n+Enable DDoS Protection Standard on the VNets associated with your Azure Cache for Redis instances to guard against distributed denial-of-service (DDoS) attacks. Use Azure Security Center Integrated Threat Intelligence to deny communications with known malicious or unused Internet IP addresses.\n+\n+How to configure Virtual Network Support for a Premium Azure Cache for Redis:\n+\n+https://docs.microsoft.com/azure/azure-cache-for-redis/cache-how-to-premium-vnet\n+\n+Manage Azure DDoS Protection Standard using the Azure portal:\n+\n+https://docs.microsoft.com/azure/virtual-network/manage-ddos-protection\n+\n+**Azure Security Center monitoring**: Yes\n+\n+**Responsibility**: Customer\n+\n+### 1.5: Record network packets and flow logs\n+\n+**Guidance**: When virtual machines are deployed in the same virtual network as your Azure Cache for Redis instance, you can use network security groups (NSG) to reduce the risk of data exfiltration. Enable NSG flow logs and send logs into an Azure Storage Account for traffic audit. You may also send NSG flow logs to a Log Analytics workspace and use Traffic Analytics to provide insights into traffic flow in your Azure cloud. Some advantages of Traffic Analytics are the ability to visualize network activity and identify hot spots, identify security threats, understand traffic flow patterns, and pinpoint network misconfigurations.\n+\n+How to Enable NSG Flow Logs:\n+\n+https://docs.microsoft.com/azure/network-watcher/network-watcher-nsg-flow-logging-portal\n+\n+How to Enable and use Traffic Analytics:\n+\n+https://docs.microsoft.com/azure/network-watcher/traffic-analytics\n+\n+**Azure Security Center monitoring**: Yes\n+\n+**Responsibility**: Customer\n+\n+### 1.6: Deploy network based intrusion detection/intrusion prevention systems (IDS/IPS)\n+\n+**Guidance**: When using Azure Cache for Redis with your web applications running on Azure App Service or compute instances, deploy all resources within an Azure Virtual Network (VNet) and secure with an Azure Web Application Firewall (WAF) on Web Application Gateway. Configure the WAF to run in \"Prevention Mode\". Prevention Mode blocks intrusions and attacks that the rules detect. The attacker receives a \"403 unauthorized access\" exception, and the connection is closed. Prevention mode records such attacks in the WAF logs.\n+\n+Alternatively, you may select an offer from the Azure Marketplace that supports IDS/IPS functionality with payload inspection and/or anomaly detection capabilities.\n+\n+Understand Azure WAF capabilities:\n+\n+https://docs.microsoft.com/azure/web-application-firewall/ag/ag-overview\n+\n+How to deploy Azure WAF:\n+\n+https://docs.microsoft.com/azure/web-application-firewall/ag/create-waf-policy-ag\n+\n+Azure Marketplace:\n+\n+https://azuremarketplace.microsoft.com/marketplace/?term=Firewall\n+\n+**Azure Security Center monitoring**: Currently not available\n+\n+**Responsibility**: Customer\n+\n+### 1.7: Manage traffic to web applications\n+\n+**Guidance**: Not applicable; this recommendation is intended for web applications running on Azure App Service or compute resources.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 1.8: Minimize complexity and administrative overhead of network security rules\n+\n+**Guidance**: Use virtual network service tags to define network access controls on network security groups (NSG) or Azure Firewall. You can use service tags in place of specific IP addresses when creating security rules. By specifying the service tag name (e.g., ApiManagement) in the appropriate source or destination field of a rule, you can allow or deny the traffic for the corresponding service. Microsoft manages the address prefixes encompassed by the service tag and automatically updates the service tag as addresses change.\n+\n+You may also use application security groups (ASG) to help simplify complex security configuration. ASGs enable you to configure network security as a natural extension of an application's structure, allowing you to group virtual machines and define network security policies based on those groups.\n+\n+Virtual network service tags:\n+\n+https://docs.microsoft.com/azure/virtual-network/service-tags-overview\n+\n+Application Security Groups:\n+\n+https://docs.microsoft.com/azure/virtual-network/security-overview#application-security-groups\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 1.9: Maintain standard security configurations for network devices\n+\n+**Guidance**: Define and implement standard security configurations for network resources related to your Azure Cache for Redis instances with Azure Policy. Use Azure Policy aliases in the \"Microsoft.Cache\" and \"Microsoft.Network\" namespaces to create custom policies to audit or enforce the network configuration of your Azure Cache for Redis instances. You may also make use of built-in policy definitions such as:\n+\n+Only secure connections to your Redis Cache should be enabled\n+\n+DDoS Protection Standard should be enabled\n+\n+You may also use Azure Blueprints to simplify large scale Azure deployments by packaging key environment artifacts, such as Azure Resource Manager (ARM) templates, role-based access control (RBAC), and policies, in a single blueprint definition. Easily apply the blueprint to new subscriptions and environments, and fine-tune control and management through versioning.\n+\n+How to configure and manage Azure Policy:\n+\n+https://docs.microsoft.com/azure/governance/policy/tutorials/create-and-manage\n+\n+How to create an Azure Blueprint:\n+\n+https://docs.microsoft.com/azure/governance/blueprints/create-blueprint-portal\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 1.10: Document traffic configuration rules\n+\n+**Guidance**: Use tags for network resources associated with your Azure Cache for Redis deployment in order to logically organize them into a taxonomy.\n+\n+How to create and use tags:\n+\n+https://docs.microsoft.com/azure/azure-resource-manager/resource-group-using-tags\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 1.11: Use automated tools to monitor network resource configurations and detect changes\n+\n+**Guidance**: Use the Azure Activity log to monitor network resource configurations and detect changes for network resources related to your Azure Cache for Redis instances. Create alerts within Azure Monitor that will trigger when changes to critical network resources take place.\n+\n+How to view and retrieve Azure Activity Log events:\n+\n+https://docs.microsoft.com/azure/azure-monitor/platform/activity-log-view\n+\n+How to create alerts in Azure Monitor:\n+\n+https://docs.microsoft.com/azure/azure-monitor/platform/alerts-activity-log\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+## Logging and Monitoring\n+\n+*For more information, see [Security Control: Logging and Monitoring](https://docs.microsoft.com/azure/security/benchmarks/security-control-logging-monitoring).*\n+\n+### 2.1: Use approved time synchronization sources\n+\n+**Guidance**: Microsoft maintains the time source used for Azure resources such as Azure Cache for Redis for timestamps in the logs.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Microsoft\n+\n+### 2.2: Configure central security log management\n+\n+**Guidance**: Enable Azure Activity Log diagnostic settings and send the logs to a Log Analytics workspace, Azure event hub, or Azure storage account for archive. Activity logs provide insight into the operations that were performed on your Azure Cache for Redis instances at the control plane level. Using Azure Activity Log data, you can determine the \"what, who, and when\" for any write operations (PUT, POST, DELETE) performed at the control plane level for your Azure Cache for Redis instances.\n+\n+How to enable Diagnostic Settings for Azure Activity Log: https://docs.microsoft.com/azure/azure-monitor/platform/diagnostic-settings-legacy\n+\n+**Azure Security Center monitoring**: Currently not available\n+\n+**Responsibility**: Customer\n+\n+### 2.3: Enable audit logging for Azure resources\n+\n+**Guidance**: Enable Azure Activity Log diagnostic settings and send the logs to a Log Analytics workspace, Azure event hub, or Azure storage account for archive. Activity logs provide insight into the operations that were performed on your Azure Cache for Redis instances at the control plane level. Using Azure Activity Log data, you can determine the \"what, who, and when\" for any write operations (PUT, POST, DELETE) performed at the control plane level for your Azure Cache for Redis instances.\n+\n+While metrics are available by enabling Diagnostic Settings, audit logging at the data plane is not yet available for Azure Cache for Redis.\n+\n+How to enable Diagnostic Settings for Azure Activity Log: https://docs.microsoft.com/azure/azure-monitor/platform/diagnostic-settings-legacy\n+\n+**Azure Security Center monitoring**: Currently not available\n+\n+**Responsibility**: Customer\n+\n+### 2.4: Collect security logs from operating systems\n+\n+**Guidance**: Not applicable; this recommendation is intended for compute resources.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+### 2.5: Configure security log storage retention\n+\n+**Guidance**: In Azure Monitor, set log retention period for Log Analytics workspaces associated with your Azure Cache for Redis instances according to your organization's compliance regulations.\n+\n+Note that audit logging at the data plane is not yet available for Azure Cache for Redis.\n+\n+How to set log retention parameters:\n+\n+https://docs.microsoft.com/azure/azure-monitor/platform/manage-cost-storage#change-the-data-retention-period\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 2.6: Monitor and review Logs\n+\n+**Guidance**: Enable Azure Activity Log diagnostic settings and send the logs to a Log Analytics workspace. Perform queries in Log Analytics to search terms, identify trends, analyze patterns, and provide many other insights based on the Activity Log Data that may have been collected for Azure Cache for Redis.\n+\n+Note that audit logging at the data plane is not yet available for Azure Cache for Redis.\n+\n+How to enable Diagnostic Settings for Azure Activity Log: https://docs.microsoft.com/azure/azure-monitor/platform/diagnostic-settings-legacy\n+\n+How to collect and analyze Azure activity logs in Log Analytics workspace in Azure Monitor: https://docs.microsoft.com/azure/azure-monitor/platform/activity-log-collect\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 2.7: Enable alerts for anomalous activity\n+\n+**Guidance**: You can configure to receive alerts based on metrics and activity logs related to your Azure Cache for Redis instances. Azure Monitor allows you to configure an alert to send an email notification, call a webhook, or invoke an Azure Logic App.\n+\n+While metrics are available by enabling Diagnostic Settings, audit logging at the data plane is not yet available for Azure Cache for Redis.\n+\n+How to configure alerts for Azure Cache for Redis:\n+https://docs.microsoft.com/azure/azure-cache-for-redis/cache-how-to-monitor#alerts\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 2.8: Centralize anti-malware logging\n+\n+**Guidance**: Not applicable; Azure Cache for Redis does not process or produce anti-malware related logs.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+### 2.9: Enable DNS query logging\n+\n+**Guidance**: Not applicable; Azure Cache for Redis does not process or produce DNS related logs.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+### 2.10: Enable command-line audit logging\n+\n+**Guidance**: Not applicable; this guideline is intended for compute resources.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+## Identity and Access Control\n+\n+*For more information, see [Security Control: Identity and Access Control](https://docs.microsoft.com/azure/security/benchmarks/security-control-identity-access-control).*\n+\n+### 3.1: Maintain an inventory of administrative accounts\n+\n+**Guidance**: Azure Active Directory (AD) has built-in roles that must be explicitly assigned and are queryable. Use the Azure AD PowerShell module to perform ad hoc queries to discover accounts that are members of administrative groups.\n+\n+How to get a directory role in Azure AD with PowerShell:\n+https://docs.microsoft.com/powershell/module/azuread/get-azureaddirectoryrole?view=azureadps-2.0\n+\n+How to get members of a directory role in Azure AD with PowerShell:\n+https://docs.microsoft.com/powershell/module/azuread/get-azureaddirectoryrolemember?view=azureadps-2.0\n+\n+**Azure Security Center monitoring**: Yes\n+\n+**Responsibility**: Customer\n+\n+### 3.2: Change default passwords where applicable\n+\n+**Guidance**: Control plane access to Azure Cache for Redis is controlled through Azure Active Directory (AD). Azure AD does not have the concept of default passwords. \n+\n+Data plane access to Azure Cache for Redis is controlled through access keys. These keys are used by the clients connecting to your cache and can be regenerated at any time.\n+\n+It is not recommended that you build default passwords into your application. Instead, you can store your passwords in Azure Key Vault and then use Azure Active Directory to retrieve them.\n+\n+How to regenerate Azure Cache for Redis access keys:\n+https://docs.microsoft.com/azure/azure-cache-for-redis/cache-configure#settings\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Shared\n+\n+### 3.3: Use dedicated administrative accounts\n+\n+**Guidance**: Create standard operating procedures around the use of dedicated administrative accounts. Use Azure Security Center Identity and Access Management to monitor the number of administrative accounts.\n+\n+Additionally, to help you keep track of dedicated administrative accounts, you may use recommendations from Azure Security Center or built-in Azure Policies, such as:\n+\n+- There should be more than one owner assigned to your subscription\n+\n+- Deprecated accounts with owner permissions should be removed from your subscription\n+\n+- External accounts with owner permissions should be removed from your subscription\n+\n+How to use Azure Security Center to monitor identity and access (Preview): https://docs.microsoft.com/azure/security-center/security-center-identity-access\n+\n+How to use Azure Policy: https://docs.microsoft.com/azure/governance/policy/tutorials/create-and-manage\n+\n+\n+**Azure Security Center monitoring**: Yes\n+\n+**Responsibility**: Customer\n+\n+### 3.4: Use single sign-on (SSO) with Azure Active Directory\n+\n+**Guidance**: Azure Cache for Redis uses access keys to authenticate users and does not support single sign-on (SSO) at the data plane level. Access to the control plane for Azure Cache for Redis is available via REST API and supports SSO. To authenticate, set the Authorization header for your requests to a JSON Web Token that you obtain from Azure Active Directory.\n+\n+Understand Azure Cache for Redis REST API: https://docs.microsoft.com/rest/api/redis/\n+\n+Understand SSO with Azure AD: https://docs.microsoft.com/azure/active-directory/manage-apps/what-is-single-sign-on\n+\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 3.5: Use multi-factor authentication for all Azure Active Directory based access\n+\n+**Guidance**: Enable Azure Active Directory (AD) Multi-Factor Authentication (MFA) and follow Azure Security Center Identity and Access Management recommendations.\n+\n+How to enable MFA in Azure:\n+https://docs.microsoft.com/azure/active-directory/authentication/howto-mfa-getstarted\n+\n+How to monitor identity and access within Azure Security Center:\n+https://docs.microsoft.com/azure/security-center/security-center-identity-access\n+\n+**Azure Security Center monitoring**: Yes\n+\n+**Responsibility**: Customer\n+\n+### 3.6: Use dedicated machines (Privileged Access Workstations) for all administrative tasks\n+\n+**Guidance**: Use privileged access workstations (PAW) with Multi-Factor Authentication (MFA) configured to log into and configure Azure resources.\n+\n+Learn about Privileged Access Workstations:\n+\n+https://docs.microsoft.com/windows-server/identity/securing-privileged-access/privileged-access-workstations\n+\n+How to enable MFA in Azure:\n+\n+https://docs.microsoft.com/azure/active-directory/authentication/howto-mfa-getstarted\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 3.7: Log and alert on suspicious activity from administrative accounts\n+\n+**Guidance**: Use Azure Active Directory (AD) Privileged Identity Management (PIM) for generation of logs and alerts when suspicious or unsafe activity occurs in the environment.\n+\n+In addition, use Azure AD risk detections to view alerts and reports on risky user behavior.\n+\n+How to deploy Privileged Identity Management (PIM):\n+https://docs.microsoft.com/azure/active-directory/privileged-identity-management/pim-deployment-plan\n+\n+Understand Azure AD risk detections:\n+https://docs.microsoft.com/azure/active-directory/reports-monitoring/concept-risk-events\n+\n+**Azure Security Center monitoring**: Yes\n+\n+**Responsibility**: Customer\n+\n+### 3.8: Manage Azure resources from only approved locations\n+\n+**Guidance**: Configure named locations in Azure Active Directory (AD) Conditional Access to allow access from only specific logical groupings of IP address ranges or countries/regions.\n+\n+How to configure Named Locations in Azure:\n+https://docs.microsoft.com/azure/active-directory/reports-monitoring/quickstart-configure-named-locations\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 3.9: Use Azure Active Directory\n+\n+**Guidance**: Use Azure Active Directory (AD) as the central authentication and authorization system. Azure AD protects data by using strong encryption for data at rest and in transit. Azure AD also salts, hashes, and securely stores user credentials.\n+\n+Azure AD authentication cannot be used for direct access to Azure Cache for Redis' data plane, however, Azure AD credentials may be used for administration at the control plane level (i.e. the Azure portal) to control Azure Cache for Redis access keys.\n+\n+\n+**Azure Security Center monitoring**: Yes\n+\n+**Responsibility**: Customer\n+\n+### 3.10: Regularly review and reconcile user access\n+\n+**Guidance**: Azure Active Directory (AD) provides logs to help you discover stale accounts. In addition, use Azure Identity Access Reviews to efficiently manage group memberships, access to enterprise applications, and role assignments. User access can be reviewed on a regular basis to make sure only the right Users have continued access. \n+\n+Understand Azure AD reporting:\n+https://docs.microsoft.com/azure/active-directory/reports-monitoring/\n+\n+How to use Azure Identity Access Reviews:\n+https://docs.microsoft.com/azure/active-directory/governance/access-reviews-overview\n+\n+**Azure Security Center monitoring**: Yes\n+\n+**Responsibility**: Customer\n+\n+### 3.11: Monitor attempts to access deactivated accounts\n+\n+**Guidance**: You have access to Azure Active Directory (AD) sign-in activity, audit and risk event log sources, which allow you to integrate with Azure Sentinel or a third-party SIEM.\n+\n+You can streamline this process by creating diagnostic settings for Azure AD user accounts and sending the audit logs and sign-in logs to a Log Analytics workspace. You can configure desired log alerts within Log Analytics.\n+\n+How to integrate Azure Activity Logs into Azure Monitor: https://docs.microsoft.com/azure/active-directory/reports-monitoring/howto-integrate-activity-logs-with-log-analytics\n+\n+How to on-board Azure Sentinel: https://docs.microsoft.com/azure/sentinel/quickstart-onboard\n+\n+**Azure Security Center monitoring**: Currently not available\n+\n+**Responsibility**: Customer\n+\n+### 3.12: Alert on account login behavior deviation\n+\n+**Guidance**: For account login behavior deviation on the control plane, use Azure Active Directory (AD) Identity Protection and risk detection features to configure automated responses to detected suspicious actions related to user identities. You can also ingest data into Azure Sentinel for further investigation.\n+\n+How to view Azure AD risky sign-ins:\n+https://docs.microsoft.com/azure/active-directory/reports-monitoring/concept-risky-sign-ins\n+\n+How to configure and enable Identity Protection risk policies:\n+https://docs.microsoft.com/azure/active-directory/identity-protection/howto-identity-protection-configure-risk-policies\n+\n+How to onboard Azure Sentinel:https://docs.microsoft.com/azure/sentinel/quickstart-onboard\n+\n+**Azure Security Center monitoring**: Currently not available\n+\n+**Responsibility**: Customer\n+\n+### 3.13: Provide Microsoft with access to relevant customer data during support scenarios\n+\n+**Guidance**: Not yet available; Customer Lockbox is not yet supported for Azure Cache for Redis.\n+\n+List of Customer Lockbox-supported services:\n+\n+https://docs.microsoft.com/azure/security/fundamentals/customer-lockbox-overview#supported-services-and-scenarios-in-general-availability\n+\n+**Azure Security Center monitoring**: Currently not available\n+\n+**Responsibility**: Not applicable\n+\n+## Data Protection\n+\n+*For more information, see [Security Control: Data Protection](https://docs.microsoft.com/azure/security/benchmarks/security-control-data-protection).*\n+\n+### 4.1: Maintain an inventory of sensitive Information\n+\n+**Guidance**: Use tags to assist in tracking Azure resources that store or process sensitive information.\n+\n+How to create and use tags:\n+\n+https://docs.microsoft.com/azure/azure-resource-manager/resource-group-using-tags\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 4.2: Isolate systems storing or processing sensitive information\n+\n+**Guidance**: Implement separate subscriptions and/or management groups for development, test, and production. Azure Cache for Redis instances should be separated by virtual network/subnet and tagged appropriately. Optionally, use the Azure Cache for Redis firewall to define rules so that only client connections from specified IP address ranges can connect to the cache.\n+\n+How to create additional Azure subscriptions:\n+\n+https://docs.microsoft.com/azure/billing/billing-create-subscription\n+\n+How to create Management Groups:\n+\n+https://docs.microsoft.com/azure/governance/management-groups/create\n+\n+How to deploy Azure Cache for Redis into a Vnet:\n+\n+https://docs.microsoft.com/azure/azure-cache-for-redis/cache-how-to-premium-vnet\n+\n+How to configure Azure Cache for Redis firewall rules:\n+\n+https://docs.microsoft.com/azure/azure-cache-for-redis/cache-configure#firewall\n+\n+How to create and use tags:\n+\n+https://docs.microsoft.com/azure/azure-resource-manager/resource-group-using-tags\n+\n+**Azure Security Center monitoring**: Currently not available\n+\n+**Responsibility**: Customer\n+\n+### 4.3: Monitor and block unauthorized transfer of sensitive information\n+\n+**Guidance**: Not yet available; data identification, classification, and loss prevention features are not yet available for Azure Cache for Redis.\n+\n+Microsoft manages the underlying infrastructure for Azure Cache for Redis and has implemented strict controls to prevent the loss or exposure of customer data.\n+\n+Understand customer data protection in Azure:\n+https://docs.microsoft.com/azure/security/fundamentals/protection-customer-data\n+\n+**Azure Security Center monitoring**: Currently not available\n+\n+**Responsibility**: Shared\n+\n+### 4.4: Encrypt all sensitive information in transit\n+\n+**Guidance**: Azure Cache for Redis requires TLS-encrypted communications by default. TLS versions 1.0, 1.1 and 1.2 are currently supported. However, TLS 1.0 and 1.1 are on a path to deprecation industry-wide, so use TLS 1.2 if at all possible. If your client library or tool doesn't support TLS, then enabling unencrypted connections can be done through the Azure portal or management APIs. In such cases where encrypted connections aren't possible, placing your cache and client application into a virtual network would be recommended.\n+\n+Understand encryption in transit for Azure Cache for Redis:\n+\n+https://docs.microsoft.com/azure/azure-cache-for-redis/cache-best-practices\n+\n+Understand required ports used in Vnet cache scenarios:\n+\n+https://docs.microsoft.com/azure/azure-cache-for-redis/cache-how-to-premium-vnet#outbound-port-requirements\n+\n+**Azure Security Center monitoring**: Yes\n+\n+**Responsibility**: Shared\n+\n+### 4.5: Use an active discovery tool to identify sensitive data\n+\n+**Guidance**: Data identification, classification, and loss prevention features are not yet available for Azure Cache for Redis. Tag instances containing sensitive information as such and implement third-party solution if required for compliance purposes.\n+\n+For the underlying platform which is managed by Microsoft, Microsoft treats all customer content as sensitive and goes to great lengths to guard against customer data loss and exposure. To ensure customer data within Azure remains secure, Microsoft has implemented and maintains a suite of robust data protection controls and capabilities.\n+\n+Understand customer data protection in Azure:\n+https://docs.microsoft.com/azure/security/fundamentals/protection-customer-data\n+\n+**Azure Security Center monitoring**: Currently not available\n+\n+**Responsibility**: Customer\n+\n+### 4.6: Use Azure RBAC to control access to resources\n+\n+**Guidance**: Use Azure Active Directory (AAD) role-based access control (RBAC) to control access to the Azure Cache for Redis control plane (i.e. Azure portal). \n+\n+How to configure RBAC in Azure:\n+\n+https://docs.microsoft.com/azure/role-based-access-control/role-assignments-portal\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 4.7: Use host-based data loss prevention to enforce access control\n+\n+**Guidance**: Not applicable; this recommendation is intended for compute resources.\n+\n+Microsoft manages the underlying infrastructure for Azure Cache for Redis and has implemented strict controls to prevent the loss or exposure of customer data.\n+\n+Understand customer data protection in Azure:\n+\n+https://docs.microsoft.com/azure/security/fundamentals/protection-customer-data\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+### 4.8: Encrypt sensitive information at rest\n+\n+**Guidance**: Azure Cache for Redis stores customer data in memory, and while strongly protected by many controls implemented by Microsoft, memory is not encrypted by default. If required by your organization, encrypt content before storing in Azure Cache for Redis.\n+\n+If using the Azure Cache for Redis feature \"Redis Data Persistence\", data is sent to an Azure Storage account that you own and manage. You can configure persistence from the \"New Azure Cache for Redis\" blade during cache creation and on the Resource menu for existing premium caches.\n+\n+Data in Azure Storage is encrypted and decrypted transparently using 256-bit AES encryption, one of the strongest block ciphers available, and is FIPS 140-2 compliant. Azure Storage encryption cannot be disabled. You can rely on Microsoft-managed keys for the encryption of your storage account, or you can manage encryption with your own keys.\n+\n+How to configure persistence in Azure Cache for Redis: https://docs.microsoft.com/azure/azure-cache-for-redis/cache-how-to-premium-persistence\n+\n+Understand encryption for Azure Storage accounts: https://docs.microsoft.com/azure/storage/common/storage-service-encryption\n+\n+Understand Azure customer data protection: https://docs.microsoft.com/azure/security/fundamentals/protection-customer-data\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Shared\n+\n+### 4.9: Log and alert on changes to critical Azure resources\n+\n+**Guidance**: Use Azure Monitor with the Azure Activity log to create alerts for when changes take place to production instances of Azure Cache for Redis and other critical or related resources.\n+\n+How to create alerts for Azure Activity Log events:\n+\n+https://docs.microsoft.com/azure/azure-monitor/platform/alerts-activity-log\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+## Vulnerability Management\n+\n+*For more information, see [Security Control: Vulnerability Management](https://docs.microsoft.com/azure/security/benchmarks/security-control-vulnerability-management).*\n+\n+### 5.1: Run automated vulnerability scanning tools\n+\n+**Guidance**: Follow recommendations from Azure Security Center on securing your Azure Cache for Redis instances and related resources.\n+\n+Microsoft performs vulnerability management on the underlying systems that support Azure Cache for Redis.\n+\n+Understand Azure Security Center recommendations:\n+https://docs.microsoft.com/azure/security-center/recommendations-reference\n+\n+**Azure Security Center monitoring**: Yes\n+\n+**Responsibility**: Shared\n+\n+### 5.2: Deploy automated operating system patch management solution\n+\n+**Guidance**: Not applicable; this recommendation is intended for compute resources.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+### 5.3: Deploy automated third-party software patch management solution\n+\n+**Guidance**: Not applicable; this recommendation is intended for compute resources.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+### 5.4: Compare back-to-back vulnerability scans\n+\n+**Guidance**: Not applicable; this recommendation is intended for compute resources.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+### 5.5: Use a risk-rating process to prioritize the remediation of discovered vulnerabilities\n+\n+**Guidance**: Microsoft performs vulnerability management on the underlying systems that support Azure Cache for Redis.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Microsoft\n+\n+## Inventory and Asset Management\n+\n+*For more information, see [Security Control: Inventory and Asset Management](https://docs.microsoft.com/azure/security/benchmarks/security-control-inventory-asset-management).*\n+\n+### 6.1: Use Azure Asset Discovery\n+\n+**Guidance**: Use Azure Resource Graph to query/discover all resources (such as compute, storage, network, ports, and protocols etc.) within your subscription(s).  Ensure appropriate (read) permissions in your tenant and enumerate all Azure subscriptions as well as resources within your subscriptions.\n+\n+Although classic Azure resources may be discovered via Resource Graph, it is highly recommended to create and use Azure Resource Manager resources going forward.\n+\n+How to create queries with Azure Resource Graph:\n+https://docs.microsoft.com/azure/governance/resource-graph/first-query-portal\n+\n+How to view your Azure Subscriptions:\n+https://docs.microsoft.com/powershell/module/az.accounts/get-azsubscription?view=azps-3.0.0\n+\n+Understand Azure RBAC:\n+https://docs.microsoft.com/azure/role-based-access-control/overview\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 6.2: Maintain asset metadata\n+\n+**Guidance**: Apply tags to Azure resources giving metadata to logically organize them into a taxonomy.\n+\n+How to create and use tags:\n+\n+https://docs.microsoft.com/azure/azure-resource-manager/resource-group-using-tags\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 6.3: Delete unauthorized Azure resources\n+\n+**Guidance**: Use tagging, management groups, and separate subscriptions, where appropriate, to organize and track Azure Cache for Redis instances and related resources. Reconcile inventory on a regular basis and ensure unauthorized resources are deleted from the subscription in a timely manner.\n+\n+In addition, use Azure policy to put restrictions on the type of resources that can be created in customer subscription(s) using the following built-in policy definitions:\n+\n+- Not allowed resource types\n+\n+- Allowed resource types\n+\n+How to create additional Azure subscriptions: https://docs.microsoft.com/azure/billing/billing-create-subscription\n+\n+How to create Management Groups: https://docs.microsoft.com/azure/governance/management-groups/create\n+\n+How to create and use Tags: https://docs.microsoft.com/azure/azure-resource-manager/resource-group-using-tags\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 6.4: Maintain an inventory of approved Azure resources and software titles\n+\n+**Guidance**: Not applicable; this recommendation is intended for compute resources and Azure as a whole.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+### 6.5: Monitor for unapproved Azure resources\n+\n+**Guidance**: Use Azure policy to put restrictions on the type of resources that can be created in customer subscription(s) using the following built-in policy definitions:\n+\n+Not allowed resource types\n+\n+Allowed resource types\n+\n+In addition, use Azure Resource Graph to query/discover resources within the subscription(s).\n+\n+How to configure and manage Azure Policy:\n+\n+https://docs.microsoft.com/azure/governance/policy/tutorials/create-and-manage\n+\n+How to create queries with Azure Graph:\n+\n+https://docs.microsoft.com/azure/governance/resource-graph/first-query-portal\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 6.6: Monitor for unapproved software applications within compute resources\n+\n+**Guidance**: Not applicable; this recommendation is intended for compute resources.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+### 6.7: Remove unapproved Azure resources and software applications\n+\n+**Guidance**: Not applicable; this recommendation is intended for compute resources and Azure as a whole.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+### 6.8: Use only approved applications\n+\n+**Guidance**: Not applicable; this recommendation is intended for compute resources.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+### 6.9: Use only approved Azure services\n+\n+**Guidance**: Use Azure Policy to put restrictions on the type of resources that can be created in customer subscription(s) using the following built-in policy definitions:\n+\n+Not allowed resource types\n+\n+Allowed resource types\n+\n+How to configure and manage Azure Policy:\n+\n+https://docs.microsoft.com/azure/governance/policy/tutorials/create-and-manage\n+\n+How to deny a specific resource type with Azure Policy:\n+\n+https://docs.microsoft.com/azure/governance/policy/samples/not-allowed-resource-types\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 6.10: Implement approved application list\n+\n+**Guidance**: Not applicable; this recommendation is intended for compute resources.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+### 6.11: Limit users' ability to interact with Azure Resources Manager via scripts\n+\n+**Guidance**: Configure Azure Conditional Access to limit users' ability to interact with Azure Resource Manager (ARM) by configuring \"Block access\" for the \"Microsoft Azure Management\" App.\n+\n+How to configure Conditional Access to block access to ARM:\n+\n+https://docs.microsoft.com/azure/role-based-access-control/conditional-access-azure-management\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 6.12: Limit users' ability to execute scripts within compute resources\n+\n+**Guidance**: Not applicable; this recommendation is intended for compute resources.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+### 6.13: Physically or logically segregate high risk applications\n+\n+**Guidance**: Not applicable; this recommendation is intended for web applications running on Azure App Service or compute resources.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+## Secure Configuration\n+\n+*For more information, see [Security Control: Secure Configuration](https://docs.microsoft.com/azure/security/benchmarks/security-control-secure-configuration).*\n+\n+### 7.1: Establish secure configurations for all Azure resources\n+\n+**Guidance**: Define and implement standard security configurations for your Azure Cache for Redis instances with Azure Policy. Use Azure Policy aliases in the \"Microsoft.Cache\" namespace to create custom policies to audit or enforce the configuration of your Azure Cache for Redis instances. You may also make use of built-in policy definitions related to your Azure Cache for Redis instances, such as:\n+\n+Only secure connections to your Redis Cache should be enabled\n+\n+How to view available Azure Policy Aliases:\n+https://docs.microsoft.com/powershell/module/az.resources/get-azpolicyalias?view=azps-3.3.0\n+\n+How to configure and manage Azure Policy:\n+https://docs.microsoft.com/azure/governance/policy/tutorials/create-and-manage\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 7.2: Establish secure operating system configurations\n+\n+**Guidance**: Not applicable; this guideline is intended for compute resources.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+### 7.3: Maintain secure Azure resource configurations\n+\n+**Guidance**: Use Azure policy [deny] and [deploy if not exist] to enforce secure settings across your Azure resources.\n+\n+How to configure and manage Azure Policy:\n+https://docs.microsoft.com/azure/governance/policy/tutorials/create-and-manage\n+\n+Understand Azure Policy Effects:\n+https://docs.microsoft.com/azure/governance/policy/concepts/effects\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 7.4: Maintain secure operating system configurations\n+\n+**Guidance**: Not applicable; this guideline is intended for compute resources.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+### 7.5: Securely store configuration of Azure resources\n+\n+**Guidance**: If using custom Azure Policy definitions or Azure Resource Manager templates for your Azure Cache for Redis instances and related resources, use Azure Repos to securely store and manage your code.\n+\n+How to store code in Azure DevOps: https://docs.microsoft.com/azure/devops/repos/git/gitworkflow?view=azure-devops\n+\n+Azure Repos Documentation: https://docs.microsoft.com/azure/devops/repos/index?view=azure-devops\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 7.6: Securely store custom operating system images\n+\n+**Guidance**: Not applicable; this recommendation is intended for compute resources.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+### 7.7: Deploy system configuration management tools\n+\n+**Guidance**: Use Azure Policy aliases in the \"Microsoft.Cache\" namespace to create custom policies to alert, audit, and enforce system configurations. Additionally, develop a process and pipeline for managing policy exceptions.\n+\n+How to configure and manage Azure Policy:\n+https://docs.microsoft.com/azure/governance/policy/tutorials/create-and-manage\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 7.8: Deploy system configuration management tools for operating systems\n+\n+**Guidance**: Not applicable; this recommendation is intended for compute resources.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+### 7.9: Implement automated configuration monitoring for Azure services\n+\n+**Guidance**: Use Azure Policy aliases in the \"Microsoft.Cache\" namespace to create custom policies to alert, audit, and enforce system configurations. Use Azure policy [audit], [deny], and [deploy if not exist] to automatically enforce configurations for your Azure Cache for Redis instances and related resources.\n+\n+How to configure and manage Azure Policy:\n+https://docs.microsoft.com/azure/governance/policy/tutorials/create-and-manage\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 7.10: Implement automated configuration monitoring for operating systems\n+\n+**Guidance**: Not applicable; this recommendation is intended for compute resources.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+### 7.11: Manage Azure secrets securely\n+\n+**Guidance**: For Azure virtual machines or web applications running on Azure App Service being used to access your Azure Cache for Redis instances, use Managed Service Identity in conjunction with Azure Key Vault to simplify and secure Azure Cache for Redis secret management. Ensure Key Vault soft delete is enabled.\n+\n+How to integrate with Azure Managed Identities:\n+\n+https://docs.microsoft.com/azure/azure-app-configuration/howto-integrate-azure-managed-service-identity\n+\n+How to create a Key Vault:\n+\n+https://docs.microsoft.com/azure/key-vault/quick-create-portal\n+\n+How to provide Key Vault authentication with a managed identity:\n+\n+https://docs.microsoft.com/azure/key-vault/managed-identity\n+\n+**Azure Security Center monitoring**: Yes\n+\n+**Responsibility**: Customer\n+\n+### 7.12: Manage identities securely and automatically\n+\n+**Guidance**: For Azure virtual machines or web applications running on Azure App Service being used to access your Azure Cache for Redis instances, use Managed Service Identity in conjunction with Azure Key Vault to simplify and secure Azure Cache for Redis secret management. Ensure Key Vault Soft Delete is enabled.\n+\n+Use Managed Identities to provide Azure services with an automatically managed identity in Azure Active Directory. Managed Identities allows you to authenticate to any service that supports AAD authentication, including Azure Key Vault, without any credentials in your code.\n+\n+How to configure Managed Identities:\n+\n+https://docs.microsoft.com/azure/active-directory/managed-identities-azure-resources/qs-configure-portal-windows-vm\n+\n+How to integrate with Azure Managed Identities:\n+\n+https://docs.microsoft.com/azure/azure-app-configuration/howto-integrate-azure-managed-service-identity\n+\n+**Azure Security Center monitoring**: Yes\n+\n+**Responsibility**: Customer\n+\n+### 7.13: Eliminate unintended credential exposure\n+\n+**Guidance**: Implement Credential Scanner to identify credentials within code. Credential Scanner will also encourage moving discovered credentials to more secure locations such as Azure Key Vault.\n+\n+How to setup Credential Scanner:\n+https://secdevtools.azurewebsites.net/helpcredscan.html\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+## Malware Defense\n+\n+*For more information, see [Security Control: Malware Defense](https://docs.microsoft.com/azure/security/benchmarks/security-control-malware-defense).*\n+\n+### 8.1: Use centrally managed anti-malware software\n+\n+**Guidance**: Not applicable; this recommendation is intended for compute resources.\n+\n+Microsoft anti-malware is enabled on the underlying host that supports Azure services (for example, Azure App Service), however it does not run on customer content.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+### 8.2: Pre-scan files to be uploaded to non-compute Azure resources\n+\n+**Guidance**: Microsoft anti-malware is enabled on the underlying host that supports Azure services (for example, Azure Cache for Redis), however it does not run on customer content.\n+\n+Pre-scan any content being uploaded to non-compute Azure resources, such as App Service, Data Lake Storage, Blob Storage, Azure Database for PostgreSQL, etc. Microsoft cannot access your data in these instances.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 8.3: Ensure anti-malware software and signatures are updated\n+\n+**Guidance**: Not applicable; this recommendation is intended for compute resources.\n+\n+Microsoft anti-malware is enabled on the underlying host that supports Azure services (for example, Azure Cache for Redis), however it does not run on customer content.\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Not applicable\n+\n+## Data Recovery\n+\n+*For more information, see [Security Control: Data Recovery](https://docs.microsoft.com/azure/security/benchmarks/security-control-data-recovery).*\n+\n+### 9.1: Ensure regular automated back ups\n+\n+**Guidance**: Enable Redis persistence. Redis persistence allows you to persist data stored in Redis. You can also take snapshots and back up the data, which you can load in case of a hardware failure. This is a huge advantage over Basic or Standard tier where all the data is stored in memory and there can be potential data loss in case of a failure where Cache nodes are down.\n+\n+You may also use Azure Cache for Redis Export. Export allows you to export the data stored in Azure Cache for Redis to Redis compatible RDB file(s). You can use this feature to move data from one Azure Cache for Redis instance to another or to another Redis server. During the export process, a temporary file is created on the virtual machine that hosts the Azure Cache for Redis server instance, and the file is uploaded to the designated storage account. When the export operation completes with either a status of success or failure, the temporary file is deleted.\n+\n+How to enable Redis persistence:\n+\n+https://docs.microsoft.com/azure/azure-cache-for-redis/cache-how-to-premium-persistence\n+\n+How to use Azure Cache for Redis Export:\n+\n+https://docs.microsoft.com/azure/azure-cache-for-redis/cache-how-to-import-export-data\n+\n+**Azure Security Center monitoring**: Currently not available\n+\n+**Responsibility**: Customer\n+\n+### 9.2: Perform complete system backups and backup any customer managed keys\n+\n+**Guidance**: Enable Redis persistence. Redis persistence allows you to persist data stored in Redis. You can also take snapshots and back up the data, which you can load in case of a hardware failure. This is a huge advantage over Basic or Standard tier where all the data is stored in memory and there can be potential data loss in case of a failure where Cache nodes are down.\n+\n+You may also use Azure Cache for Redis Export. Export allows you to export the data stored in Azure Cache for Redis to Redis compatible RDB file(s). You can use this feature to move data from one Azure Cache for Redis instance to another or to another Redis server. During the export process, a temporary file is created on the virtual machine that hosts the Azure Cache for Redis server instance, and the file is uploaded to the designated storage account. When the export operation completes with either a status of success or failure, the temporary file is deleted.\n+\n+If using Azure Key Vault to store credentials for your Azure Cache for Redis instances, ensure regular automated backups of your keys.\n+\n+How to enable Redis persistence:\n+\n+https://docs.microsoft.com/azure/azure-cache-for-redis/cache-how-to-premium-persistence\n+\n+How to use Azure Cache for Redis Export:\n+\n+https://docs.microsoft.com/azure/azure-cache-for-redis/cache-how-to-import-export-data\n+\n+How to backup Key Vault Keys:\n+\n+https://docs.microsoft.com/powershell/module/azurerm.keyvault/backup-azurekeyvaultkey\n+\n+**Azure Security Center monitoring**: Currently not available\n+\n+**Responsibility**: Customer\n+\n+### 9.3: Validate all backups including customer managed keys\n+\n+**Guidance**: Use Azure Cache for Redis Import. Import can be used to bring Redis-compatible RDB files from any Redis server running in any cloud or environment, including Redis running on Linux, Windows, or any cloud provider such as Amazon Web Services and others. Importing data is an easy way to create a cache with pre-populated data. During the import process, Azure Cache for Redis loads the RDB files from Azure storage into memory and then inserts the keys into the cache.\n+\n+Periodically test data restoration of your Azure Key Vault secrets.\n+\n+How to use Azure Cache for Redis Import:\n+\n+https://docs.microsoft.com/azure/azure-cache-for-redis/cache-how-to-import-export-data\n+\n+How to restore Key Vault Secrets:\n+\n+https://docs.microsoft.com/powershell/module/azurerm.keyvault/restore-azurekeyvaultsecret?view=azurermps-6.13.0\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 9.4: Ensure protection of backups and customer managed keys\n+\n+**Guidance**: Azure Cache for Redis backups from Redis Export and Redis persistence are stored within your selected Azure Storage account. Data in Azure Storage is encrypted and decrypted transparently using 256-bit AES encryption, one of the strongest block ciphers available, and is FIPS 140-2 compliant. Azure Storage encryption cannot be disabled. You can rely on Microsoft-managed keys for the encryption of your storage account, or you can manage encryption with your own keys.\n+\n+Understand encryption for Azure Storage accounts:\n+https://docs.microsoft.com/azure/storage/common/storage-service-encryption\n+\n+**Azure Security Center monitoring**: Yes\n+\n+**Responsibility**: Microsoft\n+\n+## Incident Response\n+\n+*For more information, see [Security Control: Incident Response](https://docs.microsoft.com/azure/security/benchmarks/security-control-incident-response).*\n+\n+### 10.1: Create an incident response guide\n+\n+**Guidance**: Build out an incident response guide for your organization. Ensure that there are written incident response plans that define all roles of personnel as well as phases of incident handling/management from detection to post-incident review.\n+\n+How to configure Workflow Automations within Azure Security Center:\n+\n+https://docs.microsoft.com/azure/security-center/security-center-planning-and-operations-guide\n+\n+Guidance on building your own security incident response process:\n+\n+https://msrc-blog.microsoft.com/2019/07/01/inside-the-msrc-building-your-own-security-incident-response-process/\n+\n+Microsoft Security Response Center's Anatomy of an Incident:\n+\n+https://msrc-blog.microsoft.com/2019/07/01/inside-the-msrc-building-your-own-security-incident-response-process/\n+\n+Customer may also leverage NIST's Computer Security Incident Handling Guide to aid in the creation of their own incident response plan:\n+\n+https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-61r2.pdf\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 10.2: Create an incident scoring and prioritization procedure\n+\n+**Guidance**: Security Center assigns a severity to each alert to help you prioritize which alerts should be investigated first. The severity is based on how confident Security Center is in the finding or the analytic used to issue the alert as well as the confidence level that there was malicious intent behind the activity that led to the alert.\n+\n+Additionally, clearly mark subscriptions (for ex. production, non-prod) and create a naming system to clearly identify and categorize Azure resources.\n+\n+**Azure Security Center monitoring**: Yes\n+\n+**Responsibility**: Customer\n+\n+### 10.3: Test security response procedures\n+\n+**Guidance**: Conduct exercises to test your systems’ incident response capabilities on a regular cadence. Identify weak points and gaps and revise plan as needed.\n+\n+Refer to NIST's publication: Guide to Test, Training, and Exercise Programs for IT Plans and Capabilities:\n+\n+https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-84.pdf\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 10.4: Provide security incident contact details and configure alert notifications for security incidents\n+\n+**Guidance**: Security incident contact information will be used by Microsoft to contact you if the Microsoft Security Response Center (MSRC) discovers that the customer's data has been accessed by an unlawful or unauthorized party.  Review incidents after the fact to ensure that issues are resolved.\n+\n+How to set the Azure Security Center Security Contact:\n+\n+https://docs.microsoft.com/azure/security-center/security-center-provide-security-contact-details\n+\n+**Azure Security Center monitoring**: Yes\n+\n+**Responsibility**: Customer\n+\n+### 10.5: Incorporate security alerts into your incident response system\n+\n+**Guidance**: Export your Azure Security Center alerts and recommendations using the Continuous Export feature. Continuous Export allows you to export alerts and recommendations either manually or in an ongoing, continuous fashion. You may use the Azure Security Center data connector to stream the alerts Sentinel.\n+\n+How to configure continuous export:\n+\n+https://docs.microsoft.com/azure/security-center/continuous-export\n+\n+How to stream alerts into Azure Sentinel:\n+\n+https://docs.microsoft.com/azure/sentinel/connect-azure-security-center\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+### 10.6: Automate the response to security alerts\n+\n+**Guidance**: Use the Workflow Automation feature in Azure Security Center to automatically trigger responses via \"Logic Apps\" on security alerts and recommendations.\n+\n+How to configure Workflow Automation and Logic Apps:\n+\n+https://docs.microsoft.com/azure/security-center/workflow-automation\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Customer\n+\n+## Penetration Tests and Red Team Exercises\n+\n+*For more information, see [Security Control: Penetration Tests and Red Team Exercises](https://docs.microsoft.com/azure/security/benchmarks/security-control-penetration-tests-red-team-exercises).*\n+\n+### 11.1: Conduct regular penetration testing of your Azure resources and ensure remediation of all critical security findings within 60 days\n+\n+**Guidance**: Follow the Microsoft Rules of Engagement to ensure your Penetration Tests are not in violation of Microsoft policies:\n+\n+https://www.microsoft.com/msrc/pentest-rules-of-engagement?rtc=1\n+\n+You can find more information on Microsoft’s strategy and execution of Red Teaming and live site penetration testing against Microsoft-managed cloud infrastructure, services, and applications, here: \n+\n+https://gallery.technet.microsoft.com/Cloud-Red-Teaming-b837392e\n+\n+**Azure Security Center monitoring**: Not applicable\n+\n+**Responsibility**: Shared\n+\n+## Next steps\n+\n+- See the [Azure Security Benchmark](https://docs.microsoft.com/azure/security/benchmarks/overview)\n+- Learn more about [Azure Security Baselines](https://docs.microsoft.com/azure/security/benchmarks/security-baselines-overview)"
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/azure-government/documentation-government-developer-guide.md",
    "Addition": 0,
    "Delections": 1,
    "Changes": 1,
    "Patch": "@@ -80,7 +80,6 @@ The following table shows the mapping between some Azure services and Azure Gove\n | Azure Cloud Shell |  https:\\//portal.azure.us | https:\\//portal.azure.com |\n | Active Directory Endpoint and Authority | https:\\//login.microsoftonline.us | https:\\//login.microsoftonline.com <br/> https:\\//login.windows.net |\n | Active Directory tenant names | [yourtenantname].onmicrosoft.com | [yourtenantname].onmicrosoft.com |\n-| Active Directory Graph API | https:\\//graph.windows.net/ | https:\\//graph.windows.net/ |\n | Microsoft Graph API | https:\\//graph.microsoft.us/ | https:\\//graph.microsoft.com/ |\n | Azure API | https:\\//management.usgovcloudapi.net/ | https:\\//management.azure.com/ |\n | SQL Database DNS Suffix | \\*.database.usgovcloudapi.net | \\*.database.windows.net |"
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/azure-government/documentation-government-services-securityandidentity.md",
    "Addition": 0,
    "Delections": 1,
    "Changes": 1,
    "Patch": "@@ -145,7 +145,6 @@ The URLs for accessing Azure Active Directory in Azure Government are different:\n | Service Type | Azure Public | Azure Government |\n | --- | --- | --- |\n | Active Directory Endpoint and Authority | https://login.microsoftonline.com | https://login.microsoftonline.us |\n-| Active Directory Graph API| https://graph.windows.net/ | https://graph.windows.net/ |\n \n ## Azure Active Directory Premium P1 and P2\n "
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/azure-maps/data-driven-style-expressions-web-sdk.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -67,7 +67,7 @@ All examples in this document use the following feature to demonstrate different\n         \"subTitle\": \"Building 40\", \n         \"temperature\": 72,\n         \"title\": \"Cafeteria\", \n-\t\t\"zoneColor\": \"red\"\n+        \"zoneColor\": \"red\"\n \t}\n }\n ```"
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/azure-maps/map-add-shape.md",
    "Addition": 3,
    "Delections": 3,
    "Changes": 6,
    "Patch": "@@ -35,11 +35,11 @@ dataSource.add(new atlas.data.Feature(\n \t]])\n ));\n \n-//Create and add a polygon layer to render the polygon to the map.\n+//Create and add a polygon layer to render the polygon to the map, below the label layer.\n map.layers.add(new atlas.layer.PolygonLayer(dataSource, null,{\n \tfillColor: 'red',\n-\topacaty: 0.5\n-}));\n+\tfillOpacity: 0.7\n+}), 'labels');\n ```\n \n Below is the complete and running sample of the above code."
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/azure-monitor/app/change-analysis.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -33,7 +33,7 @@ See the *Viewing changes for all resources in Azure* section to access Change An\n Using [Azure Resource Graph](https://docs.microsoft.com/azure/governance/resource-graph/overview), Change Analysis provides a historical record of how the Azure resources that host your application have changed over time. Tracked settings such as managed identities, Platform OS upgrade, and hostnames can be detected.\n \n ### Azure Resource Manager proxied setting changes\n-Settings such as IP Configuration rule, SSL settings, and extension versions are not yet available in ARG, so Change Analysis queries and computes these changes securely to provide more details in what changed in the app. These information is not available yet in Azure Resource Graph but will be available soon.\n+Settings such as IP Configuration rule, SSL settings, and extension versions are not yet available in ARG, so Change Analysis queries and computes these changes securely to provide more details in what changed in the app. This information is not available yet in Azure Resource Graph but will be available soon.\n \n ### Changes in web app deployment and configuration (in-guest changes)\n "
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/azure-monitor/insights/network-performance-monitor.md",
    "Addition": 2,
    "Delections": 1,
    "Changes": 3,
    "Patch": "@@ -35,7 +35,8 @@ NPM can monitor connectivity between networks and applications in any part of th\n * North Europe\n * West Europe\n * France Central\n-\n+* Canada Central\n+* West US\n * West Central US\n * North Central US\n * South Central US"
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/azure-monitor/log-query/examples.md",
    "Addition": 40,
    "Delections": 32,
    "Changes": 72,
    "Patch": "@@ -5,7 +5,7 @@ ms.subservice: logs\n ms.topic: conceptual\n author: bwren\n ms.author: bwren\n-ms.date: 10/01/2019\n+ms.date: 03/16/2020\n \n ---\n \n@@ -349,12 +349,12 @@ Using **join**, and **let** statements we can check if the same suspicious accou\n ```Kusto\n let timeframe = 1d;\n let suspicious_users = \n-\tSecurityEvent\n-\t| where TimeGenerated > ago(timeframe)\n-\t| where AccountType == 'User' and EventID == 4625 // 4625 - failed login\n-\t| summarize failed_login_attempts=count(), latest_failed_login=arg_max(TimeGenerated, Account) by Account \n-\t| where failed_login_attempts > 5\n-\t| project-away Account1;\n+    SecurityEvent\n+    | where TimeGenerated > ago(timeframe)\n+    | where AccountType == 'User' and EventID == 4625 // 4625 - failed login\n+    | summarize failed_login_attempts=count(), latest_failed_login=arg_max(TimeGenerated, Account) by Account \n+    | where failed_login_attempts > 5\n+    | project-away Account1;\n let suspicious_users_that_later_logged_in = \n     suspicious_users \n     | join kind=innerunique (\n@@ -371,41 +371,49 @@ suspicious_users_that_later_logged_in\n \n ## Usage\n \n-### Calculate the average size of perf usage reports per computer\n+The `Usage` data type can be used to track the ingested data volume by solution or data type. There are other techniques to study ingested data volumes by [computer](https://docs.microsoft.com/azure/azure-monitor/platform/manage-cost-storage#data-volume-by-computer) or [Azure subscription, resource group or resource](https://docs.microsoft.com/azure/azure-monitor/platform/manage-cost-storage#data-volume-by-azure-resource-resource-group-or-subscription).\n \n-This example calculates the average size of perf usage reports per computer, over the last 3 hours.\n-The results are shown in a bar chart.\n-```Kusto\n+#### Data volume by solution\n+\n+The query used to view the billable data volume by solution over the last month (excluding the last partial day) is:\n+\n+```kusto\n Usage \n-| where TimeGenerated > ago(3h)\n-| where DataType == \"Perf\" \n-| where QuantityUnit == \"MBytes\" \n-| summarize avg(Quantity) by Computer\n-| sort by avg_Quantity desc nulls last\n-| render barchart\n+| where TimeGenerated > ago(32d)\n+| where StartTime >= startofday(ago(31d)) and EndTime < startofday(now())\n+| where IsBillable == true\n+| summarize BillableDataGB = sum(Quantity) / 1000. by bin(StartTime, 1d), Solution | render barchart\n ```\n \n-### Timechart latency percentiles 50 and 95\n+Note that the clause `where IsBillable = true` filters out data types from certain solutions for which there is no ingestion charge.  Also the clause with `TimeGenerated` is only to ensure that the query experience in the Azure portal will look back beyond the default 24 hours. When using the Usage data type, `StartTime` and `EndTime` represent the time buckets for which results are presented. \n \n-This example calculates and charts the 50th and 95th percentiles of reported **avgLatency** by hour over the last 24 hours.\n+#### Data volume by type\n \n-```Kusto\n-Usage\n-| where TimeGenerated > ago(24h)\n-| summarize percentiles(AvgLatencyInSeconds, 50, 95) by bin(TimeGenerated, 1h) \n-| render timechart\n+You can drill in further to see data trends for by data type:\n+\n+```kusto\n+Usage \n+| where TimeGenerated > ago(32d)\n+| where StartTime >= startofday(ago(31d)) and EndTime < startofday(now())\n+| where IsBillable == true\n+| summarize BillableDataGB = sum(Quantity) / 1000. by bin(StartTime, 1d), DataType | render barchart\n ```\n \n-### Usage of specific computers today\n-This example retrieves **Usage** data from the last day for computer names that contains the string _ContosoFile_. The results are sorted by **TimeGenerated**.\n+Or to see a table by solution and type for the last month,\n \n-```Kusto\n-Usage\n-| where TimeGenerated > ago(1d)\n-| where  Computer contains \"ContosoFile\" \n-| sort by TimeGenerated desc nulls last\n+```kusto\n+Usage \n+| where TimeGenerated > ago(32d)\n+| where StartTime >= startofday(ago(31d)) and EndTime < startofday(now())\n+| where IsBillable == true\n+| summarize BillableDataGB = sum(Quantity) / 1000. by Solution, DataType\n+| sort by Solution asc, DataType asc\n ```\n \n+> [!NOTE]\n+> Some of the fields of the Usage data type, while still in the schema, have been deprecated and will their values are no longer populated. \n+> These are **Computer** as well as fields related to ingestion (**TotalBatches**, **BatchesWithinSla**, **BatchesOutsideSla**, **BatchesCapped** and **AverageProcessingTimeMs**.\n+\n ## Updates\n \n ### Computers Still Missing Updates\n@@ -427,4 +435,4 @@ Update\n ## Next steps\n \n - Refer to the [Kusto language reference](/azure/kusto/query) for details on the language.\n-- Walk through a [lesson on writing log queries in Azure Monitor](get-started-queries.md).\n+- Walk through a [lesson on writing log queries in Azure Monitor](get-started-queries.md).\n\\ No newline at end of file"
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/azure-monitor/platform/manage-cost-storage.md",
    "Addition": 39,
    "Delections": 16,
    "Changes": 55,
    "Patch": "@@ -11,7 +11,7 @@ ms.service: azure-monitor\n ms.workload: na\n ms.tgt_pltfrm: na\n ms.topic: conceptual\n-ms.date: 11/05/2019\n+ms.date: 03/16/2020\n ms.author: bwren\n ms.subservice: \n ---\n@@ -107,7 +107,7 @@ To set the default retention for your workspace,\n 3. On the pane, move the slider to increase or decrease the number of days and then click **OK**.  If you are on the *free* tier, you will not be able to modify the data retention period and you need to upgrade to the paid tier in order to control this setting.\n \n     ![Change workspace data retention setting](media/manage-cost-storage/manage-cost-change-retention-01.png)\n-\t\n+    \n The retention can also be [set via Azure Resource Manager](https://docs.microsoft.com/azure/azure-monitor/platform/template-workspace-configuration#configure-a-log-analytics-workspace) using the `retentionInDays` parameter. Additionally, if you set the data retention to 30 days, you can trigger an immediate purge of older data using the `immediatePurgeDataOn30Days` parameter, which may be useful for compliance-related scenarios. This functionality is only exposed via Azure Resource Manager. \n \n Two data types -- `Usage` and `AzureActivity` -- are retained for 90 days by default, and there is no charge for for this 90 day retention. These data types are also free from data ingestion charges. \n@@ -137,9 +137,9 @@ To set the retention of a particular data type (in this example SecurityEvent) t\n ```JSON\n     PUT /subscriptions/00000000-0000-0000-0000-00000000000/resourceGroups/MyResourceGroupName/providers/Microsoft.OperationalInsights/workspaces/MyWorkspaceName/Tables/SecurityEvent?api-version=2017-04-26-preview\n     {\n-\t    \"properties\": \n-\t\t{\n-\t\t    \"retentionInDays\": 730\n+        \"properties\": \n+        {\n+            \"retentionInDays\": 730\n         }\n     }\n ```\n@@ -169,15 +169,15 @@ When the daily limit is reached, the collection of billable data types stops for\n \n ### Identify what daily data limit to define\n \n-Review [Log Analytics Usage and estimated costs](usage-estimated-costs.md) to understand the data ingestion trend and what is the daily volume cap to define. It should be considered with care, since you won’t be able to monitor your resources after the limit is reached. \n+Review [Log Analytics Usage and estimated costs](usage-estimated-costs.md) to understand the data ingestion trend and what is the daily volume cap to define. It should be considered with care, since you won�t be able to monitor your resources after the limit is reached. \n \n ### Set the Daily Cap\n \n The following steps describe how to configure a limit to manage the volume of data that Log Analytics workspace will ingest per day.  \n \n 1. From your workspace, select **Usage and estimated costs** from the left pane.\n 2. On the **Usage and estimated costs** page for the selected workspace, click **Data volume management** from the top of the page. \n-3. Daily cap is **OFF** by default – click **ON** to enable it, and then set the data volume limit in GB/day.\n+3. Daily cap is **OFF** by default � click **ON** to enable it, and then set the data volume limit in GB/day.\n \n     ![Log Analytics configure data limit](media/manage-cost-storage/set-daily-volume-cap-01.png)\n \n@@ -217,10 +217,11 @@ Heartbeat\n | summarize nodes = dcount(Computer) by bin(TimeGenerated, 1d)    \n | render timechart\n ```\n-The get a count of nodes sending data seen can be determined using: \n+The get a count of nodes sending data in the last 24 hours use the query: \n \n ```kusto\n union withsource = tt * \n+| where TimeGenerated > ago(24h)\n | extend computerName = tolower(tostring(split(Computer, '.')[0]))\n | where computerName != \"\"\n | summarize nodes = dcount(computerName)\n@@ -230,6 +231,7 @@ To get a list of nodes sending any data (and the amount of data sent by each) th\n \n ```kusto\n union withsource = tt * \n+| where TimeGenerated > ago(24h)\n | extend computerName = tolower(tostring(split(Computer, '.')[0]))\n | where computerName != \"\"\n | summarize TotalVolumeBytes=sum(_BilledSize) by computerName\n@@ -242,35 +244,52 @@ union withsource = tt *\n \n On the **Usage and Estimated Costs** page, the *Data ingestion per solution* chart shows the total volume of data sent and how much is being sent by each solution. This allows you to determine trends such as whether the overall data usage (or usage by a particular solution) is growing, remaining steady or decreasing. \n \n+### Data volume for specific events\n+\n+To look at the size of ingested data for a particular set of events, you can query the specific table (in this example `Event`) and then restrict the query to the events of interest (in this example event ID 5145 or 5156):\n+\n+```kusto\n+Event\n+| where TimeGenerated > startofday(ago(31d)) and TimeGenerated < startofday(now()) \n+| where EventID == 5145 or EventID == 5156\n+| where _IsBillable == true\n+| summarize count(), Bytes=sum(_BilledSize) by EventID, bin(TimeGenerated, 1d)\n+``` \n+\n+Note that the clause `where IsBillable = true` filters out data types from certain solutions for which there is no ingestion charge. \n+\n ### Data volume by solution\n \n-The query used to view the billable data volume by solution is\n+The query used to view the billable data volume by solution over the last month (excluding the last partial day) is:\n \n ```kusto\n Usage \n-| where TimeGenerated > startofday(ago(31d))\n+| where TimeGenerated > ago(32d)\n+| where StartTime >= startofday(ago(31d)) and EndTime < startofday(now())\n | where IsBillable == true\n-| summarize BillableDataGB = sum(Quantity) / 1000. by bin(TimeGenerated, 1d), Solution | render barchart\n+| summarize BillableDataGB = sum(Quantity) / 1000. by bin(StartTime, 1d), Solution | render barchart\n ```\n \n-Note that the clause `where IsBillable = true` filters out data types from certain solutions for which there is no ingestion charge. \n+The clause with `TimeGenerated` is only to ensure that the query experience in the Azure portal will look back beyond the default 24 hours. When using the Usage data type, `StartTime` and `EndTime` represent the time buckets for which results are presented. \n \n ### Data volume by type\n \n You can drill in further to see data trends for by data type:\n \n ```kusto\n-Usage | where TimeGenerated > startofday(ago(31d))| where IsBillable == true\n-| where TimeGenerated > startofday(ago(31d))\n+Usage \n+| where TimeGenerated > ago(32d)\n+| where StartTime >= startofday(ago(31d)) and EndTime < startofday(now())\n | where IsBillable == true\n-| summarize BillableDataGB = sum(Quantity) / 1000. by bin(TimeGenerated, 1d), DataType | render barchart\n+| summarize BillableDataGB = sum(Quantity) / 1000. by bin(StartTime, 1d), DataType | render barchart\n ```\n \n Or to see a table by solution and type for the last month,\n \n ```kusto\n Usage \n-| where TimeGenerated > startofday(ago(31d))\n+| where TimeGenerated > ago(32d)\n+| where StartTime >= startofday(ago(31d)) and EndTime < startofday(now())\n | where IsBillable == true\n | summarize BillableDataGB = sum(Quantity) by Solution, DataType\n | sort by Solution asc, DataType asc\n@@ -282,6 +301,7 @@ The `Usage` data type does not include information at the completer level. To se\n \n ```kusto\n union withsource = tt * \n+| where TimeGenerated > ago(24h)\n | where _IsBillable == true \n | extend computerName = tolower(tostring(split(Computer, '.')[0]))\n | summarize BillableDataBytes = sum(_BilledSize) by  computerName | sort by Bytes nulls last\n@@ -293,6 +313,7 @@ To see the **count** of billable events ingested per computer, use\n \n ```kusto\n union withsource = tt * \n+| where TimeGenerated > ago(24h)\n | where _IsBillable == true \n | extend computerName = tolower(tostring(split(Computer, '.')[0]))\n | summarize eventCount = count() by computerName  | sort by eventCount nulls last\n@@ -304,6 +325,7 @@ For data from nodes hosted in Azure you can get the **size** of ingested data __\n \n ```kusto\n union withsource = tt * \n+| where TimeGenerated > ago(24h)\n | where _IsBillable == true \n | summarize BillableDataBytes = sum(_BilledSize) by _ResourceId | sort by Bytes nulls last\n ```\n@@ -312,6 +334,7 @@ For data from nodes hosted in Azure you can get the **size** of ingested data __\n \n ```kusto\n union withsource = tt * \n+| where TimeGenerated > ago(24h)\n | where _IsBillable == true \n | parse tolower(_ResourceId) with \"/subscriptions/\" subscriptionId \"/resourcegroups/\" \n     resourceGroup \"/providers/\" provider \"/\" resourceType \"/\" resourceName   "
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/azure-monitor/platform/metrics-supported.md",
    "Addition": 33,
    "Delections": 33,
    "Changes": 66,
    "Patch": "@@ -462,9 +462,9 @@ Azure Monitor provides several ways to interact with metrics, including charting\n \r\n |Metric|Metric Display Name|Unit|Aggregation Type|Description|Dimensions|\r\n |---|---|---|---|---|---|\r\n-|BlobCapacity|Blob Capacity|Bytes|Average|The amount of storage used by the storage account’s Blob service in bytes.|BlobType,Tier|\r\n-|BlobCount|Blob Count|Count|Average|The number of Blob in the storage account’s Blob service.|BlobType,Tier|\r\n-|ContainerCount|Blob Container Count|Count|Average|The number of containers in the storage account’s Blob service.|None|\r\n+|BlobCapacity|Blob Capacity|Bytes|Average|The amount of storage used by the storage account's Blob service in bytes.|BlobType,Tier|\r\n+|BlobCount|Blob Count|Count|Average|The number of Blob in the storage account's Blob service.|BlobType,Tier|\r\n+|ContainerCount|Blob Container Count|Count|Average|The number of containers in the storage account's Blob service.|None|\r\n |IndexCapacity|Index Capacity|Bytes|Average|The amount of storage used by ADLS Gen2 (Hierarchical) Index in bytes.|None|\r\n |Transactions|Transactions|Count|Total|The number of requests made to a storage service or the specified API operation. This number includes successful and failed requests, as well as requests which produced errors. Use ResponseType dimension for the number of different type of response.|ResponseType,GeoType,ApiName,Authentication|\r\n |Ingress|Ingress|Bytes|Total|The amount of ingress data, in bytes. This number includes ingress from an external client into Azure Storage as well as ingress within Azure.|GeoType,ApiName,Authentication|\r\n@@ -477,9 +477,9 @@ Azure Monitor provides several ways to interact with metrics, including charting\n \r\n |Metric|Metric Display Name|Unit|Aggregation Type|Description|Dimensions|\r\n |---|---|---|---|---|---|\r\n-|TableCapacity|Table Capacity|Bytes|Average|The amount of storage used by the storage account’s Table service in bytes.|None|\r\n-|TableCount|Table Count|Count|Average|The number of table in the storage account’s Table service.|None|\r\n-|TableEntityCount|Table Entity Count|Count|Average|The number of table entities in the storage account’s Table service.|None|\r\n+|TableCapacity|Table Capacity|Bytes|Average|The amount of storage used by the storage account's Table service in bytes.|None|\r\n+|TableCount|Table Count|Count|Average|The number of table in the storage account's Table service.|None|\r\n+|TableEntityCount|Table Entity Count|Count|Average|The number of table entities in the storage account's Table service.|None|\r\n |Transactions|Transactions|Count|Total|The number of requests made to a storage service or the specified API operation. This number includes successful and failed requests, as well as requests which produced errors. Use ResponseType dimension for the number of different type of response.|ResponseType,GeoType,ApiName,Authentication|\r\n |Ingress|Ingress|Bytes|Total|The amount of ingress data, in bytes. This number includes ingress from an external client into Azure Storage as well as ingress within Azure.|GeoType,ApiName,Authentication|\r\n |Egress|Egress|Bytes|Total|The amount of egress data, in bytes. This number includes egress from an external client into Azure Storage as well as egress within Azure. As a result, this number does not reflect billable egress.|GeoType,ApiName,Authentication|\r\n@@ -491,11 +491,11 @@ Azure Monitor provides several ways to interact with metrics, including charting\n \r\n |Metric|Metric Display Name|Unit|Aggregation Type|Description|Dimensions|\r\n |---|---|---|---|---|---|\r\n-|FileCapacity|File Capacity|Bytes|Average|The amount of storage used by the storage account’s File service in bytes.|FileShare|\r\n-|FileCount|File Count|Count|Average|The number of file in the storage account’s File service.|FileShare|\r\n-|FileShareCount|File Share Count|Count|Average|The number of file shares in the storage account’s File service.|None|\r\n-|FileShareSnapshotCount|File Share Snapshot Count|Count|Average|The number of snapshots present on the share in storage account’s Files Service.|FileShare|\r\n-|FileShareSnapshotSize|File Share Snapshot Size|Bytes|Average|The amount of storage used by the snapshots in storage account’s File service in bytes.|FileShare|\r\n+|FileCapacity|File Capacity|Bytes|Average|The amount of storage used by the storage account's File service in bytes.|FileShare|\r\n+|FileCount|File Count|Count|Average|The number of file in the storage account's File service.|FileShare|\r\n+|FileShareCount|File Share Count|Count|Average|The number of file shares in the storage account's File service.|None|\r\n+|FileShareSnapshotCount|File Share Snapshot Count|Count|Average|The number of snapshots present on the share in storage account's Files Service.|FileShare|\r\n+|FileShareSnapshotSize|File Share Snapshot Size|Bytes|Average|The amount of storage used by the snapshots in storage account's File service in bytes.|FileShare|\r\n |FileShareQuota|File share quota size|Bytes|Average|The upper limit on the amount of storage that can be used by Azure Files Service in bytes.|FileShare|\r\n |Transactions|Transactions|Count|Total|The number of requests made to a storage service or the specified API operation. This number includes successful and failed requests, as well as requests which produced errors. Use ResponseType dimension for the number of different type of response.|ResponseType,GeoType,ApiName,Authentication,FileShare|\r\n |Ingress|Ingress|Bytes|Total|The amount of ingress data, in bytes. This number includes ingress from an external client into Azure Storage as well as ingress within Azure.|GeoType,ApiName,Authentication,FileShare|\r\n@@ -508,9 +508,9 @@ Azure Monitor provides several ways to interact with metrics, including charting\n \r\n |Metric|Metric Display Name|Unit|Aggregation Type|Description|Dimensions|\r\n |---|---|---|---|---|---|\r\n-|QueueCapacity|Queue Capacity|Bytes|Average|The amount of storage used by the storage account’s Queue service in bytes.|None|\r\n-|QueueCount|Queue Count|Count|Average|The number of queue in the storage account’s Queue service.|None|\r\n-|QueueMessageCount|Queue Message Count|Count|Average|The approximate number of queue messages in the storage account’s Queue service.|None|\r\n+|QueueCapacity|Queue Capacity|Bytes|Average|The amount of storage used by the storage account's Queue service in bytes.|None|\r\n+|QueueCount|Queue Count|Count|Average|The number of queue in the storage account's Queue service.|None|\r\n+|QueueMessageCount|Queue Message Count|Count|Average|The approximate number of queue messages in the storage account's Queue service.|None|\r\n |Transactions|Transactions|Count|Total|The number of requests made to a storage service or the specified API operation. This number includes successful and failed requests, as well as requests which produced errors. Use ResponseType dimension for the number of different type of response.|ResponseType,GeoType,ApiName,Authentication|\r\n |Ingress|Ingress|Bytes|Total|The amount of ingress data, in bytes. This number includes ingress from an external client into Azure Storage as well as ingress within Azure.|GeoType,ApiName,Authentication|\r\n |Egress|Egress|Bytes|Total|The amount of egress data, in bytes. This number includes egress from an external client into Azure Storage as well as egress within Azure. As a result, this number does not reflect billable egress.|GeoType,ApiName,Authentication|\r\n@@ -1176,7 +1176,7 @@ Azure Monitor provides several ways to interact with metrics, including charting\n |Metric|Metric Display Name|Unit|Aggregation Type|Description|Dimensions|\r\n |---|---|---|---|---|---|\r\n |CacheUtilization|Cache utilization|Percent|Average|Utilization level in the cluster scope|None|\r\n-|QueryDuration|Query duration|Milliseconds|Average|Queries’ duration in seconds|QueryStatus|\r\n+|QueryDuration|Query duration|Milliseconds|Average|Queries' duration in seconds|QueryStatus|\r\n |IngestionUtilization|Ingestion utilization|Percent|Average|Ratio of used ingestion slots in the cluster|None|\r\n |KeepAlive|Keep alive|Count|Average|Sanity check indicates the cluster responds to queries|None|\r\n |IngestionVolumeInMB|Ingestion volume (in MB)|Count|Total|Overall volume of ingested data to the cluster (in MB)|Database|\r\n@@ -1844,8 +1844,8 @@ Azure Monitor provides several ways to interact with metrics, including charting\n |dwu_used|DWU used|Count|Maximum|DWU used. Applies only to data warehouses.|None|\r\n |cache_hit_percent|Cache hit percentage|Percent|Maximum|Cache hit percentage. Applies only to data warehouses.|None|\r\n |cache_used_percent|Cache used percentage|Percent|Maximum|Cache used percentage. Applies only to data warehouses.|None|\r\n-|sqlserver_process_core_percent|SQL Server process core percent|Percent|Maximum|CPU usage percentage for the SQL Server process, as measured by the operating system. Currently available for serverless databases only.|None|\r\n-|sqlserver_process_memory_percent|SQL Server process memory percent|Percent|Maximum|Memory usage percentage for the SQL Server process, as measured by the operating system. Currently available for serverless databases only.|None|\r\n+|sqlserver_process_core_percent|SQL Server process core percent|Percent|Maximum|CPU usage percentage for the SQL Server process, as measured by the operating system.|None|\r\n+|sqlserver_process_memory_percent|SQL Server process memory percent|Percent|Maximum|Memory usage percentage for the SQL Server process, as measured by the operating system.|None|\r\n |tempdb_data_size|Tempdb Data File Size Kilobytes|Count|Maximum|Tempdb Data File Size Kilobytes. Not applicable to data warehouses.|None|\r\n |tempdb_log_size|Tempdb Log File Size Kilobytes|Count|Maximum|Tempdb Log File Size Kilobytes. Not applicable to data warehouses.|None|\r\n |tempdb_log_used_percent|Tempdb Percent Log Used|Percent|Maximum|Tempdb Percent Log Used. Not applicable to data warehouses.|None|\r\n@@ -1888,8 +1888,8 @@ Azure Monitor provides several ways to interact with metrics, including charting\n |database_cpu_limit|CPU limit|Count|Average|CPU limit|DatabaseResourceId|\r\n |cpu_used|CPU used|Count|Average|CPU used. Applies to vCore-based elastic pools.|None|\r\n |database_cpu_used|CPU used|Count|Average|CPU used|DatabaseResourceId|\r\n-|sqlserver_process_core_percent|SQL Server process core percent|Percent|Maximum|CPU usage as a percentage of the SQL DB process. Applies to elastic pools.|None|\r\n-|sqlserver_process_memory_percent|SQL Server process memory percent|Percent|Maximum|Memory usage as a percentage of the SQL DB process. Applies to elastic pools.|None|\r\n+|sqlserver_process_core_percent|SQL Server process core percent|Percent|Maximum|CPU usage percentage for the SQL Server process, as measured by the operating system. Applies to elastic pools.|None|\r\n+|sqlserver_process_memory_percent|SQL Server process memory percent|Percent|Maximum|Memory usage percentage for the SQL Server process, as measured by the operating system. Applies to elastic pools.|None|\r\n |tempdb_data_size|Tempdb Data File Size Kilobytes|Count|Maximum|Tempdb Data File Size Kilobytes|None|\r\n |tempdb_log_size|Tempdb Log File Size Kilobytes|Count|Maximum|Tempdb Log File Size Kilobytes|None|\r\n |tempdb_log_used_percent|Tempdb Percent Log Used|Percent|Maximum|Tempdb Percent Log Used|None|\r\n@@ -1937,9 +1937,9 @@ Azure Monitor provides several ways to interact with metrics, including charting\n \r\n |Metric|Metric Display Name|Unit|Aggregation Type|Description|Dimensions|\r\n |---|---|---|---|---|---|\r\n-|BlobCapacity|Blob Capacity|Bytes|Average|The amount of storage used by the storage account’s Blob service in bytes.|BlobType,Tier|\r\n-|BlobCount|Blob Count|Count|Average|The number of Blob in the storage account’s Blob service.|BlobType,Tier|\r\n-|ContainerCount|Blob Container Count|Count|Average|The number of containers in the storage account’s Blob service.|None|\r\n+|BlobCapacity|Blob Capacity|Bytes|Average|The amount of storage used by the storage account's Blob service in bytes.|BlobType,Tier|\r\n+|BlobCount|Blob Count|Count|Average|The number of Blob in the storage account's Blob service.|BlobType,Tier|\r\n+|ContainerCount|Blob Container Count|Count|Average|The number of containers in the storage account's Blob service.|None|\r\n |IndexCapacity|Index Capacity|Bytes|Average|The amount of storage used by ADLS Gen2 (Hierarchical) Index in bytes.|None|\r\n |Transactions|Transactions|Count|Total|The number of requests made to a storage service or the specified API operation. This number includes successful and failed requests, as well as requests which produced errors. Use ResponseType dimension for the number of different type of response.|ResponseType,GeoType,ApiName,Authentication|\r\n |Ingress|Ingress|Bytes|Total|The amount of ingress data, in bytes. This number includes ingress from an external client into Azure Storage as well as ingress within Azure.|GeoType,ApiName,Authentication|\r\n@@ -1952,9 +1952,9 @@ Azure Monitor provides several ways to interact with metrics, including charting\n \r\n |Metric|Metric Display Name|Unit|Aggregation Type|Description|Dimensions|\r\n |---|---|---|---|---|---|\r\n-|TableCapacity|Table Capacity|Bytes|Average|The amount of storage used by the storage account’s Table service in bytes.|None|\r\n-|TableCount|Table Count|Count|Average|The number of table in the storage account’s Table service.|None|\r\n-|TableEntityCount|Table Entity Count|Count|Average|The number of table entities in the storage account’s Table service.|None|\r\n+|TableCapacity|Table Capacity|Bytes|Average|The amount of storage used by the storage account's Table service in bytes.|None|\r\n+|TableCount|Table Count|Count|Average|The number of table in the storage account's Table service.|None|\r\n+|TableEntityCount|Table Entity Count|Count|Average|The number of table entities in the storage account's Table service.|None|\r\n |Transactions|Transactions|Count|Total|The number of requests made to a storage service or the specified API operation. This number includes successful and failed requests, as well as requests which produced errors. Use ResponseType dimension for the number of different type of response.|ResponseType,GeoType,ApiName,Authentication|\r\n |Ingress|Ingress|Bytes|Total|The amount of ingress data, in bytes. This number includes ingress from an external client into Azure Storage as well as ingress within Azure.|GeoType,ApiName,Authentication|\r\n |Egress|Egress|Bytes|Total|The amount of egress data, in bytes. This number includes egress from an external client into Azure Storage as well as egress within Azure. As a result, this number does not reflect billable egress.|GeoType,ApiName,Authentication|\r\n@@ -1966,11 +1966,11 @@ Azure Monitor provides several ways to interact with metrics, including charting\n \r\n |Metric|Metric Display Name|Unit|Aggregation Type|Description|Dimensions|\r\n |---|---|---|---|---|---|\r\n-|FileCapacity|File Capacity|Bytes|Average|The amount of storage used by the storage account’s File service in bytes.|FileShare|\r\n-|FileCount|File Count|Count|Average|The number of file in the storage account’s File service.|FileShare|\r\n-|FileShareCount|File Share Count|Count|Average|The number of file shares in the storage account’s File service.|None|\r\n-|FileShareSnapshotCount|File share snapshot count|Count|Average|The number of snapshots present on the share in storage account’s Files Service.|FileShare|\r\n-|FileShareSnapshotSize|File share snapshot size|Bytes|Average|The amount of storage used by the snapshots in storage account’s File service in bytes.|FileShare|\r\n+|FileCapacity|File Capacity|Bytes|Average|The amount of storage used by the storage account's File service in bytes.|FileShare|\r\n+|FileCount|File Count|Count|Average|The number of file in the storage account's File service.|FileShare|\r\n+|FileShareCount|File Share Count|Count|Average|The number of file shares in the storage account's File service.|None|\r\n+|FileShareSnapshotCount|File share snapshot count|Count|Average|The number of snapshots present on the share in storage account's Files Service.|FileShare|\r\n+|FileShareSnapshotSize|File share snapshot size|Bytes|Average|The amount of storage used by the snapshots in storage account's File service in bytes.|FileShare|\r\n |FileShareQuota|File share quota size|Bytes|Average|The upper limit on the amount of storage that can be used by Azure Files Service in bytes.|FileShare|\r\n |Transactions|Transactions|Count|Total|The number of requests made to a storage service or the specified API operation. This number includes successful and failed requests, as well as requests which produced errors. Use ResponseType dimension for the number of different type of response.|ResponseType,GeoType,ApiName,Authentication,FileShare|\r\n |Ingress|Ingress|Bytes|Total|The amount of ingress data, in bytes. This number includes ingress from an external client into Azure Storage as well as ingress within Azure.|GeoType,ApiName,Authentication,FileShare|\r\n@@ -1983,9 +1983,9 @@ Azure Monitor provides several ways to interact with metrics, including charting\n \r\n |Metric|Metric Display Name|Unit|Aggregation Type|Description|Dimensions|\r\n |---|---|---|---|---|---|\r\n-|QueueCapacity|Queue Capacity|Bytes|Average|The amount of storage used by the storage account’s Queue service in bytes.|None|\r\n-|QueueCount|Queue Count|Count|Average|The number of queue in the storage account’s Queue service.|None|\r\n-|QueueMessageCount|Queue Message Count|Count|Average|The approximate number of queue messages in the storage account’s Queue service.|None|\r\n+|QueueCapacity|Queue Capacity|Bytes|Average|The amount of storage used by the storage account's Queue service in bytes.|None|\r\n+|QueueCount|Queue Count|Count|Average|The number of queue in the storage account's Queue service.|None|\r\n+|QueueMessageCount|Queue Message Count|Count|Average|The approximate number of queue messages in the storage account's Queue service.|None|\r\n |Transactions|Transactions|Count|Total|The number of requests made to a storage service or the specified API operation. This number includes successful and failed requests, as well as requests which produced errors. Use ResponseType dimension for the number of different type of response.|ResponseType,GeoType,ApiName,Authentication|\r\n |Ingress|Ingress|Bytes|Total|The amount of ingress data, in bytes. This number includes ingress from an external client into Azure Storage as well as ingress within Azure.|GeoType,ApiName,Authentication|\r\n |Egress|Egress|Bytes|Total|The amount of egress data, in bytes. This number includes egress from an external client into Azure Storage as well as egress within Azure. As a result, this number does not reflect billable egress.|GeoType,ApiName,Authentication|\r"
  },
  {
    "Number": 107964,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-17T11:00:36Z",
    "User": "PMEds28",
    "FileName": "articles/azure-monitor/platform/partners.md",
    "Addition": 18,
    "Delections": 8,
    "Changes": 26,
    "Patch": "@@ -45,7 +45,7 @@ You can create JIRA tickets on Azure Monitor alerts.\n \n ![Circonus Logo](./media/partners/circonus.png)\n \n-Circonus is a microservices monitoring and analytics platform built for on premises or SaaS deployment. It is fully automatable API-Centric platform is more scalable and reliable than systems it monitors. Developed for the requirements of DevOps, Circonus delivers percentile-based alerts, graphs, dashboards, and machine-learning intelligence that enable business optimization. Circonus monitors your Microsoft Azure cloud resources and their applications in real time. You can use Circonus to collect and track metrics for the variables you want to measure for your resources and applications. With Circonus, you gain system-wide visibility into Azure’s resource utilization, application performance, and operational health.\n+Circonus is a microservices monitoring and analytics platform built for on premises or SaaS deployment. It is fully automatable API-Centric platform is more scalable and reliable than systems it monitors. Developed for the requirements of DevOps, Circonus delivers percentile-based alerts, graphs, dashboards, and machine-learning intelligence that enable business optimization. Circonus monitors your Microsoft Azure cloud resources and their applications in real time. You can use Circonus to collect and track metrics for the variables you want to measure for your resources and applications. With Circonus, you gain system-wide visibility into Azure's resource utilization, application performance, and operational health.\n \n [Go to the documentation.][circonus-doc]\n \n@@ -70,7 +70,7 @@ CloudMonix offers monitoring, automation, and self-healing services for Microsof\n \n ![DataDog Logo](./media/partners/datadog.png)\n \n-Datadog is the world’s leading monitoring service for cloud-scale applications. It brings together data from servers, databases, tools, and services to present a unified view of your entire stack. These capabilities are provided on a SaaS-based data analytics platform. This service enables Dev and Ops teams to work collaboratively to avoid downtime, resolve performance problems, and ensure that development and deployment cycles finish on time. By integrating Datadog and Azure, you can collect and view metrics from across your infrastructure. Correlate VM metrics with application-level metrics. Slice and dice your metrics using any combination of properties and custom tags.\n+Datadog is the world's leading monitoring service for cloud-scale applications. It brings together data from servers, databases, tools, and services to present a unified view of your entire stack. These capabilities are provided on a SaaS-based data analytics platform. This service enables Dev and Ops teams to work collaboratively to avoid downtime, resolve performance problems, and ensure that development and deployment cycles finish on time. By integrating Datadog and Azure, you can collect and view metrics from across your infrastructure. Correlate VM metrics with application-level metrics. Slice and dice your metrics using any combination of properties and custom tags.\n \n [Go to the documentation.][datadog-doc]\n \n@@ -102,7 +102,7 @@ Grafana is an open-source application that enables you to visualize time series\n \n ![InfluxData Logo](./media/partners/Influxdata.png)\n \n-InfluxData, the creator of InfluxDB, delivers a modern Open Source Platform built from the ground up for analyzing metrics and events (time series data) for DevOps and IoT applications. Whether the data comes from humans, sensors, or machines, InfluxData empowers developers to build next-generation monitoring, analytics, and IoT applications faster, easier, and to scale delivering real business value quickly. Based in San Francisco, InfluxData’s more than 420 customers include Cisco, eBay, IBM, and Siemens.\n+InfluxData, the creator of InfluxDB, delivers a modern Open Source Platform built from the ground up for analyzing metrics and events (time series data) for DevOps and IoT applications. Whether the data comes from humans, sensors, or machines, InfluxData empowers developers to build next-generation monitoring, analytics, and IoT applications faster, easier, and to scale delivering real business value quickly. Based in San Francisco, InfluxData's more than 420 customers include Cisco, eBay, IBM, and Siemens.\n \n [Go to the documentation.][influxdata-doc]\n \n@@ -111,15 +111,15 @@ InfluxData, the creator of InfluxDB, delivers a modern Open Source Platform buil\n \n ![Logic Monitor Logo](./media/partners/logicmonitor.png)\n \n-LogicMonitor® is the leading SaaS-based, performance monitoring platform for complex IT infrastructure. With coverage for thousands of technologies, LogicMonitor provides granular visibility into infrastructure and application performance. LM Cloud’s comprehensive Azure monitoring enables users to correlate the performance of Azure cloud, on-premises, and hybrid cloud resources -- all from a single platform. Automated resource discovery, built in monitoring templates, preconfigured alert thresholds, and customizable dashboards combine to give IT the speed, flexibility, and visibility required to succeed.\n+LogicMonitor&reg; is the leading SaaS-based, performance monitoring platform for complex IT infrastructure. With coverage for thousands of technologies, LogicMonitor provides granular visibility into infrastructure and application performance. LM Cloud's comprehensive Azure monitoring enables users to correlate the performance of Azure cloud, on-premises, and hybrid cloud resources -- all from a single platform. Automated resource discovery, built in monitoring templates, preconfigured alert thresholds, and customizable dashboards combine to give IT the speed, flexibility, and visibility required to succeed.\n \n [Go to the documentation.][logicmonitor-doc]\n \n ## LogRhythm\n \n ![LogRhythm Logo](./media/partners/logrhythm.png)\n \n-LogRhythm, a leader in NextGen SIEM, empowers organizations on six continents to measurably reduce risk by rapidly detecting, responding to, and neutralizing cyberthreats. LogRhythm’s Threat Lifecycle Management (TLM) workflow is the foundation for security operations centers, helping customers secure their cloud, physical, and virtual infrastructures for IT and OT environments. If you’re a LogRhythm customer and are ready to start your Azure journey, you’ll need to install and configure the LogRhythm Open Collector and EventHub integration. More details, including documentation of both configuring Azure Monitor and the Open Collector, can be found [here](https://logrhythm.com/six-tips-for-securing-your-azure-cloud-environment/). \n+LogRhythm, a leader in NextGen SIEM, empowers organizations on six continents to measurably reduce risk by rapidly detecting, responding to, and neutralizing cyberthreats. LogRhythm's Threat Lifecycle Management (TLM) workflow is the foundation for security operations centers, helping customers secure their cloud, physical, and virtual infrastructures for IT and OT environments. If you're a LogRhythm customer and are ready to start your Azure journey, you'll need to install and configure the LogRhythm Open Collector and EventHub integration. More details, including documentation of both configuring Azure Monitor and the Open Collector, can be found [here](https://logrhythm.com/six-tips-for-securing-your-azure-cloud-environment/). \n \n \n \n@@ -175,7 +175,7 @@ OpsGenie acts as a dispatcher for the alerts generated by Azure. OpsGenie determ\n \n ![PagerDuty Logo](./media/partners/pagerduty.png)\n \n-PagerDuty, the leading incident management solution, has provided first-class support for Azure Alerts on metrics. PagerDuty supports notifications on Azure Monitor alerts, autoscale notifications, activity log events, and platform-level metrics for Azure services. These enhancements give you increased visibility into the core Azure Platform. You can take full advantage of PagerDuty’s incident management capabilities for real-time response. The expanded Azure integration is made possible through webhooks. Webhooks allow you to set up and customize the solution quickly and easily.\n+PagerDuty, the leading incident management solution, has provided first-class support for Azure Alerts on metrics. PagerDuty supports notifications on Azure Monitor alerts, autoscale notifications, activity log events, and platform-level metrics for Azure services. These enhancements give you increased visibility into the core Azure Platform. You can take full advantage of PagerDuty's incident management capabilities for real-time response. The expanded Azure integration is made possible through webhooks. Webhooks allow you to set up and customize the solution quickly and easily.\n \n [Go to the documentation.][pagerduty-doc]\n \n@@ -189,7 +189,7 @@ The Microsoft Azure DSM and Microsoft Azure Event Hub Protocol are available for\n \n ![ScienceLogic Logo](./media/partners/sciencelogic.png)\n \n-ScienceLogic delivers the next generation IT service assurance platform for managing any technology, anywhere. ScienceLogic delivers the scale, security, automation, and resiliency necessary to simplify the tasks of managing IT resources, services, and applications. The ScienceLogic platform uses Azure APIs to interface with Microsoft Azure. ScienceLogic gives you real-time visibility into your Azure services and resources. So you know when something’s not working and you can fix it faster. You can also manage Azure alongside your other clouds and data center systems and services.\n+ScienceLogic delivers the next generation IT service assurance platform for managing any technology, anywhere. ScienceLogic delivers the scale, security, automation, and resiliency necessary to simplify the tasks of managing IT resources, services, and applications. The ScienceLogic platform uses Azure APIs to interface with Microsoft Azure. ScienceLogic gives you real-time visibility into your Azure services and resources. So you know when something's not working and you can fix it faster. You can also manage Azure alongside your other clouds and data center systems and services.\n \n [Learn more.][sciencelogic-doc]\n \n@@ -205,11 +205,12 @@ Serverless360 is a one platform tool to operate, manage, and monitor Azure serve\n \n ![SignalFX Logo](./media/partners/signalfx.png)\n \n-SignalFx is the leader in real-time operational intelligence for data-driven DevOps. The service discovers and collects metrics across every component in the cloud. It replaces traditional point tools and provides real-time visibility into today’s dynamic environments. Leveraging the massively scalable SignalFx platform, the SaaS platform is optimized for container and microservices based architectures and provides powerful visualization, proactive alerting, and collaborative triage capabilities across organizations of all sizes. SignalFx integrates directly with Azure Monitor as well as through open-source connectors such as *Telegraf*, *statsD*, and *collectd* to provide best in class dashboards, analytics, and alerts for Azure.\n+SignalFx is the leader in real-time operational intelligence for data-driven DevOps. The service discovers and collects metrics across every component in the cloud. It replaces traditional point tools and provides real-time visibility into today's dynamic environments. Leveraging the massively scalable SignalFx platform, the SaaS platform is optimized for container and microservices based architectures and provides powerful visualization, proactive alerting, and collaborative triage capabilities across organizations of all sizes. SignalFx integrates directly with Azure Monitor as well as through open-source connectors such as *Telegraf*, *statsD*, and *collectd* to provide best in class dashboards, analytics, and alerts for Azure.\n \n [Go to the documentation.][signalfx-doc]\n \n ## SIGNL4\n+\n ![SIGNL4 Logo](./media/partners/signl4.png)\n \n SIGNL4 - the mobile alerting app for operations teams - is the fastest way to route critical alerts from Azure Monitor to the right people at the right time – anywhere by push, text, and voice calls. SIGNL4 manages on-call duties and shifts of your team, tracks delivery and ownership of alerts and escalates if necessary. Full transparency across your team is provided. Using the super-easy REST web-hook of SIGNL4 any Azure service can be connected with no effort. With SIGNL4, you will see up to 10x faster response over email notifications and manual alerting.\n@@ -228,6 +229,14 @@ The Azure Monitor Add-on for Splunk is [available in the Splunkbase here](https:\n \n [Go to the documentation.][splunk-doc]\n \n+## SquaredUp \n+\n+![SquaredUp Logo](./media/partners/squaredup.png)\n+\n+SquaredUp for Azure makes visualizing your Azure applications beautifully simple. It gives you real time, interactive dashboards. You can drill down into subscriptions, resource groups, tags and individual resources to see metrics such as CPU, most inbound connections, Application Insights response time, total cost, and summary health status, and drill across to see related data such as alerts, Log Analytics events, more detailed metrics, or to see related data from the other tools you use – such as ServiceNow, Dynatrace, PagerDuty or Pingdom for example.  You can customize your own dashboards, publish, and share them with individuals or on intranet pages. \n+\n+[Learn more.](https://squaredup.com/)\n+\n ## Sumo Logic\n \n ![Sumo Logic Logo](./media/partners/SumoLogic.png)\n@@ -245,6 +254,7 @@ Turbonomic delivers workload automation for hybrid clouds by simultaneously opti\n [Learn more.][turbonomic-doc]\n \n ## Next steps\n+\n - [Learn more about Azure Monitor](../../azure-monitor/overview.md)\n - [Access metrics using the REST API](rest-api-walkthrough.md)\n - [Stream the Activity Log to a non-Microsoft service](../../azure-monitor/platform/activity-logs-stream-event-hubs.md)"
  },
  {
    "Number": 107953,
    "Title": "Add squaredup",
    "ClosedAt": "2020-03-17T09:39:35Z",
    "User": "rboucher",
    "FileName": "articles/azure-monitor/platform/partners.md",
    "Addition": 18,
    "Delections": 8,
    "Changes": 26,
    "Patch": "@@ -45,7 +45,7 @@ You can create JIRA tickets on Azure Monitor alerts.\n \n ![Circonus Logo](./media/partners/circonus.png)\n \n-Circonus is a microservices monitoring and analytics platform built for on premises or SaaS deployment. It is fully automatable API-Centric platform is more scalable and reliable than systems it monitors. Developed for the requirements of DevOps, Circonus delivers percentile-based alerts, graphs, dashboards, and machine-learning intelligence that enable business optimization. Circonus monitors your Microsoft Azure cloud resources and their applications in real time. You can use Circonus to collect and track metrics for the variables you want to measure for your resources and applications. With Circonus, you gain system-wide visibility into Azure’s resource utilization, application performance, and operational health.\n+Circonus is a microservices monitoring and analytics platform built for on premises or SaaS deployment. It is fully automatable API-Centric platform is more scalable and reliable than systems it monitors. Developed for the requirements of DevOps, Circonus delivers percentile-based alerts, graphs, dashboards, and machine-learning intelligence that enable business optimization. Circonus monitors your Microsoft Azure cloud resources and their applications in real time. You can use Circonus to collect and track metrics for the variables you want to measure for your resources and applications. With Circonus, you gain system-wide visibility into Azure's resource utilization, application performance, and operational health.\n \n [Go to the documentation.][circonus-doc]\n \n@@ -70,7 +70,7 @@ CloudMonix offers monitoring, automation, and self-healing services for Microsof\n \n ![DataDog Logo](./media/partners/datadog.png)\n \n-Datadog is the world’s leading monitoring service for cloud-scale applications. It brings together data from servers, databases, tools, and services to present a unified view of your entire stack. These capabilities are provided on a SaaS-based data analytics platform. This service enables Dev and Ops teams to work collaboratively to avoid downtime, resolve performance problems, and ensure that development and deployment cycles finish on time. By integrating Datadog and Azure, you can collect and view metrics from across your infrastructure. Correlate VM metrics with application-level metrics. Slice and dice your metrics using any combination of properties and custom tags.\n+Datadog is the world's leading monitoring service for cloud-scale applications. It brings together data from servers, databases, tools, and services to present a unified view of your entire stack. These capabilities are provided on a SaaS-based data analytics platform. This service enables Dev and Ops teams to work collaboratively to avoid downtime, resolve performance problems, and ensure that development and deployment cycles finish on time. By integrating Datadog and Azure, you can collect and view metrics from across your infrastructure. Correlate VM metrics with application-level metrics. Slice and dice your metrics using any combination of properties and custom tags.\n \n [Go to the documentation.][datadog-doc]\n \n@@ -102,7 +102,7 @@ Grafana is an open-source application that enables you to visualize time series\n \n ![InfluxData Logo](./media/partners/Influxdata.png)\n \n-InfluxData, the creator of InfluxDB, delivers a modern Open Source Platform built from the ground up for analyzing metrics and events (time series data) for DevOps and IoT applications. Whether the data comes from humans, sensors, or machines, InfluxData empowers developers to build next-generation monitoring, analytics, and IoT applications faster, easier, and to scale delivering real business value quickly. Based in San Francisco, InfluxData’s more than 420 customers include Cisco, eBay, IBM, and Siemens.\n+InfluxData, the creator of InfluxDB, delivers a modern Open Source Platform built from the ground up for analyzing metrics and events (time series data) for DevOps and IoT applications. Whether the data comes from humans, sensors, or machines, InfluxData empowers developers to build next-generation monitoring, analytics, and IoT applications faster, easier, and to scale delivering real business value quickly. Based in San Francisco, InfluxData's more than 420 customers include Cisco, eBay, IBM, and Siemens.\n \n [Go to the documentation.][influxdata-doc]\n \n@@ -111,15 +111,15 @@ InfluxData, the creator of InfluxDB, delivers a modern Open Source Platform buil\n \n ![Logic Monitor Logo](./media/partners/logicmonitor.png)\n \n-LogicMonitor® is the leading SaaS-based, performance monitoring platform for complex IT infrastructure. With coverage for thousands of technologies, LogicMonitor provides granular visibility into infrastructure and application performance. LM Cloud’s comprehensive Azure monitoring enables users to correlate the performance of Azure cloud, on-premises, and hybrid cloud resources -- all from a single platform. Automated resource discovery, built in monitoring templates, preconfigured alert thresholds, and customizable dashboards combine to give IT the speed, flexibility, and visibility required to succeed.\n+LogicMonitor&reg; is the leading SaaS-based, performance monitoring platform for complex IT infrastructure. With coverage for thousands of technologies, LogicMonitor provides granular visibility into infrastructure and application performance. LM Cloud's comprehensive Azure monitoring enables users to correlate the performance of Azure cloud, on-premises, and hybrid cloud resources -- all from a single platform. Automated resource discovery, built in monitoring templates, preconfigured alert thresholds, and customizable dashboards combine to give IT the speed, flexibility, and visibility required to succeed.\n \n [Go to the documentation.][logicmonitor-doc]\n \n ## LogRhythm\n \n ![LogRhythm Logo](./media/partners/logrhythm.png)\n \n-LogRhythm, a leader in NextGen SIEM, empowers organizations on six continents to measurably reduce risk by rapidly detecting, responding to, and neutralizing cyberthreats. LogRhythm’s Threat Lifecycle Management (TLM) workflow is the foundation for security operations centers, helping customers secure their cloud, physical, and virtual infrastructures for IT and OT environments. If you’re a LogRhythm customer and are ready to start your Azure journey, you’ll need to install and configure the LogRhythm Open Collector and EventHub integration. More details, including documentation of both configuring Azure Monitor and the Open Collector, can be found [here](https://logrhythm.com/six-tips-for-securing-your-azure-cloud-environment/). \n+LogRhythm, a leader in NextGen SIEM, empowers organizations on six continents to measurably reduce risk by rapidly detecting, responding to, and neutralizing cyberthreats. LogRhythm's Threat Lifecycle Management (TLM) workflow is the foundation for security operations centers, helping customers secure their cloud, physical, and virtual infrastructures for IT and OT environments. If you're a LogRhythm customer and are ready to start your Azure journey, you'll need to install and configure the LogRhythm Open Collector and EventHub integration. More details, including documentation of both configuring Azure Monitor and the Open Collector, can be found [here](https://logrhythm.com/six-tips-for-securing-your-azure-cloud-environment/). \n \n \n \n@@ -175,7 +175,7 @@ OpsGenie acts as a dispatcher for the alerts generated by Azure. OpsGenie determ\n \n ![PagerDuty Logo](./media/partners/pagerduty.png)\n \n-PagerDuty, the leading incident management solution, has provided first-class support for Azure Alerts on metrics. PagerDuty supports notifications on Azure Monitor alerts, autoscale notifications, activity log events, and platform-level metrics for Azure services. These enhancements give you increased visibility into the core Azure Platform. You can take full advantage of PagerDuty’s incident management capabilities for real-time response. The expanded Azure integration is made possible through webhooks. Webhooks allow you to set up and customize the solution quickly and easily.\n+PagerDuty, the leading incident management solution, has provided first-class support for Azure Alerts on metrics. PagerDuty supports notifications on Azure Monitor alerts, autoscale notifications, activity log events, and platform-level metrics for Azure services. These enhancements give you increased visibility into the core Azure Platform. You can take full advantage of PagerDuty's incident management capabilities for real-time response. The expanded Azure integration is made possible through webhooks. Webhooks allow you to set up and customize the solution quickly and easily.\n \n [Go to the documentation.][pagerduty-doc]\n \n@@ -189,7 +189,7 @@ The Microsoft Azure DSM and Microsoft Azure Event Hub Protocol are available for\n \n ![ScienceLogic Logo](./media/partners/sciencelogic.png)\n \n-ScienceLogic delivers the next generation IT service assurance platform for managing any technology, anywhere. ScienceLogic delivers the scale, security, automation, and resiliency necessary to simplify the tasks of managing IT resources, services, and applications. The ScienceLogic platform uses Azure APIs to interface with Microsoft Azure. ScienceLogic gives you real-time visibility into your Azure services and resources. So you know when something’s not working and you can fix it faster. You can also manage Azure alongside your other clouds and data center systems and services.\n+ScienceLogic delivers the next generation IT service assurance platform for managing any technology, anywhere. ScienceLogic delivers the scale, security, automation, and resiliency necessary to simplify the tasks of managing IT resources, services, and applications. The ScienceLogic platform uses Azure APIs to interface with Microsoft Azure. ScienceLogic gives you real-time visibility into your Azure services and resources. So you know when something's not working and you can fix it faster. You can also manage Azure alongside your other clouds and data center systems and services.\n \n [Learn more.][sciencelogic-doc]\n \n@@ -205,11 +205,12 @@ Serverless360 is a one platform tool to operate, manage, and monitor Azure serve\n \n ![SignalFX Logo](./media/partners/signalfx.png)\n \n-SignalFx is the leader in real-time operational intelligence for data-driven DevOps. The service discovers and collects metrics across every component in the cloud. It replaces traditional point tools and provides real-time visibility into today’s dynamic environments. Leveraging the massively scalable SignalFx platform, the SaaS platform is optimized for container and microservices based architectures and provides powerful visualization, proactive alerting, and collaborative triage capabilities across organizations of all sizes. SignalFx integrates directly with Azure Monitor as well as through open-source connectors such as *Telegraf*, *statsD*, and *collectd* to provide best in class dashboards, analytics, and alerts for Azure.\n+SignalFx is the leader in real-time operational intelligence for data-driven DevOps. The service discovers and collects metrics across every component in the cloud. It replaces traditional point tools and provides real-time visibility into today's dynamic environments. Leveraging the massively scalable SignalFx platform, the SaaS platform is optimized for container and microservices based architectures and provides powerful visualization, proactive alerting, and collaborative triage capabilities across organizations of all sizes. SignalFx integrates directly with Azure Monitor as well as through open-source connectors such as *Telegraf*, *statsD*, and *collectd* to provide best in class dashboards, analytics, and alerts for Azure.\n \n [Go to the documentation.][signalfx-doc]\n \n ## SIGNL4\n+\n ![SIGNL4 Logo](./media/partners/signl4.png)\n \n SIGNL4 - the mobile alerting app for operations teams - is the fastest way to route critical alerts from Azure Monitor to the right people at the right time – anywhere by push, text, and voice calls. SIGNL4 manages on-call duties and shifts of your team, tracks delivery and ownership of alerts and escalates if necessary. Full transparency across your team is provided. Using the super-easy REST web-hook of SIGNL4 any Azure service can be connected with no effort. With SIGNL4, you will see up to 10x faster response over email notifications and manual alerting.\n@@ -228,6 +229,14 @@ The Azure Monitor Add-on for Splunk is [available in the Splunkbase here](https:\n \n [Go to the documentation.][splunk-doc]\n \n+## SquaredUp \n+\n+![SquaredUp Logo](./media/partners/squaredup.png)\n+\n+SquaredUp for Azure makes visualizing your Azure applications beautifully simple. It gives you real time, interactive dashboards. You can drill down into subscriptions, resource groups, tags and individual resources to see metrics such as CPU, most inbound connections, Application Insights response time, total cost, and summary health status, and drill across to see related data such as alerts, Log Analytics events, more detailed metrics, or to see related data from the other tools you use – such as ServiceNow, Dynatrace, PagerDuty or Pingdom for example.  You can customize your own dashboards, publish, and share them with individuals or on intranet pages. \n+\n+[Learn more.](https://squaredup.com/)\n+\n ## Sumo Logic\n \n ![Sumo Logic Logo](./media/partners/SumoLogic.png)\n@@ -245,6 +254,7 @@ Turbonomic delivers workload automation for hybrid clouds by simultaneously opti\n [Learn more.][turbonomic-doc]\n \n ## Next steps\n+\n - [Learn more about Azure Monitor](../../azure-monitor/overview.md)\n - [Access metrics using the REST API](rest-api-walkthrough.md)\n - [Stream the Activity Log to a non-Microsoft service](../../azure-monitor/platform/activity-logs-stream-event-hubs.md)"
  },
  {
    "Number": 107949,
    "Title": "Adding 2 new regions",
    "ClosedAt": "2020-03-17T06:39:51Z",
    "User": "vinynigam",
    "FileName": "articles/azure-monitor/insights/network-performance-monitor.md",
    "Addition": 2,
    "Delections": 1,
    "Changes": 3,
    "Patch": "@@ -35,7 +35,8 @@ NPM can monitor connectivity between networks and applications in any part of th\n * North Europe\n * West Europe\n * France Central\n-\n+* Canada Central\n+* West US\n * West Central US\n * North Central US\n * South Central US"
  },
  {
    "Number": 107925,
    "Title": "Fixed descriptions for Sql metrics",
    "ClosedAt": "2020-03-17T01:58:21Z",
    "User": "dimitri-furman",
    "FileName": "articles/azure-monitor/platform/metrics-supported.md",
    "Addition": 33,
    "Delections": 33,
    "Changes": 66,
    "Patch": "@@ -462,9 +462,9 @@ Azure Monitor provides several ways to interact with metrics, including charting\n \r\n |Metric|Metric Display Name|Unit|Aggregation Type|Description|Dimensions|\r\n |---|---|---|---|---|---|\r\n-|BlobCapacity|Blob Capacity|Bytes|Average|The amount of storage used by the storage account’s Blob service in bytes.|BlobType,Tier|\r\n-|BlobCount|Blob Count|Count|Average|The number of Blob in the storage account’s Blob service.|BlobType,Tier|\r\n-|ContainerCount|Blob Container Count|Count|Average|The number of containers in the storage account’s Blob service.|None|\r\n+|BlobCapacity|Blob Capacity|Bytes|Average|The amount of storage used by the storage account's Blob service in bytes.|BlobType,Tier|\r\n+|BlobCount|Blob Count|Count|Average|The number of Blob in the storage account's Blob service.|BlobType,Tier|\r\n+|ContainerCount|Blob Container Count|Count|Average|The number of containers in the storage account's Blob service.|None|\r\n |IndexCapacity|Index Capacity|Bytes|Average|The amount of storage used by ADLS Gen2 (Hierarchical) Index in bytes.|None|\r\n |Transactions|Transactions|Count|Total|The number of requests made to a storage service or the specified API operation. This number includes successful and failed requests, as well as requests which produced errors. Use ResponseType dimension for the number of different type of response.|ResponseType,GeoType,ApiName,Authentication|\r\n |Ingress|Ingress|Bytes|Total|The amount of ingress data, in bytes. This number includes ingress from an external client into Azure Storage as well as ingress within Azure.|GeoType,ApiName,Authentication|\r\n@@ -477,9 +477,9 @@ Azure Monitor provides several ways to interact with metrics, including charting\n \r\n |Metric|Metric Display Name|Unit|Aggregation Type|Description|Dimensions|\r\n |---|---|---|---|---|---|\r\n-|TableCapacity|Table Capacity|Bytes|Average|The amount of storage used by the storage account’s Table service in bytes.|None|\r\n-|TableCount|Table Count|Count|Average|The number of table in the storage account’s Table service.|None|\r\n-|TableEntityCount|Table Entity Count|Count|Average|The number of table entities in the storage account’s Table service.|None|\r\n+|TableCapacity|Table Capacity|Bytes|Average|The amount of storage used by the storage account's Table service in bytes.|None|\r\n+|TableCount|Table Count|Count|Average|The number of table in the storage account's Table service.|None|\r\n+|TableEntityCount|Table Entity Count|Count|Average|The number of table entities in the storage account's Table service.|None|\r\n |Transactions|Transactions|Count|Total|The number of requests made to a storage service or the specified API operation. This number includes successful and failed requests, as well as requests which produced errors. Use ResponseType dimension for the number of different type of response.|ResponseType,GeoType,ApiName,Authentication|\r\n |Ingress|Ingress|Bytes|Total|The amount of ingress data, in bytes. This number includes ingress from an external client into Azure Storage as well as ingress within Azure.|GeoType,ApiName,Authentication|\r\n |Egress|Egress|Bytes|Total|The amount of egress data, in bytes. This number includes egress from an external client into Azure Storage as well as egress within Azure. As a result, this number does not reflect billable egress.|GeoType,ApiName,Authentication|\r\n@@ -491,11 +491,11 @@ Azure Monitor provides several ways to interact with metrics, including charting\n \r\n |Metric|Metric Display Name|Unit|Aggregation Type|Description|Dimensions|\r\n |---|---|---|---|---|---|\r\n-|FileCapacity|File Capacity|Bytes|Average|The amount of storage used by the storage account’s File service in bytes.|FileShare|\r\n-|FileCount|File Count|Count|Average|The number of file in the storage account’s File service.|FileShare|\r\n-|FileShareCount|File Share Count|Count|Average|The number of file shares in the storage account’s File service.|None|\r\n-|FileShareSnapshotCount|File Share Snapshot Count|Count|Average|The number of snapshots present on the share in storage account’s Files Service.|FileShare|\r\n-|FileShareSnapshotSize|File Share Snapshot Size|Bytes|Average|The amount of storage used by the snapshots in storage account’s File service in bytes.|FileShare|\r\n+|FileCapacity|File Capacity|Bytes|Average|The amount of storage used by the storage account's File service in bytes.|FileShare|\r\n+|FileCount|File Count|Count|Average|The number of file in the storage account's File service.|FileShare|\r\n+|FileShareCount|File Share Count|Count|Average|The number of file shares in the storage account's File service.|None|\r\n+|FileShareSnapshotCount|File Share Snapshot Count|Count|Average|The number of snapshots present on the share in storage account's Files Service.|FileShare|\r\n+|FileShareSnapshotSize|File Share Snapshot Size|Bytes|Average|The amount of storage used by the snapshots in storage account's File service in bytes.|FileShare|\r\n |FileShareQuota|File share quota size|Bytes|Average|The upper limit on the amount of storage that can be used by Azure Files Service in bytes.|FileShare|\r\n |Transactions|Transactions|Count|Total|The number of requests made to a storage service or the specified API operation. This number includes successful and failed requests, as well as requests which produced errors. Use ResponseType dimension for the number of different type of response.|ResponseType,GeoType,ApiName,Authentication,FileShare|\r\n |Ingress|Ingress|Bytes|Total|The amount of ingress data, in bytes. This number includes ingress from an external client into Azure Storage as well as ingress within Azure.|GeoType,ApiName,Authentication,FileShare|\r\n@@ -508,9 +508,9 @@ Azure Monitor provides several ways to interact with metrics, including charting\n \r\n |Metric|Metric Display Name|Unit|Aggregation Type|Description|Dimensions|\r\n |---|---|---|---|---|---|\r\n-|QueueCapacity|Queue Capacity|Bytes|Average|The amount of storage used by the storage account’s Queue service in bytes.|None|\r\n-|QueueCount|Queue Count|Count|Average|The number of queue in the storage account’s Queue service.|None|\r\n-|QueueMessageCount|Queue Message Count|Count|Average|The approximate number of queue messages in the storage account’s Queue service.|None|\r\n+|QueueCapacity|Queue Capacity|Bytes|Average|The amount of storage used by the storage account's Queue service in bytes.|None|\r\n+|QueueCount|Queue Count|Count|Average|The number of queue in the storage account's Queue service.|None|\r\n+|QueueMessageCount|Queue Message Count|Count|Average|The approximate number of queue messages in the storage account's Queue service.|None|\r\n |Transactions|Transactions|Count|Total|The number of requests made to a storage service or the specified API operation. This number includes successful and failed requests, as well as requests which produced errors. Use ResponseType dimension for the number of different type of response.|ResponseType,GeoType,ApiName,Authentication|\r\n |Ingress|Ingress|Bytes|Total|The amount of ingress data, in bytes. This number includes ingress from an external client into Azure Storage as well as ingress within Azure.|GeoType,ApiName,Authentication|\r\n |Egress|Egress|Bytes|Total|The amount of egress data, in bytes. This number includes egress from an external client into Azure Storage as well as egress within Azure. As a result, this number does not reflect billable egress.|GeoType,ApiName,Authentication|\r\n@@ -1176,7 +1176,7 @@ Azure Monitor provides several ways to interact with metrics, including charting\n |Metric|Metric Display Name|Unit|Aggregation Type|Description|Dimensions|\r\n |---|---|---|---|---|---|\r\n |CacheUtilization|Cache utilization|Percent|Average|Utilization level in the cluster scope|None|\r\n-|QueryDuration|Query duration|Milliseconds|Average|Queries’ duration in seconds|QueryStatus|\r\n+|QueryDuration|Query duration|Milliseconds|Average|Queries' duration in seconds|QueryStatus|\r\n |IngestionUtilization|Ingestion utilization|Percent|Average|Ratio of used ingestion slots in the cluster|None|\r\n |KeepAlive|Keep alive|Count|Average|Sanity check indicates the cluster responds to queries|None|\r\n |IngestionVolumeInMB|Ingestion volume (in MB)|Count|Total|Overall volume of ingested data to the cluster (in MB)|Database|\r\n@@ -1844,8 +1844,8 @@ Azure Monitor provides several ways to interact with metrics, including charting\n |dwu_used|DWU used|Count|Maximum|DWU used. Applies only to data warehouses.|None|\r\n |cache_hit_percent|Cache hit percentage|Percent|Maximum|Cache hit percentage. Applies only to data warehouses.|None|\r\n |cache_used_percent|Cache used percentage|Percent|Maximum|Cache used percentage. Applies only to data warehouses.|None|\r\n-|sqlserver_process_core_percent|SQL Server process core percent|Percent|Maximum|CPU usage percentage for the SQL Server process, as measured by the operating system. Currently available for serverless databases only.|None|\r\n-|sqlserver_process_memory_percent|SQL Server process memory percent|Percent|Maximum|Memory usage percentage for the SQL Server process, as measured by the operating system. Currently available for serverless databases only.|None|\r\n+|sqlserver_process_core_percent|SQL Server process core percent|Percent|Maximum|CPU usage percentage for the SQL Server process, as measured by the operating system.|None|\r\n+|sqlserver_process_memory_percent|SQL Server process memory percent|Percent|Maximum|Memory usage percentage for the SQL Server process, as measured by the operating system.|None|\r\n |tempdb_data_size|Tempdb Data File Size Kilobytes|Count|Maximum|Tempdb Data File Size Kilobytes. Not applicable to data warehouses.|None|\r\n |tempdb_log_size|Tempdb Log File Size Kilobytes|Count|Maximum|Tempdb Log File Size Kilobytes. Not applicable to data warehouses.|None|\r\n |tempdb_log_used_percent|Tempdb Percent Log Used|Percent|Maximum|Tempdb Percent Log Used. Not applicable to data warehouses.|None|\r\n@@ -1888,8 +1888,8 @@ Azure Monitor provides several ways to interact with metrics, including charting\n |database_cpu_limit|CPU limit|Count|Average|CPU limit|DatabaseResourceId|\r\n |cpu_used|CPU used|Count|Average|CPU used. Applies to vCore-based elastic pools.|None|\r\n |database_cpu_used|CPU used|Count|Average|CPU used|DatabaseResourceId|\r\n-|sqlserver_process_core_percent|SQL Server process core percent|Percent|Maximum|CPU usage as a percentage of the SQL DB process. Applies to elastic pools.|None|\r\n-|sqlserver_process_memory_percent|SQL Server process memory percent|Percent|Maximum|Memory usage as a percentage of the SQL DB process. Applies to elastic pools.|None|\r\n+|sqlserver_process_core_percent|SQL Server process core percent|Percent|Maximum|CPU usage percentage for the SQL Server process, as measured by the operating system. Applies to elastic pools.|None|\r\n+|sqlserver_process_memory_percent|SQL Server process memory percent|Percent|Maximum|Memory usage percentage for the SQL Server process, as measured by the operating system. Applies to elastic pools.|None|\r\n |tempdb_data_size|Tempdb Data File Size Kilobytes|Count|Maximum|Tempdb Data File Size Kilobytes|None|\r\n |tempdb_log_size|Tempdb Log File Size Kilobytes|Count|Maximum|Tempdb Log File Size Kilobytes|None|\r\n |tempdb_log_used_percent|Tempdb Percent Log Used|Percent|Maximum|Tempdb Percent Log Used|None|\r\n@@ -1937,9 +1937,9 @@ Azure Monitor provides several ways to interact with metrics, including charting\n \r\n |Metric|Metric Display Name|Unit|Aggregation Type|Description|Dimensions|\r\n |---|---|---|---|---|---|\r\n-|BlobCapacity|Blob Capacity|Bytes|Average|The amount of storage used by the storage account’s Blob service in bytes.|BlobType,Tier|\r\n-|BlobCount|Blob Count|Count|Average|The number of Blob in the storage account’s Blob service.|BlobType,Tier|\r\n-|ContainerCount|Blob Container Count|Count|Average|The number of containers in the storage account’s Blob service.|None|\r\n+|BlobCapacity|Blob Capacity|Bytes|Average|The amount of storage used by the storage account's Blob service in bytes.|BlobType,Tier|\r\n+|BlobCount|Blob Count|Count|Average|The number of Blob in the storage account's Blob service.|BlobType,Tier|\r\n+|ContainerCount|Blob Container Count|Count|Average|The number of containers in the storage account's Blob service.|None|\r\n |IndexCapacity|Index Capacity|Bytes|Average|The amount of storage used by ADLS Gen2 (Hierarchical) Index in bytes.|None|\r\n |Transactions|Transactions|Count|Total|The number of requests made to a storage service or the specified API operation. This number includes successful and failed requests, as well as requests which produced errors. Use ResponseType dimension for the number of different type of response.|ResponseType,GeoType,ApiName,Authentication|\r\n |Ingress|Ingress|Bytes|Total|The amount of ingress data, in bytes. This number includes ingress from an external client into Azure Storage as well as ingress within Azure.|GeoType,ApiName,Authentication|\r\n@@ -1952,9 +1952,9 @@ Azure Monitor provides several ways to interact with metrics, including charting\n \r\n |Metric|Metric Display Name|Unit|Aggregation Type|Description|Dimensions|\r\n |---|---|---|---|---|---|\r\n-|TableCapacity|Table Capacity|Bytes|Average|The amount of storage used by the storage account’s Table service in bytes.|None|\r\n-|TableCount|Table Count|Count|Average|The number of table in the storage account’s Table service.|None|\r\n-|TableEntityCount|Table Entity Count|Count|Average|The number of table entities in the storage account’s Table service.|None|\r\n+|TableCapacity|Table Capacity|Bytes|Average|The amount of storage used by the storage account's Table service in bytes.|None|\r\n+|TableCount|Table Count|Count|Average|The number of table in the storage account's Table service.|None|\r\n+|TableEntityCount|Table Entity Count|Count|Average|The number of table entities in the storage account's Table service.|None|\r\n |Transactions|Transactions|Count|Total|The number of requests made to a storage service or the specified API operation. This number includes successful and failed requests, as well as requests which produced errors. Use ResponseType dimension for the number of different type of response.|ResponseType,GeoType,ApiName,Authentication|\r\n |Ingress|Ingress|Bytes|Total|The amount of ingress data, in bytes. This number includes ingress from an external client into Azure Storage as well as ingress within Azure.|GeoType,ApiName,Authentication|\r\n |Egress|Egress|Bytes|Total|The amount of egress data, in bytes. This number includes egress from an external client into Azure Storage as well as egress within Azure. As a result, this number does not reflect billable egress.|GeoType,ApiName,Authentication|\r\n@@ -1966,11 +1966,11 @@ Azure Monitor provides several ways to interact with metrics, including charting\n \r\n |Metric|Metric Display Name|Unit|Aggregation Type|Description|Dimensions|\r\n |---|---|---|---|---|---|\r\n-|FileCapacity|File Capacity|Bytes|Average|The amount of storage used by the storage account’s File service in bytes.|FileShare|\r\n-|FileCount|File Count|Count|Average|The number of file in the storage account’s File service.|FileShare|\r\n-|FileShareCount|File Share Count|Count|Average|The number of file shares in the storage account’s File service.|None|\r\n-|FileShareSnapshotCount|File share snapshot count|Count|Average|The number of snapshots present on the share in storage account’s Files Service.|FileShare|\r\n-|FileShareSnapshotSize|File share snapshot size|Bytes|Average|The amount of storage used by the snapshots in storage account’s File service in bytes.|FileShare|\r\n+|FileCapacity|File Capacity|Bytes|Average|The amount of storage used by the storage account's File service in bytes.|FileShare|\r\n+|FileCount|File Count|Count|Average|The number of file in the storage account's File service.|FileShare|\r\n+|FileShareCount|File Share Count|Count|Average|The number of file shares in the storage account's File service.|None|\r\n+|FileShareSnapshotCount|File share snapshot count|Count|Average|The number of snapshots present on the share in storage account's Files Service.|FileShare|\r\n+|FileShareSnapshotSize|File share snapshot size|Bytes|Average|The amount of storage used by the snapshots in storage account's File service in bytes.|FileShare|\r\n |FileShareQuota|File share quota size|Bytes|Average|The upper limit on the amount of storage that can be used by Azure Files Service in bytes.|FileShare|\r\n |Transactions|Transactions|Count|Total|The number of requests made to a storage service or the specified API operation. This number includes successful and failed requests, as well as requests which produced errors. Use ResponseType dimension for the number of different type of response.|ResponseType,GeoType,ApiName,Authentication,FileShare|\r\n |Ingress|Ingress|Bytes|Total|The amount of ingress data, in bytes. This number includes ingress from an external client into Azure Storage as well as ingress within Azure.|GeoType,ApiName,Authentication,FileShare|\r\n@@ -1983,9 +1983,9 @@ Azure Monitor provides several ways to interact with metrics, including charting\n \r\n |Metric|Metric Display Name|Unit|Aggregation Type|Description|Dimensions|\r\n |---|---|---|---|---|---|\r\n-|QueueCapacity|Queue Capacity|Bytes|Average|The amount of storage used by the storage account’s Queue service in bytes.|None|\r\n-|QueueCount|Queue Count|Count|Average|The number of queue in the storage account’s Queue service.|None|\r\n-|QueueMessageCount|Queue Message Count|Count|Average|The approximate number of queue messages in the storage account’s Queue service.|None|\r\n+|QueueCapacity|Queue Capacity|Bytes|Average|The amount of storage used by the storage account's Queue service in bytes.|None|\r\n+|QueueCount|Queue Count|Count|Average|The number of queue in the storage account's Queue service.|None|\r\n+|QueueMessageCount|Queue Message Count|Count|Average|The approximate number of queue messages in the storage account's Queue service.|None|\r\n |Transactions|Transactions|Count|Total|The number of requests made to a storage service or the specified API operation. This number includes successful and failed requests, as well as requests which produced errors. Use ResponseType dimension for the number of different type of response.|ResponseType,GeoType,ApiName,Authentication|\r\n |Ingress|Ingress|Bytes|Total|The amount of ingress data, in bytes. This number includes ingress from an external client into Azure Storage as well as ingress within Azure.|GeoType,ApiName,Authentication|\r\n |Egress|Egress|Bytes|Total|The amount of egress data, in bytes. This number includes egress from an external client into Azure Storage as well as egress within Azure. As a result, this number does not reflect billable egress.|GeoType,ApiName,Authentication|\r"
  },
  {
    "Number": 107843,
    "Title": "(AzureCXP) Fix Typo",
    "ClosedAt": "2020-03-17T00:20:43Z",
    "User": "BhargaviAnnadevara-MSFT",
    "FileName": "articles/azure-monitor/app/change-analysis.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -33,7 +33,7 @@ See the *Viewing changes for all resources in Azure* section to access Change An\n Using [Azure Resource Graph](https://docs.microsoft.com/azure/governance/resource-graph/overview), Change Analysis provides a historical record of how the Azure resources that host your application have changed over time. Tracked settings such as managed identities, Platform OS upgrade, and hostnames can be detected.\n \n ### Azure Resource Manager proxied setting changes\n-Settings such as IP Configuration rule, SSL settings, and extension versions are not yet available in ARG, so Change Analysis queries and computes these changes securely to provide more details in what changed in the app. These information is not available yet in Azure Resource Graph but will be available soon.\n+Settings such as IP Configuration rule, SSL settings, and extension versions are not yet available in ARG, so Change Analysis queries and computes these changes securely to provide more details in what changed in the app. This information is not available yet in Azure Resource Graph but will be available soon.\n \n ### Changes in web app deployment and configuration (in-guest changes)\n "
  },
  {
    "Number": 107894,
    "Title": "Azure Monitor manage cost storage update",
    "ClosedAt": "2020-03-16T22:38:19Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/log-query/examples.md",
    "Addition": 40,
    "Delections": 32,
    "Changes": 72,
    "Patch": "@@ -5,7 +5,7 @@ ms.subservice: logs\n ms.topic: conceptual\n author: bwren\n ms.author: bwren\n-ms.date: 10/01/2019\n+ms.date: 03/16/2020\n \n ---\n \n@@ -349,12 +349,12 @@ Using **join**, and **let** statements we can check if the same suspicious accou\n ```Kusto\n let timeframe = 1d;\n let suspicious_users = \n-\tSecurityEvent\n-\t| where TimeGenerated > ago(timeframe)\n-\t| where AccountType == 'User' and EventID == 4625 // 4625 - failed login\n-\t| summarize failed_login_attempts=count(), latest_failed_login=arg_max(TimeGenerated, Account) by Account \n-\t| where failed_login_attempts > 5\n-\t| project-away Account1;\n+    SecurityEvent\n+    | where TimeGenerated > ago(timeframe)\n+    | where AccountType == 'User' and EventID == 4625 // 4625 - failed login\n+    | summarize failed_login_attempts=count(), latest_failed_login=arg_max(TimeGenerated, Account) by Account \n+    | where failed_login_attempts > 5\n+    | project-away Account1;\n let suspicious_users_that_later_logged_in = \n     suspicious_users \n     | join kind=innerunique (\n@@ -371,41 +371,49 @@ suspicious_users_that_later_logged_in\n \n ## Usage\n \n-### Calculate the average size of perf usage reports per computer\n+The `Usage` data type can be used to track the ingested data volume by solution or data type. There are other techniques to study ingested data volumes by [computer](https://docs.microsoft.com/azure/azure-monitor/platform/manage-cost-storage#data-volume-by-computer) or [Azure subscription, resource group or resource](https://docs.microsoft.com/azure/azure-monitor/platform/manage-cost-storage#data-volume-by-azure-resource-resource-group-or-subscription).\n \n-This example calculates the average size of perf usage reports per computer, over the last 3 hours.\n-The results are shown in a bar chart.\n-```Kusto\n+#### Data volume by solution\n+\n+The query used to view the billable data volume by solution over the last month (excluding the last partial day) is:\n+\n+```kusto\n Usage \n-| where TimeGenerated > ago(3h)\n-| where DataType == \"Perf\" \n-| where QuantityUnit == \"MBytes\" \n-| summarize avg(Quantity) by Computer\n-| sort by avg_Quantity desc nulls last\n-| render barchart\n+| where TimeGenerated > ago(32d)\n+| where StartTime >= startofday(ago(31d)) and EndTime < startofday(now())\n+| where IsBillable == true\n+| summarize BillableDataGB = sum(Quantity) / 1000. by bin(StartTime, 1d), Solution | render barchart\n ```\n \n-### Timechart latency percentiles 50 and 95\n+Note that the clause `where IsBillable = true` filters out data types from certain solutions for which there is no ingestion charge.  Also the clause with `TimeGenerated` is only to ensure that the query experience in the Azure portal will look back beyond the default 24 hours. When using the Usage data type, `StartTime` and `EndTime` represent the time buckets for which results are presented. \n \n-This example calculates and charts the 50th and 95th percentiles of reported **avgLatency** by hour over the last 24 hours.\n+#### Data volume by type\n \n-```Kusto\n-Usage\n-| where TimeGenerated > ago(24h)\n-| summarize percentiles(AvgLatencyInSeconds, 50, 95) by bin(TimeGenerated, 1h) \n-| render timechart\n+You can drill in further to see data trends for by data type:\n+\n+```kusto\n+Usage \n+| where TimeGenerated > ago(32d)\n+| where StartTime >= startofday(ago(31d)) and EndTime < startofday(now())\n+| where IsBillable == true\n+| summarize BillableDataGB = sum(Quantity) / 1000. by bin(StartTime, 1d), DataType | render barchart\n ```\n \n-### Usage of specific computers today\n-This example retrieves **Usage** data from the last day for computer names that contains the string _ContosoFile_. The results are sorted by **TimeGenerated**.\n+Or to see a table by solution and type for the last month,\n \n-```Kusto\n-Usage\n-| where TimeGenerated > ago(1d)\n-| where  Computer contains \"ContosoFile\" \n-| sort by TimeGenerated desc nulls last\n+```kusto\n+Usage \n+| where TimeGenerated > ago(32d)\n+| where StartTime >= startofday(ago(31d)) and EndTime < startofday(now())\n+| where IsBillable == true\n+| summarize BillableDataGB = sum(Quantity) / 1000. by Solution, DataType\n+| sort by Solution asc, DataType asc\n ```\n \n+> [!NOTE]\n+> Some of the fields of the Usage data type, while still in the schema, have been deprecated and will their values are no longer populated. \n+> These are **Computer** as well as fields related to ingestion (**TotalBatches**, **BatchesWithinSla**, **BatchesOutsideSla**, **BatchesCapped** and **AverageProcessingTimeMs**.\n+\n ## Updates\n \n ### Computers Still Missing Updates\n@@ -427,4 +435,4 @@ Update\n ## Next steps\n \n - Refer to the [Kusto language reference](/azure/kusto/query) for details on the language.\n-- Walk through a [lesson on writing log queries in Azure Monitor](get-started-queries.md).\n+- Walk through a [lesson on writing log queries in Azure Monitor](get-started-queries.md).\n\\ No newline at end of file"
  },
  {
    "Number": 107894,
    "Title": "Azure Monitor manage cost storage update",
    "ClosedAt": "2020-03-16T22:38:19Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/platform/manage-cost-storage.md",
    "Addition": 39,
    "Delections": 16,
    "Changes": 55,
    "Patch": "@@ -11,7 +11,7 @@ ms.service: azure-monitor\n ms.workload: na\n ms.tgt_pltfrm: na\n ms.topic: conceptual\n-ms.date: 11/05/2019\n+ms.date: 03/16/2020\n ms.author: bwren\n ms.subservice: \n ---\n@@ -107,7 +107,7 @@ To set the default retention for your workspace,\n 3. On the pane, move the slider to increase or decrease the number of days and then click **OK**.  If you are on the *free* tier, you will not be able to modify the data retention period and you need to upgrade to the paid tier in order to control this setting.\n \n     ![Change workspace data retention setting](media/manage-cost-storage/manage-cost-change-retention-01.png)\n-\t\n+    \n The retention can also be [set via Azure Resource Manager](https://docs.microsoft.com/azure/azure-monitor/platform/template-workspace-configuration#configure-a-log-analytics-workspace) using the `retentionInDays` parameter. Additionally, if you set the data retention to 30 days, you can trigger an immediate purge of older data using the `immediatePurgeDataOn30Days` parameter, which may be useful for compliance-related scenarios. This functionality is only exposed via Azure Resource Manager. \n \n Two data types -- `Usage` and `AzureActivity` -- are retained for 90 days by default, and there is no charge for for this 90 day retention. These data types are also free from data ingestion charges. \n@@ -137,9 +137,9 @@ To set the retention of a particular data type (in this example SecurityEvent) t\n ```JSON\n     PUT /subscriptions/00000000-0000-0000-0000-00000000000/resourceGroups/MyResourceGroupName/providers/Microsoft.OperationalInsights/workspaces/MyWorkspaceName/Tables/SecurityEvent?api-version=2017-04-26-preview\n     {\n-\t    \"properties\": \n-\t\t{\n-\t\t    \"retentionInDays\": 730\n+        \"properties\": \n+        {\n+            \"retentionInDays\": 730\n         }\n     }\n ```\n@@ -169,15 +169,15 @@ When the daily limit is reached, the collection of billable data types stops for\n \n ### Identify what daily data limit to define\n \n-Review [Log Analytics Usage and estimated costs](usage-estimated-costs.md) to understand the data ingestion trend and what is the daily volume cap to define. It should be considered with care, since you won’t be able to monitor your resources after the limit is reached. \n+Review [Log Analytics Usage and estimated costs](usage-estimated-costs.md) to understand the data ingestion trend and what is the daily volume cap to define. It should be considered with care, since you won�t be able to monitor your resources after the limit is reached. \n \n ### Set the Daily Cap\n \n The following steps describe how to configure a limit to manage the volume of data that Log Analytics workspace will ingest per day.  \n \n 1. From your workspace, select **Usage and estimated costs** from the left pane.\n 2. On the **Usage and estimated costs** page for the selected workspace, click **Data volume management** from the top of the page. \n-3. Daily cap is **OFF** by default – click **ON** to enable it, and then set the data volume limit in GB/day.\n+3. Daily cap is **OFF** by default � click **ON** to enable it, and then set the data volume limit in GB/day.\n \n     ![Log Analytics configure data limit](media/manage-cost-storage/set-daily-volume-cap-01.png)\n \n@@ -217,10 +217,11 @@ Heartbeat\n | summarize nodes = dcount(Computer) by bin(TimeGenerated, 1d)    \n | render timechart\n ```\n-The get a count of nodes sending data seen can be determined using: \n+The get a count of nodes sending data in the last 24 hours use the query: \n \n ```kusto\n union withsource = tt * \n+| where TimeGenerated > ago(24h)\n | extend computerName = tolower(tostring(split(Computer, '.')[0]))\n | where computerName != \"\"\n | summarize nodes = dcount(computerName)\n@@ -230,6 +231,7 @@ To get a list of nodes sending any data (and the amount of data sent by each) th\n \n ```kusto\n union withsource = tt * \n+| where TimeGenerated > ago(24h)\n | extend computerName = tolower(tostring(split(Computer, '.')[0]))\n | where computerName != \"\"\n | summarize TotalVolumeBytes=sum(_BilledSize) by computerName\n@@ -242,35 +244,52 @@ union withsource = tt *\n \n On the **Usage and Estimated Costs** page, the *Data ingestion per solution* chart shows the total volume of data sent and how much is being sent by each solution. This allows you to determine trends such as whether the overall data usage (or usage by a particular solution) is growing, remaining steady or decreasing. \n \n+### Data volume for specific events\n+\n+To look at the size of ingested data for a particular set of events, you can query the specific table (in this example `Event`) and then restrict the query to the events of interest (in this example event ID 5145 or 5156):\n+\n+```kusto\n+Event\n+| where TimeGenerated > startofday(ago(31d)) and TimeGenerated < startofday(now()) \n+| where EventID == 5145 or EventID == 5156\n+| where _IsBillable == true\n+| summarize count(), Bytes=sum(_BilledSize) by EventID, bin(TimeGenerated, 1d)\n+``` \n+\n+Note that the clause `where IsBillable = true` filters out data types from certain solutions for which there is no ingestion charge. \n+\n ### Data volume by solution\n \n-The query used to view the billable data volume by solution is\n+The query used to view the billable data volume by solution over the last month (excluding the last partial day) is:\n \n ```kusto\n Usage \n-| where TimeGenerated > startofday(ago(31d))\n+| where TimeGenerated > ago(32d)\n+| where StartTime >= startofday(ago(31d)) and EndTime < startofday(now())\n | where IsBillable == true\n-| summarize BillableDataGB = sum(Quantity) / 1000. by bin(TimeGenerated, 1d), Solution | render barchart\n+| summarize BillableDataGB = sum(Quantity) / 1000. by bin(StartTime, 1d), Solution | render barchart\n ```\n \n-Note that the clause `where IsBillable = true` filters out data types from certain solutions for which there is no ingestion charge. \n+The clause with `TimeGenerated` is only to ensure that the query experience in the Azure portal will look back beyond the default 24 hours. When using the Usage data type, `StartTime` and `EndTime` represent the time buckets for which results are presented. \n \n ### Data volume by type\n \n You can drill in further to see data trends for by data type:\n \n ```kusto\n-Usage | where TimeGenerated > startofday(ago(31d))| where IsBillable == true\n-| where TimeGenerated > startofday(ago(31d))\n+Usage \n+| where TimeGenerated > ago(32d)\n+| where StartTime >= startofday(ago(31d)) and EndTime < startofday(now())\n | where IsBillable == true\n-| summarize BillableDataGB = sum(Quantity) / 1000. by bin(TimeGenerated, 1d), DataType | render barchart\n+| summarize BillableDataGB = sum(Quantity) / 1000. by bin(StartTime, 1d), DataType | render barchart\n ```\n \n Or to see a table by solution and type for the last month,\n \n ```kusto\n Usage \n-| where TimeGenerated > startofday(ago(31d))\n+| where TimeGenerated > ago(32d)\n+| where StartTime >= startofday(ago(31d)) and EndTime < startofday(now())\n | where IsBillable == true\n | summarize BillableDataGB = sum(Quantity) by Solution, DataType\n | sort by Solution asc, DataType asc\n@@ -282,6 +301,7 @@ The `Usage` data type does not include information at the completer level. To se\n \n ```kusto\n union withsource = tt * \n+| where TimeGenerated > ago(24h)\n | where _IsBillable == true \n | extend computerName = tolower(tostring(split(Computer, '.')[0]))\n | summarize BillableDataBytes = sum(_BilledSize) by  computerName | sort by Bytes nulls last\n@@ -293,6 +313,7 @@ To see the **count** of billable events ingested per computer, use\n \n ```kusto\n union withsource = tt * \n+| where TimeGenerated > ago(24h)\n | where _IsBillable == true \n | extend computerName = tolower(tostring(split(Computer, '.')[0]))\n | summarize eventCount = count() by computerName  | sort by eventCount nulls last\n@@ -304,6 +325,7 @@ For data from nodes hosted in Azure you can get the **size** of ingested data __\n \n ```kusto\n union withsource = tt * \n+| where TimeGenerated > ago(24h)\n | where _IsBillable == true \n | summarize BillableDataBytes = sum(_BilledSize) by _ResourceId | sort by Bytes nulls last\n ```\n@@ -312,6 +334,7 @@ For data from nodes hosted in Azure you can get the **size** of ingested data __\n \n ```kusto\n union withsource = tt * \n+| where TimeGenerated > ago(24h)\n | where _IsBillable == true \n | parse tolower(_ResourceId) with \"/subscriptions/\" subscriptionId \"/resourcegroups/\" \n     resourceGroup \"/providers/\" provider \"/\" resourceType \"/\" resourceName   "
  },
  {
    "Number": 106840,
    "Title": "Graph scrub for azure monitor",
    "ClosedAt": "2020-03-16T22:00:25Z",
    "User": "davidmu1",
    "FileName": "articles/azure-monitor/platform/activity-log-schema.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -33,7 +33,7 @@ This category contains the record of all create, update, delete, and action oper\n         \"nbf\": \"1234567890\",\n         \"exp\": \"1234567890\",\n         \"_claim_names\": \"{\\\"groups\\\":\\\"src1\\\"}\",\n-        \"_claim_sources\": \"{\\\"src1\\\":{\\\"endpoint\\\":\\\"https://graph.windows.net/1114444b-7467-4144-a616-e3a5d63e147b/users/f409edeb-4d29-44b5-9763-ee9348ad91bb/getMemberObjects\\\"}}\",\n+        \"_claim_sources\": \"{\\\"src1\\\":{\\\"endpoint\\\":\\\"https://graph.microsoft.com/1114444b-7467-4144-a616-e3a5d63e147b/users/f409edeb-4d29-44b5-9763-ee9348ad91bb/getMemberObjects\\\"}}\",\n         \"http://schemas.microsoft.com/claims/authnclassreference\": \"1\",\n         \"aio\": \"A3GgTJdwK4vy7Fa7l6DgJC2mI0GX44tML385OpU1Q+z+jaPnFMwB\",\n         \"http://schemas.microsoft.com/claims/authnmethodsreferences\": \"rsa,mfa\","
  },
  {
    "Number": 107791,
    "Title": "fix invalid link",
    "ClosedAt": "2020-03-16T21:23:04Z",
    "User": "928PJY",
    "FileName": "articles/azure-monitor/app/source-map-support.md",
    "Addition": 3,
    "Delections": 3,
    "Changes": 6,
    "Patch": "@@ -83,6 +83,6 @@ Any user on the Portal using this feature must be at least assigned as a [Storag\n [create storage account]: https://docs.microsoft.com/azure/storage/common/storage-account-create?toc=%2Fazure%2Fstorage%2Fblobs%2Ftoc.json&tabs=azure-portal\n [create blob container]: https://docs.microsoft.com/azure/storage/blobs/storage-quickstart-blobs-portal\n [storage blob data reader]: https://docs.microsoft.com/azure/role-based-access-control/built-in-roles#storage-blob-data-reader\n-[ApplicationInsights-JS]: \"https://github.com/microsoft/applicationinsights-js\"\n-[ApplicationInsights-Node.js]: \"https://github.com/microsoft/applicationinsights-node.js\"\n-[azure file copy]: \"https://aka.ms/azurefilecopyreadme\"\n\\ No newline at end of file\n+[ApplicationInsights-JS]: https://github.com/microsoft/applicationinsights-js\n+[ApplicationInsights-Node.js]: https://github.com/microsoft/applicationinsights-node.js\n+[azure file copy]: https://aka.ms/azurefilecopyreadme\n\\ No newline at end of file"
  },
  {
    "Number": 104784,
    "Title": " Azure Monitor logic app connector",
    "ClosedAt": "2020-03-16T20:12:32Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/app/automate-with-flow.md",
    "Addition": 0,
    "Delections": 130,
    "Changes": 130,
    "Patch": "@@ -1,130 +0,0 @@\n----\n-title: Automate Azure Application Insights processes with Microsoft Flow\n-description: Learn how you can use Microsoft Flow to quickly automate repeatable processes by using the Application Insights connector.\n-ms.topic: conceptual\n-ms.date: 08/29/2019\n-\n----\n-\n-# Automate Azure Application Insights processes with the connector for Microsoft Flow\n-\n-Do you find yourself repeatedly running the same queries on your telemetry data to check that your service is functioning properly? Are you looking to automate these queries for finding trends and anomalies and then build your own workflows around them? The Azure Application Insights connector for Microsoft Flow is the right tool for these purposes.\n-\n-With this integration, you can now automate numerous processes without writing a single line of code. After you create a flow by using an Application Insights action, the flow automatically runs your Application Insights Analytics query.\n-\n-You can add additional actions as well. Microsoft Flow makes hundreds of actions available. For example, you can use Microsoft Flow to automatically send an email notification or create a bug in Azure DevOps. You can also use one of the many [templates](https://ms.flow.microsoft.com/connectors/shared_applicationinsights/?slug=azure-application-insights) that are available for the connector for Microsoft Flow. These templates speed up the process of creating a flow.\n-\n-<!--The Application Insights connector also works with [Azure Power Apps](https://powerapps.microsoft.com/) and [Azure Logic Apps](https://azure.microsoft.com/services/logic-apps/?v=17.23h). -->\n-\n-## Create a flow for Application Insights\n-\n-In this tutorial, you will learn how to create a flow that uses the Analytics autocluster algorithm to group attributes in the data for a web application. The flow automatically sends the results by email, just one example of how you can use Microsoft Flow and Application Insights Analytics together.\n-\n-### Step 1: Create a flow\n-\n-1. Sign in to [Microsoft Flow](https://flow.microsoft.com), and then select **My Flows**.\n-2. Click **New** then **Scheduled—from blank**.\n-\n-    ![Create new flow from scheduled blank](./media/automate-with-flow/1-create.png)\n-\n-### Step 2: Create a trigger for your flow\n-\n-1. In the popup **Build a scheduled flow**, fill out the name of your flow and how often you want your flow to run.\n-\n-    ![Set up schedule recurrence with entering frequency and interval](./media/automate-with-flow/2-schedule.png)\n-\n-1. Click **Create**.\n-\n-### Step 3: Add an Application Insights action\n-\n-1. Search for **Application Insights**.\n-2. Click **Azure Application Insights - Visualize Analytics query**.\n-\n-    ![Choose an action: Azure Application Insights Visualize Analytics query](./media/automate-with-flow/3-visualize.png)\n-\n-3. Select **New step**.\n-\n-### Step 4: Connect to an Application Insights resource\n-\n-To complete this step, you need an application ID and an API key for your resource. You can retrieve them from the Azure portal, as shown in the following diagram:\n-\n-![Application ID in the Azure portal](./media/automate-with-flow/5apiaccess.png)\n-\n-![API Key in the Azure portal](./media/automate-with-flow/6apikey.png)\n-\n-Provide a name for your connection, along with the application ID and API key.\n-\n-   ![Microsoft Flow connection window](./media/automate-with-flow/4-connection.png)\n-\n-If the connection box does not show up right away and instead goes straight to entering the query, click the ellipses at the top right of the box. Then select my connections or use an existing one.\n-\n-Click **Create**.\n-\n-### Step 5: Specify the Analytics query and chart type\n-This example query selects the failed requests within the last day and correlates them with exceptions that occurred as part of the operation. Analytics correlates them based on the operation_Id identifier. The query then segments the results by using the autocluster algorithm.\n-\n-When you create your own queries, verify that they are working properly in Analytics before you add it to your flow.\n-\n-- Add the following Analytics query, and select the HTML table chart type. Then select **New step**.\n-\n-    ```\n-    requests\n-    | where timestamp > ago(1d)\n-    | where success == \"False\"\n-    | project name, operation_Id\n-    | join ( exceptions\n-        | project problemId, outerMessage, operation_Id\n-    ) on operation_Id\n-    | evaluate autocluster()\n-    ```\n-    \n-    ![Analytics query configuration window](./media/automate-with-flow/5-query.png)\n-\n-### Step 6: Configure the flow to send email\n-\n-1. Search for **Office 365 Outlook**.\n-2. Click **Office 365 Outlook - Send an email**.\n-\n-    ![Office 365 Outlook selection window](./media/automate-with-flow/6-outlook.png)\n-\n-1. In the **Send an email** window:\n-\n-   a. Type the email address of the recipient.\n-\n-   b. Type a subject for the email.\n-\n-   c. Click anywhere in the **Body** box and then, on the dynamic content menu that opens at the right, select **Body**.\n-\n-   e. Select **Show advanced options**\n-\n-1. On the dynamic content menu:\n-\n-    a. Select **Attachment Name**.\n-\n-    b. Select **Attachment Content**.\n-    \n-    c. In the **Is HTML** box, select **Yes**.\n-\n-    ![Office 365 Outlook configuration](./media/automate-with-flow/7-email.png)\n-\n-### Step 7: Save and test your flow\n-\n-Click **Save**.\n-\n-You can wait for the trigger to run this action, or can click on ![beaker test icon](./media/automate-with-flow/testicon.png) **Test** in the top.\n-\n-After selecting **Test**:\n-\n-1. Select **I'll perform the trigger action**.\n-2. Select **Run Flow**.\n-\n-When the flow runs, the recipients you have specified in the email list receive an email message like the one below.\n-\n-![Sample email](./media/automate-with-flow/flow9.png)\n-\n-## Next steps\n-\n-- Learn more about creating [Analytics queries](../../azure-monitor/log-query/get-started-queries.md).\n-- Learn more about [Microsoft Flow](https://ms.flow.microsoft.com).\n-\n-<!--Link references-->"
  },
  {
    "Number": 104784,
    "Title": " Azure Monitor logic app connector",
    "ClosedAt": "2020-03-16T20:12:32Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/app/automate-with-logic-apps.md",
    "Addition": 3,
    "Delections": 0,
    "Changes": 3,
    "Patch": "@@ -10,6 +10,9 @@ ms.date: 03/11/2019\n \n Do you find yourself repeatedly running the same queries on your telemetry data to check whether your service is functioning properly? Are you looking to automate these queries for finding trends and anomalies and then build your own workflows around them? The Azure Application Insights connector  for Logic Apps is the right tool for this purpose.\n \n+> [!NOTE]\n+> The Azure Application Insights connector has been replaced with the [Azure Monitor connector](../platform/logicapp-flow-connector.md) that is integrated with Azure Active Directory instead of requiring an API key and also allows you to retrieve data from a Log Analytics workspace.\n+\n With this integration, you can automate numerous processes without writing a single line of code. You can create a logic app with the Application Insights connector to quickly automate any Application Insights process. \n \n You can add additional actions as well. The Logic Apps feature of Azure App Service makes hundreds of actions available. For example, by using a logic app, you can automatically send an email notification or create a bug in Azure DevOps. You can also use one of the many available [templates](https://docs.microsoft.com/azure/logic-apps/logic-apps-use-logic-app-templates) to help speed up the process of creating your logic app. "
  },
  {
    "Number": 104784,
    "Title": " Azure Monitor logic app connector",
    "ClosedAt": "2020-03-16T20:12:32Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/platform/flow-tutorial.md",
    "Addition": 0,
    "Delections": 76,
    "Changes": 76,
    "Patch": "@@ -1,76 +0,0 @@\n----\n-title: Automate Azure Monitor log processes with Microsoft Flow\n-description: Learn how you can use Microsoft Flow to quickly automate repeatable processes by using the Azure Log Analytics connector.\n-ms.subservice: logs\n-ms.topic: conceptual\n-author: bwren\n-ms.author: bwren\n-ms.date: 09/29/2017\n-\n----\n-\n-# Automate Azure Monitor log processes with the connector for Microsoft Flow\n-[Microsoft Flow](https://ms.flow.microsoft.com) allows you to create automated workflows using hundreds of actions for a variety of services. Output from one action can be used as input to another allowing you to create integration between different services.  The Azure Log Analytics connector for Microsoft Flow allow you to build workflows that include data retrieved by log queries from a Log Analytics workspace in Azure Monitor.\n-\n-[!INCLUDE [azure-monitor-log-analytics-rebrand](../../../includes/azure-monitor-log-analytics-rebrand.md)]\n-\n-For example, you can use Microsoft Flow to use Azure Monitor log data in an email notification from Office 365, create a bug in Azure DevOps, or post a Slack message.  You can trigger a workflow by a simple schedule or from some action in a connected service such as when a mail or a tweet is received.  \n-\n-The tutorial in this article shows you how to create a flow that automatically sends the results of an Azure Monitor log query by email, just one example of how you can use the Log Analytics connector in Microsoft Flow. \n-\n-\n-## Step 1: Create a flow\n-1. Sign in to [Microsoft Flow](https://flow.microsoft.com), and select **My Flows**.\n-2. Click **+ Create from blank**.\n-\n-## Step 2: Create a trigger for your flow\n-1. Click **Search hundreds of connectors and triggers**.\n-2. Type **Schedule** in the search box.\n-3. Select **Schedule**, and then select **Schedule - Recurrence**.\n-4. In the **Frequency** box select **Day** and in the **Interval** box, enter **1**.<br><br>![Microsoft Flow trigger dialog box](media/flow-tutorial/flow01.png)\n-\n-\n-## Step 3: Add a Log Analytics action\n-1. Click **+ New step**, and then click **Add an action**.\n-2. Search for **Log Analytics**.\n-3. Click **Azure Log Analytics – Run query and visualize results**.<br><br>![Log Analytics run query window](media/flow-tutorial/flow02.png)\n-\n-## Step 4: Configure the Log Analytics action\n-\n-1. Specify the details for your workspace including the Subscription ID, Resource Group, and Workspace Name.\n-2. Add the following log query to the **Query** window.  This is only a sample query, and you can replace with any other that returns data.\n-   ```\n-\tEvent\n-\t| where EventLevelName == \"Error\" \n-\t| where TimeGenerated > ago(1day)\n-\t| summarize count() by Computer\n-\t| sort by Computer\n-   ```\n-\n-2. Select **HTML Table** for the **Chart Type**.<br><br>![Log Analytics action](media/flow-tutorial/flow03.png)\n-\n-## Step 5: Configure the flow to send email\n-\n-1. Click **New step**, and then click **+ Add an action**.\n-2. Search for **Office 365 Outlook**.\n-3. Click **Office 365 Outlook – Send an email**.<br><br>![Office 365 Outlook selection window](media/flow-tutorial/flow04.png)\n-\n-4. Specify the email address of a recipient in the **To** window and a subject for the email in **Subject**.\n-5. Click anywhere in the **Body** box.  A **Dynamic content** window opens with values from previous actions.  \n-6. Select **Body**.  This is the results of the query in the Log Analytics action.\n-6. Click **Show advanced options**.\n-7. In the **Is HTML** box, select **Yes**.<br><br>![Office 365 email configuration window](media/flow-tutorial/flow05.png)\n-\n-## Step 6: Save and test your flow\n-1. In the **Flow name** box, add a name for your flow, and then click **Create flow**.<br><br>![Save flow](media/flow-tutorial/flow06.png)\n-2. The flow is now created and will run after a day which is the schedule you specified. \n-3. To immediately test the flow, click **Run Now** and then **Run flow**.<br><br>![Run flow](media/flow-tutorial/flow07.png)\n-3. When the flow completes, check the mail of the recipient that you specified.  You should have received a mail with a body similar to the following:<br><br>![Sample email](media/flow-tutorial/flow08.png)\n-\n-\n-## Next steps\n-\n-- Learn more about [log queries in Azure Monitor](../log-query/log-query-overview.md).\n-- Learn more about [Microsoft Flow](https://ms.flow.microsoft.com).\n-\n-"
  },
  {
    "Number": 104784,
    "Title": " Azure Monitor logic app connector",
    "ClosedAt": "2020-03-16T20:12:32Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/platform/logicapp-flow-connector.md",
    "Addition": 117,
    "Delections": 0,
    "Changes": 117,
    "Patch": "@@ -0,0 +1,117 @@\n+---\n+title: Use Azure Monitor Logs with Azure Logic Apps and Power Automate\n+description: Learn how you can use Azure Logic Apps and Power Automate to quickly automate repeatable processes by using the Azure Monitor connector.\n+ms.service:  azure-monitor\n+ms.subservice: logs\n+ms.topic: conceptual\n+author: bwren\n+ms.author: bwren\n+ms.date: 03/13/2020\n+\n+---\n+\n+# Azure Monitor Logs connector for Logic Apps and Flow\n+[Azure Logic Apps](/azure/logic-apps/) and [Power Automate](https://ms.flow.microsoft.com) allow you to create automated workflows using hundreds of actions for a variety of services. The Azure Monitor Logs connector allows you to build workflows that retrieve data from a Log Analytics workspace or an Application Insights application in Azure Monitor. This article describes the actions included with the connector and provides a walkthrough to build a workflow using this data.\n+\n+For example, you can create a logic app to use Azure Monitor log data in an email notification from Office 365, create a bug in Azure DevOps, or post a Slack message.  You can trigger a workflow by a simple schedule or from some action in a connected service such as when a mail or a tweet is received. \n+\n+## Actions\n+The following table describes the actions included with the Azure Monitor Logs connector. Both allow you to run a log query against a Log Analytics workspace or Application Insights application. The difference is in the way the data is returned.\n+\n+> [!NOTE]\n+> The Azure Monitor Logs connector replaces the [Azure Log Analytics connector](https://docs.microsoft.com/connectors/azureloganalytics/) and the [Azure Application Insights connector](https://docs.microsoft.com/connectors/applicationinsights/). This connector provides the same functionality as the others and is the preferred method for running a query against a Log Analytics workspace or an Application Insights application.\n+\n+\n+| Action | Description |\n+|:---|:---|\n+| [Run query and and list results](https://docs.microsoft.com/connectors/azuremonitorlogs/#run-query-and-list-results) | Returns each row as its own object. Use this action when you want to work with each row separately in the rest of the workflow. The action is typically followed by a [For each activity](../../logic-apps/logic-apps-control-flow-loops.md#foreach-loop). |\n+| [Run query and and visualize results](https://docs.microsoft.com/connectors/azuremonitorlogs/#run-query-and-visualize-results) | Returns all rows in the result set as a single formatted object. Use this action when you want to use the result set together in the rest of the workflow, such as sending the results in a mail.  |\n+\n+## Walkthroughs\n+The following tutorials illustrate the use of the Azure Monitor connectors in Azure Logic Apps. You can perform these same example with Power Automate, the only difference being how to you create the initial workflow and run it when complete. Configuration of the workflow and actions is the same between both. See [Create a flow from a template in Power Automate](https://docs.microsoft.com/power-automate/get-started-logic-template) to get started.\n+\n+\n+### Create a Logic App\n+\n+Go to **Logic Apps** in the Azure portal and click **Add**. Select a **Subscription**, **Resource group**, and **Region** to store the new logic app and then give it a unique name. You can turn on **Log Analytics** setting to collect information about runtime data and events as described in [Set up Azure Monitor logs and collect diagnostics data for Azure Logic Apps](../../logic-apps/monitor-logic-apps-log-analytics.md). This setting isn't required for using the Azure Monitor Logs connector.\n+\n+![Create logic app](media/logicapp-flow-connector/create-logic-app.png)\n+\n+\n+Click **Review + create** and then **Create**. When the deployment is complete, click **Go to resource** to open the **Logic Apps Designer**.\n+\n+### Create a trigger for the logic app\n+Under **Start with a common trigger**, select **Recurrence**. This creates a logic app that automatically runs at a regular interval. In the **Frequency** box of the action, select **Day** and in the **Interval** box, enter **1** to run the workflow once per day.\n+\n+![Recurrence action](media/logicapp-flow-connector/recurrence-action.png)\n+\n+## Walkthrough: Mail visualized results\n+The following tutorial shows you how to create a logic app that sends the results of an Azure Monitor log query by email. \n+\n+### Add Azure Monitor Logs action\n+Click **+ New step** to add an action that runs after the recurrence action. Under **Choose an action**, type **azure monitor** and then select **Azure Monitor Logs**.\n+\n+![Azure Monitor Logs action](media/logicapp-flow-connector/select-azure-monitor-connector.png)\n+\n+Click **Azure Log Analytics – Run query and visualize results**.\n+\n+![Run query and visualize results action](media/logicapp-flow-connector/select-query-action-visualize.png)\n+\n+\n+### Add Azure Monitor Logs action\n+\n+Select the **Subscription** and **Resource Group** for your Log Analytics workspace. Select *Log Analytics Workspace* for the **Resource Type** and then select the workspace's name under **Resource Name**.\n+\n+Add the following log query to the **Query** window.  \n+\n+```Kusto\n+Event\n+| where EventLevelName == \"Error\" \n+| where TimeGenerated > ago(1day)\n+| summarize TotalErrors=count() by Computer\n+| sort by Computer asc   \n+```\n+\n+Select *Set in query* for the **Time Range** and **HTML Table** for the **Chart Type**.\n+   \n+![Run query and visualize results action](media/logicapp-flow-connector/run-query-visualize-action.png)\n+\n+The mail will be sent by the account associated with the current connection. You can specify another account by clicking on **Change connection**.\n+\n+### Add email action\n+\n+Click **+ New step**, and then click **+ Add an action**. Under **Choose an action**, type **outlook** and then select **Office 365 Outlook**.\n+\n+![Select Outlook connector](media/logicapp-flow-connector/select-outlook-connector.png)\n+\n+Select **Send an email (V2)**.\n+\n+![Office 365 Outlook selection window](media/logicapp-flow-connector/select-mail-action.png)\n+\n+Click anywhere in the **Body** box to open a **Dynamic content** window opens with values from the previous actions in the logic app. Select **See more** and then **Body** which is the results of the query in the Log Analytics action.\n+\n+![Select body](media/logicapp-flow-connector/select-body.png)\n+\n+Specify the email address of a recipient in the **To** window and a subject for the email in **Subject**. \n+\n+![Mail action](media/logicapp-flow-connector/mail-action.png)\n+\n+\n+### Save and test your logic app\n+Click **Save** and then **Run** to perform a test run of the logic app.\n+\n+![Save and run](media/logicapp-flow-connector/save-run.png)\n+\n+\n+When the logic app completes, check the mail of the recipient that you specified.  You should have received a mail with a body similar to the following:\n+\n+![Sample email](media/logicapp-flow-connector/sample-mail.png)\n+\n+\n+\n+## Next steps\n+\n+- Learn more about [log queries in Azure Monitor](../log-query/log-query-overview.md).\n+- Learn more about [Logic Apps](/azure/logic-apps/)\n+- Learn more about [Microsoft Flow](https://ms.flow.microsoft.com).\n+"
  },
  {
    "Number": 107604,
    "Title": "VM Insights GA",
    "ClosedAt": "2020-03-16T19:01:41Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/faq.md",
    "Addition": 13,
    "Delections": 56,
    "Changes": 69,
    "Patch": "@@ -6,7 +6,7 @@ ms.subservice:\n ms.topic: conceptual\n author: bwren\n ms.author: bwren\n-ms.date: 01/23/2020\n+ms.date: 03/12/2020\n \n ---\n \n@@ -192,7 +192,7 @@ See [Network firewall requirements](platform/log-analytics-agent.md#network-fire\n \n ## Visualizations\n \n-### Why can't I can’t see View Designer?\n+### Why can't I see View Designer?\n \n View Designer is only available for users assigned with Contributor permissions or higher in the Log Analytics workspace.\n \n@@ -576,7 +576,7 @@ If you receive the error **Missing Subscription registration for Microsoft.Opera\n \n ### Is there support for RBAC enabled AKS clusters?\n \n-The Container Monitoring solution doesn’t support RBAC, but it is supported with Azure Monitor for Containers. The solution details page may not show the right information in the blades that show data for these clusters.\n+The Container Monitoring solution doesn't support RBAC, but it is supported with Azure Monitor for Containers. The solution details page may not show the right information in the blades that show data for these clusters.\n \n ### How do I enable log collection for containers in the kube-system namespace through Helm?\n \n@@ -588,7 +588,7 @@ To learn how to upgrade the agent, see [Agent management](insights/container-ins\n \n ### How do I enable multi-line logging?\n \n-Currently Azure Monitor for containers doesn’t support multi-line logging, but there are workarounds available. You can configure all the services to write in JSON format and then Docker/Moby will write them as a single line.\n+Currently Azure Monitor for containers doesn't support multi-line logging, but there are workarounds available. You can configure all the services to write in JSON format and then Docker/Moby will write them as a single line.\n \n For example, you can wrap your log as a JSON object as shown in the example below for a sample node.js application:\n \n@@ -604,7 +604,7 @@ console.log(json.stringify({\n This data will look like the following example in Azure Monitor for logs when you query for it:\n \n ```\n-LogEntry : ({“Hello\": \"This example has multiple lines:\",\"Docker/Moby\": \"will not break this into multiple lines\", \"and you will receive\":\"all of them in log analytics\", \"as one\": \"log entry\"}\n+LogEntry : ({\"Hello\": \"This example has multiple lines:\",\"Docker/Moby\": \"will not break this into multiple lines\", \"and you will receive\":\"all of them in log analytics\", \"as one\": \"log entry\"}\n \n ```\n \n@@ -622,81 +622,38 @@ If after you enable Azure Monitor for containers for an AKS cluster, you delete\n \n See the [Network firewall requirements](insights/container-insights-onboard.md#network-firewall-requirements) for the proxy and firewall configuration information required for the containerized agent with Azure, Azure US Government, and Azure China 21Vianet clouds.\n \n-## Azure Monitor for VMs (preview)\n+## Azure Monitor for VMs\n This Microsoft FAQ is a list of commonly asked questions about Azure Monitor for VMs. If you have any additional questions about the solution, go to the [discussion forum](https://feedback.azure.com/forums/34192--general-feedback) and post your questions. When a question is frequently asked, we add it to this article so that it can be found quickly and easily.\n \n ### Can I onboard to an existing workspace?\n If your virtual machines are already connected to a Log Analytics workspace, you may continue to use that workspace when onboarding to Azure Monitor for VMs, provided it is in one of the supported regions listed [here](insights/vminsights-enable-overview.md#prerequisites).\n \n-When onboarding, we configure performance counters for the workspace that will cause all of the VMs reporting data to the workspace to begin collecting this information for display and analysis in Azure Monitor for VMs.  As a result, you will see performance data from all of the VMs connected to the selected workspace.  The Health and Map features are only enabled for the VMs that you have specified to onboard.\n-\n-For more information on which performance counters are enabled, refer to our [enable overview](insights/vminsights-enable-overview.md#performance-counters-enabled) article.\n \n ### Can I onboard to a new workspace? \n If your VMs are not currently connected to an existing Log Analytics workspace, you need to create a new workspace to store your data. Creating a new default workspace is done automatically if you configure a single Azure VM for Azure Monitor for VMs through the Azure portal.\n \n-If you choose to use the script-based method, these steps are covered in the [Enable Azure Monitor for VMs (preview) using Azure PowerShell or Resource Manager template](insights/vminsights-enable-at-scale-powershell.md) article. \n+If you choose to use the script-based method, these steps are covered in the [Enable Azure Monitor for VMs using Azure PowerShell or Resource Manager template](insights/vminsights-enable-at-scale-powershell.md) article. \n \n ### What do I do if my VM is already reporting to an existing workspace?\n-If you are already collecting data from your virtual machines, you may have already configured it to report data to an existing Log Analytics workspace.  As long as that workspace is in one of our supported regions, you can enable Azure Monitor for VMs to that pre-existing workspace.  If the workspace you are already using is not in one of our supported regions, you won’t be able to onboard to Azure Monitor for VMs at this time.  We are actively working to support additional regions.\n+If you are already collecting data from your virtual machines, you may have already configured it to report data to an existing Log Analytics workspace.  As long as that workspace is in one of our supported regions, you can enable Azure Monitor for VMs to that pre-existing workspace.  If the workspace you are already using is not in one of our supported regions, you won't be able to onboard to Azure Monitor for VMs at this time.  We are actively working to support additional regions.\n \n->[!NOTE]\n->We configure performance counters for the workspace that affects all VMs that report to the   workspace, whether or not you have chosen to onboard them to Azure Monitor for VMs. For more details on how performance counters are configured for the workspace, please refer to our [documentation](platform/data-sources-performance-counters.md). For information about the counters configured for Azure Monitor for VMs, please refer to our [enable Azure Monitor for VMs](insights/vminsights-enable-overview.md#performance-counters-enabled) article.  \n \n ### Why did my VM fail to onboard?\n When onboarding an Azure VM from the Azure portal, the following steps occur:\n \n * A default Log Analytics workspace is created, if that option was selected.\n-* The performance counters are configured for selected workspace. If this step fails, you notice that some of the performance charts and tables aren't showing data for the VM you onboarded. You can fix this by running the PowerShell script documented [here](insights/vminsights-enable-at-scale-powershell.md#enable-performance-counters).\n * The Log Analytics agent is installed on Azure VMs using a VM extension, if determined it is required.  \n-* The Azure Monitor for VMs Map Dependency agent is installed on Azure VMs using an extension, if determined it is required.  \n-* Azure Monitor components supporting the Health feature are configured, if needed, and the VM is configured to report health data.\n+* The Azure Monitor for VMs Map Dependency agent is installed on Azure VMs using an extension, if determined it is required. \n \n-During the onboard process, we check for status on each of the above to return a notification status to you in the portal. Configuration of the workspace and the agent installation typically takes 5 to 10 minutes. Viewing monitoring and health data in the portal take an additional 5 to 10 minutes.  \n+During the onboard process, we check for status on each of the above to return a notification status to you in the portal. Configuration of the workspace and the agent installation typically takes 5 to 10 minutes. Viewing monitoring data in the portal take an additional 5 to 10 minutes.  \n \n If you have initiated onboarding and see messages indicating the VM needs to be onboarded,  allow for up to 30 minutes for the VM to complete the process. \n \n-### I only enabled Azure Monitor for VMs, Why do I see all my VMs monitored by the Health feature?\n-The Health feature is enabled for all VMs that are connected to the Log Analytics workspace, even when the action is initiated for a single VM.\n-\n-### Can I modify the schedule for when health criteria evaluates a condition?\n-No, the time period and frequency of health criteria can't be modified with this release. \n-\n-### Can I disable health criteria for a condition I don't need to monitor?\n-Health criteria can't be disabled in this release.\n-\n-### Are the health alert severities configurable?  \n-Health alert severity cannot be modified, they can only be enabled or disabled. Additionally, some alert severities update based on the state of health criteria. \n-\n-### If I reconfigure the settings of a particular health criteria, can it be scoped to a specific instance?  \n-If you modify any setting of a health criterion instance, all health criteria instances of the same type on the Azure VM are modified. For example, if the threshold of the disk free-space health criterion instance that corresponds to logical disk C: is modified, this threshold applies to all other logical disks that are discovered and monitored for the same VM.\n-\n-### Does the Health feature monitor logical processors and cores?\n-No, individual processor and logical processor level health criteria is not included for a Windows, only Total CPU utilization is monitored by default to effectively evaluate CPU pressure based on the total number of logical CPUs available to the Azure VM. \n-\n-### Are all health criteria thresholds configurable?  \n-Thresholds for health criteria that target a Windows VM aren’t modifiable, because their health states are set to *running* or *available*. When you query the health state from the [Workload Monitor API](https://docs.microsoft.com/rest/api/monitor/microsoft.workloadmonitor/components), it displays the *comparisonOperator* value of **LessThan** or **GreaterThan** with a *threshold* value of **4** for the service or entity if:\n-   - DNS Client Service Health – Service isn't running. \n-   - DHCP client service health – Service isn't running. \n-   - RPC Service Health – Service isn't running. \n-   - Windows firewall service health – Service isn't running.\n-   - Windows event log service health – Service isn't running. \n-   - Server service health – Service isn't running. \n-   - Windows remote management service health – Service isn't running. \n-   - File system error or corruption – Logical Disk is unavailable.\n-\n-Thresholds for the following Linux health criteria aren’t modifiable, because their health state is already set to *true*. The health state displays the *comparisonOperator* with a value **LessThan** and *threshold* value of **1** when queried from the Workload Monitoring API for the entity, depending on its context:\n-   - Logical Disk Status – Logical disk isn't online/ available\n-   - Disk Status – Disk isn't online/ available\n-   - Network Adapter Status -  Network adapter is disabled\n-\n-### How do I modify alerts that are included with the Health feature?\n-Alert rules that are defined for each health criterion aren't displayed in the Azure portal. You can enable or disable a health alert rule only in the [Workload Monitor API](https://docs.microsoft.com/rest/api/monitor/microsoft.workloadmonitor/components). Also, you can't assign an [Azure Monitor action group](platform/action-groups.md) for health alerts in the Azure portal. You can only use the notification setting API to configure an action group to be triggered whenever a health alert is fired. Currently, you can assign action groups against a VM so that all *health alerts* fired against the VM trigger the same action groups. Unlike traditional Azure alerts, there's no concept of a separate action group for each health alert rule. Additionally, only action groups that are configured to provide email or SMS notifications are supported when health alerts are triggered. \n \n-### I don’t see some or any data in the performance charts for my VM\n+### I don't see some or any data in the performance charts for my VM\n Our performance charts have been updated to use data stored in the *InsightsMetrics* table.  To see data in these charts you will need to upgrade to use the new VM Insights solution.  Please refer to our [GA FAQ](insights/vminsights-ga-release-faq.md) for additional information.\n \n-If you don’t see performance data in the disk table or in some of the performance charts then your performance counters may not be configured in the workspace. To resolve, run the following [PowerShell script](insights/vminsights-enable-at-scale-powershell.md#enable-with-powershell).\n+If you don't see performance data in the disk table or in some of the performance charts then your performance counters may not be configured in the workspace. To resolve, run the following [PowerShell script](insights/vminsights-enable-at-scale-powershell.md#enable-with-powershell).\n \n \n ### How is Azure Monitor for VMs Map feature different from Service Map?\n@@ -743,7 +700,7 @@ This approximation works well for protocols that are request/response based: a s\n ### Are their limitations if I am on the Log Analytics Free pricing plan?\n If you have configured Azure Monitor with a Log Analytics workspace using the *Free* pricing tier, Azure Monitor for VMs Map feature will only support five connected machines connected to the workspace. If you have five VMs connected to a free workspace, you disconnect one of the VMs and then later connect a new VM, the new VM is not monitored and reflected on the Map page.  \n \n-Under this condition, you will be prompted with the **Try Now** option when you open the VM and select **Insights (preview)** from the left-hand pane, even after it has been installed already on the VM.  However, you are not prompted with options as would normally occur if this VM were not onboarded to Azure Monitor for VMs. \n+Under this condition, you will be prompted with the **Try Now** option when you open the VM and select **Insights** from the left-hand pane, even after it has been installed already on the VM.  However, you are not prompted with options as would normally occur if this VM were not onboarded to Azure Monitor for VMs. \n \n \n ## Next steps"
  },
  {
    "Number": 107604,
    "Title": "VM Insights GA",
    "ClosedAt": "2020-03-16T19:01:41Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/service-map.md",
    "Addition": 5,
    "Delections": 5,
    "Changes": 10,
    "Patch": "@@ -22,7 +22,7 @@ This article describes the details of onboarding and using Service Map. For info\n * The Dependency agent installed on the Windows computer or Linux server.\n \n >[!NOTE]\n->If you have already deployed Service Map, you can now also view your maps in Azure Monitor for VMs, which includes additional features to monitor VM health and performance. To learn more, see [Azure Monitor for VMs overview](../../azure-monitor/insights/vminsights-overview.md). To learn about the differences between the Service Map solution and Azure Monitor for VMs Map feature, see the following [FAQ](../faq.md#azure-monitor-for-vms-preview).\n+>If you have already deployed Service Map, you can now also view your maps in Azure Monitor for VMs, which includes additional features to monitor VM health and performance. To learn more, see [Azure Monitor for VMs overview](../../azure-monitor/insights/vminsights-overview.md). To learn about the differences between the Service Map solution and Azure Monitor for VMs Map feature, see the following [FAQ](../faq.md#azure-monitor-for-vms).\n \n ## Sign in to Azure\n \n@@ -51,15 +51,15 @@ By using Service Map, you can effectively plan, accelerate, and validate Azure m\n \n ### Business continuity\n \n-If you are using Azure Site Recovery and need help defining the recovery sequence for your application environment, Service Map can automatically show you how systems rely on each other to ensure that your recovery plan is reliable. By choosing a critical server or group and viewing its clients, you can identify which front-end systems to recover after the server is restored and available. Conversely, by looking at critical servers’ back-end dependencies, you can identify which systems to recover before your focus systems are restored.\n+If you are using Azure Site Recovery and need help defining the recovery sequence for your application environment, Service Map can automatically show you how systems rely on each other to ensure that your recovery plan is reliable. By choosing a critical server or group and viewing its clients, you can identify which front-end systems to recover after the server is restored and available. Conversely, by looking at critical servers' back-end dependencies, you can identify which systems to recover before your focus systems are restored.\n \n ### Patch management\n \n Service Map enhances your use of the System Update Assessment by showing you which other teams and servers depend on your service, so you can notify them in advance before you take down your systems for patching. Service Map also enhances patch management by showing you whether your services are available and properly connected after they are patched and restarted.\n \n ## Mapping overview\n \n-Service Map agents gather information about all TCP-connected processes on the server where they’re installed and details about the inbound and outbound connections for each process.\n+Service Map agents gather information about all TCP-connected processes on the server where they're installed and details about the inbound and outbound connections for each process.\n \n From the list in the left pane, you can select machines or groups that have Service Map agents to visualize their dependencies over a specified time range. Machine dependency maps focus on a specific machine, and they show all the machines that are direct TCP clients or servers of that machine.  Machine Group maps show sets of servers and their dependencies.\n \n@@ -104,7 +104,7 @@ There, you can choose **Create new** and give the group a name.\n \n ### Viewing a Group\n \n-Once you’ve created some groups, you can view them by choosing the Groups tab.\n+Once you've created some groups, you can view them by choosing the Groups tab.\n \n ![Groups tab](media/service-map/machine-groups-tab.png)\n \n@@ -216,7 +216,7 @@ You can gather process details from operating-system metadata about running proc\n \n ![Process Properties pane](media/service-map/process-properties.png)\n \n-The **Process Summary** pane provides additional information about the process’s connectivity, including its bound ports, inbound and outbound connections, and failed connections.\n+The **Process Summary** pane provides additional information about the process's connectivity, including its bound ports, inbound and outbound connections, and failed connections.\n \n ![Process Summary pane](media/service-map/process-summary.png)\n "
  },
  {
    "Number": 107604,
    "Title": "VM Insights GA",
    "ClosedAt": "2020-03-16T19:01:41Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/vminsights-dependency-agent-maintenance.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -1,11 +1,11 @@\n ---\n-title: How to upgrade the Azure Monitor for VMs Dependency agent| Microsoft Docs\n+title: How to upgrade the Azure Monitor for VMs Dependency agent\n description: This article describes how to upgrade the Azure Monitor for VMs Dependency agent using command-line, setup wizard, and other methods.\n ms.subservice: \n ms.topic: conceptual\n author: bwren\n ms.author: bwren\n-ms.date: 09/30/2019\n+ms.date: 03/12/2020\n \n ---\n "
  },
  {
    "Number": 107604,
    "Title": "VM Insights GA",
    "ClosedAt": "2020-03-16T19:01:41Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/vminsights-enable-at-scale-policy.md",
    "Addition": 33,
    "Delections": 33,
    "Changes": 66,
    "Patch": "@@ -1,35 +1,35 @@\n ---\n-title: Enable Azure Monitor for VMs by using Azure Policy | Microsoft Docs\n+title: Enable Azure Monitor for VMs by using Azure Policy\n description: This article describes how you enable Azure Monitor for VMs for multiple Azure virtual machines or virtual machine scale sets by using Azure Policy.\n ms.subservice: \n ms.topic: conceptual\n author: bwren\n ms.author: bwren\n-ms.date: 10/15/2019\n+ms.date: 03/12/2020\n \n ---\n \n-# Enable Azure Monitor for VMs (preview) by using Azure Policy\n+# Enable Azure Monitor for VMs by using Azure Policy\n \n-This article explains how to enable Azure Monitor for VMs (preview) for Azure virtual machines or virtual machine scale sets by using Azure Policy. At the end of this process, you will have successfully configured enabling the Log Analytics and Dependency agents and identified virtual machines that aren't compliant.\n+This article explains how to enable Azure Monitor for VMs for Azure virtual machines or virtual machine scale sets by using Azure Policy. At the end of this process, you will have successfully configured enabling the Log Analytics and Dependency agents and identified virtual machines that aren't compliant.\n \n To discover, manage, and enable Azure Monitor for VMs for all of your Azure virtual machines or virtual machine scale sets, you can use either Azure Policy or Azure PowerShell. Azure Policy is the method we recommend because you can manage policy definitions to effectively govern your subscriptions to ensure consistent compliance and automatic enabling of newly provisioned VMs. These policy definitions:\n \n * Deploy the Log Analytics agent and the Dependency agent.\n * Report on compliance results.\n * Remediate for noncompliant VMs.\n \n-If you're interested in accomplishing these tasks with Azure PowerShell or an Azure Resource Manager template, see [Enable Azure Monitor for VMs (preview) using Azure PowerShell or Azure Resource Manager templates](vminsights-enable-at-scale-powershell.md).\n+If you're interested in accomplishing these tasks with Azure PowerShell or an Azure Resource Manager template, see [Enable Azure Monitor for VMs using Azure PowerShell or Azure Resource Manager templates](vminsights-enable-at-scale-powershell.md).\n \n ## Manage Policy Coverage feature overview\n \n-Originally, the experience with Azure Policy for managing and deploying the policy definitions for Azure Monitor for VMs was done exclusively from Azure Policy. The Manage Policy Coverage feature makes it simpler and easier to discover, manage, and enable at scale the **Enable Azure Monitor for VMs** initiative, which includes the policy definitions mentioned earlier. You can access this new feature from the **Get Started** tab in Azure Monitor for VMs. Select **Manage Policy Coverage** to open the **Azure Monitor for VMs Policy Coverage** page.\n+Azure Monitor for VMs Policy Coverage simplifies discovering, managing, and enabling at scale the **Enable Azure Monitor for VMs** initiative, which includes the policy definitions mentioned earlier. To access this feature,  select **Other onboarding options** from the **Get Started** tab in Azure Monitor for VMs. Select **Manage Policy Coverage** to open the **Azure Monitor for VMs Policy Coverage** page.\n \n ![Azure Monitor from VMs Get Started tab](./media/vminsights-enable-at-scale-policy/get-started-page.png)\n \n From here, you can check and manage coverage for the initiative across your management groups and subscriptions. You can understand how many VMs exist in each of the management groups and subscriptions and their compliance status.\n \n-![Azure Monitor for VMs Manage Policy page](./media/vminsights-enable-at-scale-policy/manage-policy-page-01.png)\n+![Azure Monitor for VMs Manage Policy page](media/vminsights-enable-at-scale-policy/manage-policy-page-01.png)\n \n This information is useful to help you plan and execute your governance scenario for Azure Monitor for VMs from one central location. While Azure Policy provides a compliance view when a policy or initiative is assigned to a scope, with this new page you can discover where the policy or initiative isn't assigned and assign it in place. All actions like assign, view, and edit redirect to Azure Policy directly. The **Azure Monitor for VMs Policy Coverage** page is an expanded and integrated experience for only the initiative **Enable Azure Monitor for VMs**.\n \n@@ -38,7 +38,7 @@ From this page, you also can configure your Log Analytics workspace for Azure Mo\n - Installs the Service Map solution.\n - Enables the operating system performance counters used by the performance charts, workbooks, and your custom log queries and alerts.\n \n-![Azure Monitor for VMs configure workspace](./media/vminsights-enable-at-scale-policy/manage-policy-page-02.png)\n+![Azure Monitor for VMs configure workspace](media/vminsights-enable-at-scale-policy/manage-policy-page-02.png)\n \n This option isn't related to any policy actions. It's available to provide an easy way to satisfy the [prerequisites](vminsights-enable-overview.md) required for enabling Azure Monitor for VMs.  \n \n@@ -74,33 +74,33 @@ The policy definitions for an Azure VM are listed in the following table.\n \n |Name |Description |Type |\n |-----|------------|-----|\n-|\\[Preview\\]: Enable Azure Monitor for VMs |Enable Azure Monitor for the virtual machines in the specified scope (management group, subscription, or resource group). Takes Log Analytics workspace as a parameter. |Initiative |\n-|\\[Preview\\]: Audit Dependency agent deployment – VM image (OS) unlisted |Reports VMs as noncompliant if the VM image (OS) isn't defined in the list and the agent isn't installed. |Policy |\n-|\\[Preview\\]: Audit Log Analytics agent deployment – VM image (OS) unlisted |Reports VMs as noncompliant if the VM image (OS) isn't defined in the list and the agent isn't installed. |Policy |\n-|\\[Preview\\]: Deploy Dependency agent for Linux VMs |Deploy Dependency agent for Linux VMs if the VM image (OS) is defined in the list and the agent isn't installed. |Policy |\n-|\\[Preview\\]: Deploy Dependency agent for Windows VMs |Deploy Dependency agent for Windows VMs if the VM image (OS) is defined in the list and the agent isn't installed. |Policy |\n-|\\[Preview\\]: Deploy Log Analytics agent for Linux VMs |Deploy Log Analytics agent for Linux VMs if the VM image (OS) is defined in the list and the agent isn't installed. |Policy |\n-|\\[Preview\\]: Deploy Log Analytics agent for Windows VMs |Deploy Log Analytics agent for Windows VMs if the VM image (OS) is defined in the list and the agent isn't installed. |Policy |\n+|Enable Azure Monitor for VMs |Enable Azure Monitor for the virtual machines in the specified scope (management group, subscription, or resource group). Takes Log Analytics workspace as a parameter. |Initiative |\n+|Audit Dependency agent deployment – VM image (OS) unlisted |Reports VMs as noncompliant if the VM image (OS) isn't defined in the list and the agent isn't installed. |Policy |\n+|Audit Log Analytics agent deployment – VM image (OS) unlisted |Reports VMs as noncompliant if the VM image (OS) isn't defined in the list and the agent isn't installed. |Policy |\n+|Deploy Dependency agent for Linux VMs |Deploy Dependency agent for Linux VMs if the VM image (OS) is defined in the list and the agent isn't installed. |Policy |\n+|Deploy Dependency agent for Windows VMs |Deploy Dependency agent for Windows VMs if the VM image (OS) is defined in the list and the agent isn't installed. |Policy |\n+|Deploy Log Analytics agent for Linux VMs |Deploy Log Analytics agent for Linux VMs if the VM image (OS) is defined in the list and the agent isn't installed. |Policy |\n+|Deploy Log Analytics agent for Windows VMs |Deploy Log Analytics agent for Windows VMs if the VM image (OS) is defined in the list and the agent isn't installed. |Policy |\n \n ### Policies for Azure virtual machine scale sets\n \n The policy definitions for an Azure virtual machine scale set are listed in the following table.\n \n |Name |Description |Type |\n |-----|------------|-----|\n-|\\[Preview\\]: Enable Azure Monitor for virtual machine scale sets |Enable Azure Monitor for the virtual machine scale sets in the specified scope (management group, subscription, or resource group). Takes Log Analytics workspace as a parameter. Note: If your scale set upgrade policy is set to Manual, apply the extension to all the VMs in the set by calling upgrade on them. In the CLI, this is `az vmss update-instances`. |Initiative |\n-|\\[Preview\\]: Audit Dependency agent deployment in virtual machine scale sets – VM image (OS) unlisted |Reports virtual machine scale set as noncompliant if the VM image (OS) isn't defined in the list and the agent isn't installed. |Policy |\n-|\\[Preview\\]: Audit Log Analytics agent deployment in virtual machine scale sets – VM image (OS) unlisted |Reports virtual machine scale set as noncompliant if the VM image (OS) isn't defined in the list and the agent isn't installed. |Policy |\n-|\\[Preview\\]: Deploy Dependency agent for Linux virtual machine scale sets |Deploy Dependency agent for Linux virtual machine scale sets if the VM image (OS) is defined in the list and the agent isn't installed. |Policy |\n-|\\[Preview\\]: Deploy Dependency agent for Windows virtual machine scale sets |Deploy Dependency agent for Windows virtual machine scale sets if the VM image (OS) is defined in the list and the agent isn't installed. |Policy |\n-|\\[Preview\\]: Deploy Log Analytics agent for Linux virtual machine scale sets |Deploy Log Analytics agent for Linux virtual machine scale sets if the VM Image (OS) is defined in the list and the agent isn't installed. |Policy |\n-|\\[Preview\\]: Deploy Log Analytics agent for Windows virtual machine scale sets |Deploy Log Analytics agent for Windows virtual machine scale sets if the VM image (OS) is defined in the list and the agent isn't installed. |Policy |\n+|Enable Azure Monitor for virtual machine scale sets |Enable Azure Monitor for the virtual machine scale sets in the specified scope (management group, subscription, or resource group). Takes Log Analytics workspace as a parameter. Note: If your scale set upgrade policy is set to Manual, apply the extension to all the VMs in the set by calling upgrade on them. In the CLI, this is `az vmss update-instances`. |Initiative |\n+|Audit Dependency agent deployment in virtual machine scale sets – VM image (OS) unlisted |Reports virtual machine scale set as noncompliant if the VM image (OS) isn't defined in the list and the agent isn't installed. |Policy |\n+|Audit Log Analytics agent deployment in virtual machine scale sets – VM image (OS) unlisted |Reports virtual machine scale set as noncompliant if the VM image (OS) isn't defined in the list and the agent isn't installed. |Policy |\n+|Deploy Dependency agent for Linux virtual machine scale sets |Deploy Dependency agent for Linux virtual machine scale sets if the VM image (OS) is defined in the list and the agent isn't installed. |Policy |\n+|Deploy Dependency agent for Windows virtual machine scale sets |Deploy Dependency agent for Windows virtual machine scale sets if the VM image (OS) is defined in the list and the agent isn't installed. |Policy |\n+|Deploy Log Analytics agent for Linux virtual machine scale sets |Deploy Log Analytics agent for Linux virtual machine scale sets if the VM Image (OS) is defined in the list and the agent isn't installed. |Policy |\n+|Deploy Log Analytics agent for Windows virtual machine scale sets |Deploy Log Analytics agent for Windows virtual machine scale sets if the VM image (OS) is defined in the list and the agent isn't installed. |Policy |\n \n Standalone policy (not included with the initiative) is described here:\n \n |Name |Description |Type |\n |-----|------------|-----|\n-|\\[Preview\\]: Audit Log Analytics workspace for VM – Report mismatch |Report VMs as noncompliant if they aren't logging to the Log Analytics workspace specified in the policy or initiative assignment. |Policy |\n+|Audit Log Analytics workspace for VM – Report mismatch |Report VMs as noncompliant if they aren't logging to the Log Analytics workspace specified in the policy or initiative assignment. |Policy |\n \n ### Assign the Azure Monitor initiative\n \n@@ -112,7 +112,7 @@ When you assign the policy or initiative, the scope selected in the assignment c\n \n 2. In the Azure portal, select **Monitor**. \n \n-3. Choose **Virtual Machines (preview)** in the **Insights** section.\n+3. Choose **Virtual Machines** in the **Insights** section.\n  \n 4. Select the **Get Started** tab. On the page, select **Manage Policy Coverage**.\n \n@@ -148,7 +148,7 @@ The following matrix maps each possible compliance state for the initiative.\n | **Lock** | You don't have sufficient privileges to the management group.<sup>1</sup> | \n | **Blank** | No policy is assigned. | \n \n-<sup>1</sup> If you don’t have access to the management group, ask an owner to provide access. Or, view compliance and manage assignments through the child management groups or subscriptions. \n+<sup>1</sup> If you don't have access to the management group, ask an owner to provide access. Or, view compliance and manage assignments through the child management groups or subscriptions. \n \n The following table maps each possible assignment status for the initiative.\n \n@@ -161,7 +161,7 @@ The following table maps each possible assignment status for the initiative.\n | **Blank** | No VMs exist or a policy isn't assigned. | \n | **Action** | Assign a policy or edit an assignment. | \n \n-<sup>1</sup> If you don’t have access to the management group, ask an owner to provide access. Or, view compliance and manage assignments through the child management groups or subscriptions.\n+<sup>1</sup> If you don't have access to the management group, ask an owner to provide access. Or, view compliance and manage assignments through the child management groups or subscriptions.\n \n ## Review and remediate the compliance results\n \n@@ -173,19 +173,19 @@ Based on the results of the policies included with the initiative, VMs are repor\n \n * Log Analytics agent or Dependency agent isn't deployed.  \n     This scenario is typical for a scope with existing VMs. To mitigate it, deploy the required agents by [creating remediation tasks](../../governance/policy/how-to/remediate-resources.md) on a noncompliant policy.  \n-    - \\[Preview\\]: Deploy Dependency agent for Linux VMs\n-    - \\[Preview\\]: Deploy Dependency agent for Windows VMs\n-    - \\[Preview\\]: Deploy Log Analytics agent for Linux VMs\n-    - \\[Preview\\]: Deploy Log Analytics agent for Windows VMs\n+    - Deploy Dependency agent for Linux VMs\n+    - Deploy Dependency agent for Windows VMs\n+    - Deploy Log Analytics agent for Linux VMs\n+    - Deploy Log Analytics agent for Windows VMs\n \n * VM image (OS) isn't identified in the policy definition.  \n     The criteria of the deployment policy include only VMs that are deployed from well-known Azure VM images. Check the documentation to see whether the VM OS is supported. If it isn't supported, duplicate the deployment policy and update or modify it to make the image compliant.  \n-    - \\[Preview\\]: Audit Dependency agent deployment – VM image (OS) unlisted\n-    - \\[Preview\\]: Audit Log Analytics agent deployment – VM image (OS) unlisted\n+    - Audit Dependency agent deployment – VM image (OS) unlisted\n+    - Audit Log Analytics agent deployment – VM image (OS) unlisted\n \n * VMs aren't logging in to the specified Log Analytics workspace.  \n     It's possible that some VMs in the initiative scope are logging in to a Log Analytics workspace other than the one that's specified in the policy assignment. This policy is a tool to identify which VMs are reporting to a noncompliant workspace.  \n-    - \\[Preview\\]: Audit Log Analytics workspace for VM – Report mismatch\n+    - Audit Log Analytics workspace for VM – Report mismatch\n \n ## Edit an initiative assignment\n "
  },
  {
    "Number": 107604,
    "Title": "VM Insights GA",
    "ClosedAt": "2020-03-16T19:01:41Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/vminsights-enable-at-scale-powershell.md",
    "Addition": 7,
    "Delections": 13,
    "Changes": 20,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Enable Azure Monitor for VMs (classic) with PowerShell or templates\n+title: Enable Azure Monitor for VMs with PowerShell or templates\n description: This article describes how you enable Azure Monitor for VMs for one or more Azure virtual machines or virtual machine scale sets by using Azure PowerShell or Azure Resource Manager templates.\n ms.subservice:\n ms.topic: conceptual\n@@ -9,23 +9,17 @@ ms.date: 10/14/2019\n \n ---\n \n-# Enable Azure Monitor for VMs (preview) using Azure PowerShell or Resource Manager templates\n+# Enable Azure Monitor for VMs using Azure PowerShell or Resource Manager templates\n \n [!INCLUDE [updated-for-az](../../../includes/updated-for-az.md)]\n \n-This article explains how to enable Azure Monitor for VMs (preview) for Azure virtual machines or virtual machine scale sets by using Azure PowerShell or Azure Resource Manager templates. At the end of this process, you will have successfully begun monitoring all of your virtual machines and learn if any are experiencing performance or availability issues.\n+This article explains how to enable Azure Monitor for VMs for Azure virtual machines or virtual machine scale sets by using Azure PowerShell or Azure Resource Manager templates. At the end of this process, you will have successfully begun monitoring all of your virtual machines and learn if any are experiencing performance or availability issues.\n \n ## Set up a Log Analytics workspace\n \n If you don't have a Log Analytics workspace, you need to create one. Review the methods that are suggested in the [Prerequisites](vminsights-enable-overview.md#log-analytics) section before you continue with the steps to configure it. Then you can finish the deployment of Azure Monitor for VMs by using the Azure Resource Manager template method.\n \n-### Enable performance counters\n-\n-If the Log Analytics workspace that's referenced by the solution isn't already configured to collect the performance counters required by the solution, you need to enable them. You can do so in one of two ways:\n-* Manually, as described in [Windows and Linux performance data sources in Log Analytics](../../azure-monitor/platform/data-sources-performance-counters.md)\n-* By downloading and running a PowerShell script that's available from the [Azure PowerShell Gallery](https://www.powershellgallery.com/packages/Enable-VMInsightsPerfCounters/1.1)\n-\n-### Install the ServiceMap solution\n+### Install the VMInsights solution\n \n This method includes a JSON template that specifies the configuration for enabling the solution components in your Log Analytics workspace.\n \n@@ -59,7 +53,7 @@ To use the Azure CLI, you first need to install and use the CLI locally. You mus\n                     {\n                         \"apiVersion\": \"2015-11-01-preview\",\n                         \"location\": \"[parameters('WorkspaceLocation')]\",\n-                        \"name\": \"[concat('ServiceMap', '(', parameters('WorkspaceName'),')')]\",\n+                        \"name\": \"[concat('VMInsights', '(', parameters('WorkspaceName'),')')]\",\n                         \"type\": \"Microsoft.OperationsManagement/solutions\",\n                         \"dependsOn\": [\n                             \"[concat('Microsoft.OperationalInsights/workspaces/', parameters('WorkspaceName'))]\"\n@@ -69,9 +63,9 @@ To use the Azure CLI, you first need to install and use the CLI locally. You mus\n                         },\n \n                         \"plan\": {\n-                            \"name\": \"[concat('ServiceMap', '(', parameters('WorkspaceName'),')')]\",\n+                            \"name\": \"[concat('VMInsights', '(', parameters('WorkspaceName'),')')]\",\n                             \"publisher\": \"Microsoft\",\n-                            \"product\": \"[Concat('OMSGallery/', 'ServiceMap')]\",\n+                            \"product\": \"[Concat('OMSGallery/', 'VMInsights')]\",\n                             \"promotionCode\": \"\"\n                         }\n                     }"
  },
  {
    "Number": 107604,
    "Title": "VM Insights GA",
    "ClosedAt": "2020-03-16T19:01:41Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/vminsights-enable-hybrid-cloud.md",
    "Addition": 8,
    "Delections": 8,
    "Changes": 16,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Enable Azure Monitor (preview) for a hybrid environment | Microsoft Docs\n+title: Enable Azure Monitor for a hybrid environment | Microsoft Docs\n description: This article describes how you enable Azure Monitor for VMs for a hybrid cloud environment that contains one or more virtual machines.\n ms.subservice:\n ms.topic: conceptual\n@@ -9,11 +9,11 @@ ms.date: 10/15/2019\n \n ---\n \n-# Enable Azure Monitor for VMs (preview) for a hybrid environment\n+# Enable Azure Monitor for VMs for a hybrid environment\n \n [!INCLUDE [updated-for-az](../../../includes/updated-for-az.md)]\n \n-This article explains how to enable Azure Monitor for VMs (preview) for virtual machines or physical computers hosted in your datacenter or other cloud environment. At the end of this process, you will have successfully begun monitoring your virtual machines in your environment and learn if they are experiencing any performance or availability issues.\n+This article explains how to enable Azure Monitor for VMs for virtual machines or physical computers hosted in your datacenter or other cloud environment. At the end of this process, you will have successfully begun monitoring your virtual machines in your environment and learn if they are experiencing any performance or availability issues.\n \n Before you get started, be sure to review the [prerequisites](vminsights-enable-overview.md) and verify that your subscription and resources meet the requirements. Review the requirements and deployment methods for the [Log Analytics Linux and Windows agent](../../log-analytics/log-analytics-agent-overview.md).\n \n@@ -107,7 +107,7 @@ sudo sh InstallDependencyAgent-Linux64.bin -s\n To deploy the Dependency agent using Desired State Configuration (DSC), you can use the xPSDesiredStateConfiguration module with the following example code:\n \n ```powershell\n-configuration ServiceMap {\n+configuration VMInsights {\n \n     Import-DscResource -ModuleName xPSDesiredStateConfiguration\n \n@@ -180,7 +180,7 @@ To use the Azure CLI, you first need to install and use the CLI locally. You mus\n                     {\n                         \"apiVersion\": \"2015-11-01-preview\",\n                         \"location\": \"[parameters('WorkspaceLocation')]\",\n-                        \"name\": \"[concat('ServiceMap', '(', parameters('WorkspaceName'),')')]\",\n+                        \"name\": \"[concat('VMInsights', '(', parameters('WorkspaceName'),')')]\",\n                         \"type\": \"Microsoft.OperationsManagement/solutions\",\n                         \"dependsOn\": [\n                             \"[concat('Microsoft.OperationalInsights/workspaces/', parameters('WorkspaceName'))]\"\n@@ -190,9 +190,9 @@ To use the Azure CLI, you first need to install and use the CLI locally. You mus\n                         },\n \n                         \"plan\": {\n-                            \"name\": \"[concat('ServiceMap', '(', parameters('WorkspaceName'),')')]\",\n+                            \"name\": \"[concat('VMInsights', '(', parameters('WorkspaceName'),')')]\",\n                             \"publisher\": \"Microsoft\",\n-                            \"product\": \"[Concat('OMSGallery/', 'ServiceMap')]\",\n+                            \"product\": \"[Concat('OMSGallery/', 'VMInsights')]\",\n                             \"promotionCode\": \"\"\n                         }\n                     }\n@@ -236,7 +236,7 @@ If your Dependency agent installation succeeded, but you don't see your computer\n 3. Is the computer sending log and perf data to Azure Monitor Logs? Perform the following query for your computer:\n \n     ```Kusto\n-\tUsage | where Computer == \"computer-name\" | summarize sum(Quantity), any(QuantityUnit) by DataType\n+    Usage | where Computer == \"computer-name\" | summarize sum(Quantity), any(QuantityUnit) by DataType\n     ```\n \n     Did it return one or more results? Is the data recent? If so, your Log Analytics agent is operating correctly and communicating with the service. If not, check the agent on your server: [Log Analytics agent for Windows troubleshooting](../platform/agent-windows-troubleshoot.md) or [Log Analytics agent for Linux troubleshooting](../platform/agent-linux-troubleshoot.md)."
  },
  {
    "Number": 107604,
    "Title": "VM Insights GA",
    "ClosedAt": "2020-03-16T19:01:41Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/insights/vminsights-enable-overview.md",
    "Addition": 12,
    "Delections": 56,
    "Changes": 68,
    "Patch": "@@ -1,21 +1,21 @@\n ---\n-title: Enable Azure Monitor for VMs (preview) overview | Microsoft Docs\n+title: Enable Azure Monitor for VMs overview\n description: Learn how to deploy and configure Azure Monitor for VMs. Find out the system requirements.\n ms.subservice: \n ms.topic: conceptual\n author: bwren\n ms.author: bwren\n-ms.date: 11/14/2019\n+ms.date: 03/11/2020\n \n ---\n \n-# Enable Azure Monitor for VMs (preview) overview\n+# Enable Azure Monitor for VMs overview\n \n-This article provides an overview of the options available to set up Azure Monitor for VMs. Use Azure Monitor for VMs to monitor health and performance. Discover application dependencies that run on Azure virtual machines (VMs) and virtual machine scale sets, on-premises VMs, or VMs hosted in another cloud environment.  \n+This article provides an overview of the options available to enable Azure Monitor for VMs on your virtual machines to monitor health and performance. Discover application dependencies that run on Azure virtual machines (VMs) and virtual machine scale sets, on-premises VMs, or VMs hosted in another cloud environment.  \n \n To set up Azure Monitor for VMs:\n \n-* Enable a single Azure VM or virtual machine scale set by selecting **Insights (preview)** directly from the VM or virtual machine scale set.\n+* Enable a single Azure VM or virtual machine scale set by selecting **Insights** directly from the VM or virtual machine scale set.\n * Enable two or more Azure VMs and virtual machine scale sets by using Azure Policy. This method ensures that on existing and new VMs and scale sets, the required dependencies are installed and properly configured. Noncompliant VMs and scale sets are reported, so you can decide whether to enable them and to remediate them.\n * Enable two or more Azure VMs or virtual machine scale sets across a specified subscription or resource group by using PowerShell.\n * Enable Azure Monitor for VMs to monitor VMs or physical computers hosted in your corporate network or other cloud environment.\n@@ -51,20 +51,20 @@ Azure Monitor for VMs supports a Log Analytics workspace in the following region\n - Australia Southeast\n \n >[!NOTE]\n->You can deploy Azure VMs from any region. These VMs aren't limited to the regions supported by the Log Analytics workspace.\n+>You can monitor Azure VMs in any region. The VMs themselves aren't limited to the regions supported by the Log Analytics workspace.\n >\n \n-If you don't have a workspace, you can create one by using one of these resources:\n-* [The Azure CLI](../../azure-monitor/learn/quick-create-workspace-cli.md)\n+If you don't have a Log Analytics workspace, you can create one by using one of the  resources:\n+* [Azure CLI](../../azure-monitor/learn/quick-create-workspace-cli.md)\n * [PowerShell](../../azure-monitor/learn/quick-create-workspace-posh.md)\n-* [The Azure portal](../../azure-monitor/learn/quick-create-workspace.md)\n+* [Azure portal](../../azure-monitor/learn/quick-create-workspace.md)\n * [Azure Resource Manager](../../azure-monitor/platform/template-workspace-configuration.md)\n \n You can also create a workspace while you're enabling monitoring for a single Azure VM or virtual machine scale set in the Azure portal.\n \n To set up an at-scale scenario that uses Azure Policy, Azure PowerShell, or Azure Resource Manager templates, in your Log Analytics workspace:\n \n-* Install the ServiceMap and InfrastructureInsights solutions. You can complete this installation by using a provided Azure Resource Manager template. Or on the **Get Started** tab, select **Configure Workspace**.\n+* Install the *ServiceMap* and *InfrastructureInsights* solutions. You can complete this installation by using a provided Azure Resource Manager template. Or on the **Get Started** tab in the Azure portal, select **Configure Workspace**.\n * Configure the Log Analytics workspace to collect performance counters.\n \n To configure your workspace for the at-scale scenario, use one of the following methods:\n@@ -180,61 +180,17 @@ To enable and access the features in Azure Monitor for VMs, you must have the *L\n \n For more information about how to control access to a Log Analytics workspace, see [Manage workspaces](../../azure-monitor/platform/manage-access.md).\n \n-## How to enable Azure Monitor for VMs (preview)\n+## How to enable Azure Monitor for VMs\n \n Enable Azure Monitor for VMs by using one of the methods described in this table:\n \n | Deployment state | Method | Description |\n |------------------|--------|-------------|\n-| Single Azure VM or virtual machine scale set | [Enable from the VM](vminsights-enable-single-vm.md) | You can enable a single Azure VM by selecting **Insights (preview)** directly from the VM or virtual machine scale set. |\n+| Single Azure VM or virtual machine scale set | [Enable from the VM](vminsights-enable-single-vm.md) | You can enable a single Azure VM by selecting **Insights** directly from the VM or virtual machine scale set. |\n | Multiple Azure VMs or virtual machine scale sets | [Enable through Azure Policy](vminsights-enable-at-scale-policy.md) | You can enable multiple Azure VMs by using Azure Policy and available policy definitions. |\n | Multiple Azure VMs or virtual machine scale sets | [Enable through Azure PowerShell or Azure Resource Manager templates](vminsights-enable-at-scale-powershell.md) | You can enable multiple Azure VMs or virtual machine scale sets across a specified subscription or resource group by using Azure PowerShell or Azure Resource Manager templates. |\n | Hybrid cloud | [Enable for the hybrid environment](vminsights-enable-hybrid-cloud.md) | You can deploy to VMs or physical computers that are hosted in your datacenter or other cloud environments. |\n \n-## Performance counters enabled \n-\n-Azure Monitor for VMs configures a Log Analytics workspace to collect the performance counters that it uses. The following tables list the objects and counters that are collected every 60 seconds.\n-\n->[!NOTE]\n->The following list of performance counters enabled by Azure Monitor for VMs does not limit you from enabling additional counters you need to collect from VMs reporting to the workspace. Also, if you disable these counters, it will prevent the set of performance charts included with the Performance feature from showing resource utilization from your VMs.\n-\n-### Windows performance counters\n-\n-|Object name |Counter name |\n-|------------|-------------|\n-|LogicalDisk |% Free Space |\n-|LogicalDisk |Avg. Disk sec/Read |\n-|LogicalDisk |Avg. Disk sec/Transfer |\n-|LogicalDisk |Avg. Disk sec/Write |\n-|LogicalDisk |Disk Bytes/sec |\n-|LogicalDisk |Disk Read Bytes/sec |\n-|LogicalDisk |Disk Reads/sec |\n-|LogicalDisk |Disk Transfers/sec |\n-|LogicalDisk |Disk Write Bytes/sec |\n-|LogicalDisk |Disk Writes/sec |\n-|LogicalDisk |Free Megabytes |\n-|Memory |Available MBytes |\n-|Network Adapter |Bytes Received/sec |\n-|Network Adapter |Bytes Sent/sec |\n-|Processor |% Processor Time |\n-\n-### Linux performance counters\n-\n-|Object name |Counter name |\n-|------------|-------------|\n-|Logical Disk |% Used Space |\n-|Logical Disk |Disk Read Bytes/sec |\n-|Logical Disk |Disk Reads/sec |\n-|Logical Disk |Disk Transfers/sec |\n-|Logical Disk |Disk Write Bytes/sec |\n-|Logical Disk |Disk Writes/sec |\n-|Logical Disk |Free Megabytes |\n-|Logical Disk |Logical Disk Bytes/sec |\n-|Memory |Available MBytes Memory |\n-|Network |Total Bytes Received |\n-|Network |Total Bytes Transmitted |\n-|Processor |% Processor Time |\n-\n ## Management packs\n \n When Azure Monitor for VMs is enabled and configured with a Log Analytics workspace, a management pack is forwarded to all the Windows computers reporting to that workspace. If you have [integrated your System Center Operations Manager management group](../../azure-monitor/platform/om-agents.md) with the Log Analytics workspace, the Service Map management pack is deployed from the management group to the Windows computers reporting to the management group.  "
  },
  {
    "Number": 107852,
    "Title": "Update backup-azure-monitoring-use-azuremonitor.md",
    "ClosedAt": "2020-03-16T19:21:22Z",
    "User": "john-par",
    "FileName": "articles/backup/backup-azure-monitoring-use-azuremonitor.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -24,7 +24,7 @@ In Azure Monitor, you can create your own alerts in a Log Analytics workspace. I\n > [!IMPORTANT]\n > For information on the cost of creating this query, see [Azure Monitor pricing](https://azure.microsoft.com/pricing/details/monitor/).\n \n-Open the **Logs** section of the Log Analytics workspace and write a query your own Logs. When you select **New Alert Rule**, the Azure Monitor alert-creation page opens, as shown in the following image.\n+Open the **Logs** section of the Log Analytics workspace and create a query for your own Logs. When you select **New Alert Rule**, the Azure Monitor alert-creation page opens, as shown in the following image.\n \n ![Create an alert in a Log Analytics workspace](media/backup-azure-monitoring-laworkspace/custom-alert.png)\n "
  },
  {
    "Number": 107809,
    "Title": "Azure Monitor query optimization headers",
    "ClosedAt": "2020-03-16T13:41:28Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/log-query/query-optimization.md",
    "Addition": 25,
    "Delections": 8,
    "Changes": 33,
    "Patch": "@@ -55,6 +55,8 @@ Query processing time is spent on:\n \n Other than time spent in the query processing nodes, there is additional time that is spend by Azure Monitor Logs to: authenticate the user and verify that they are permitted to access this data, locate the data store, parse the query, and allocate the query processing nodes. This time is not included in the query total CPU time.\n \n+### Early filtering of records prior of using high CPU functions\n+\n Some of the query commands and functions are heavy in their CPU consumption. This is especially true for commands that parse JSON and XML or extract complex regular expressions. Such parsing can happen explicitly via [parse_json()](/azure/kusto/query/parsejsonfunction) or [parse_xml()](/azure/kusto/query/parse-xmlfunction) functions or implicitly when referring to dynamic columns.\n \n These functions consume CPU in proportion to the number of rows they are processing. The most efficient optimization is to add where conditions early in the query that can filter out as many records as possible before the CPU intensive function is executed.\n@@ -82,6 +84,8 @@ SecurityEvent\n | where FileHash != \"\" // No need to filter out %SYSTEM32 here as it was removed before\n ```\n \n+### Avoid using evaluated where clauses\n+\n Queries that contain [where](/azure/kusto/query/whereoperator) clauses on an evaluated column rather than on columns that are physically present in the dataset lose efficiency. Filtering on evaluated columns prevents some system optimizations when large sets of data are handled.\n For example, the following queries produce exactly the same result but the second one is more efficient as the [where](/azure/kusto/query/whereoperator) condition refers to built-in column\n \n@@ -100,6 +104,8 @@ Heartbeat\n | summarize count() by Computer\n ```\n \n+### Use effective aggregation commands and dimmentions in summarize and join\n+\n While some aggregation commands like [max()](/azure/kusto/query/max-aggfunction), [sum()](/azure/kusto/query/sum-aggfunction), [count()](/azure/kusto/query/count-aggfunction), and [avg()](/azure/kusto/query/avg-aggfunction) have low CPU impact due to their logic, other are  more complex and include heuristics and estimations that allow them to be executed efficiently. For example, [dcount()](/azure/kusto/query/dcount-aggfunction) uses the HyperLogLog algorithm to provide close estimation to distinct count of large sets of data without actually counting each value; the percentile functions are doing similar approximations using the nearest rank percentile algorithm. Several of the commands include optional parameters to reduce their impact. For example, the [makeset()](/azure/kusto/query/makeset-aggfunction) function has an optional parameter to define the maximum set size, which significantly affects the CPU and memory.\n \n [Join](/azure/kusto/query/joinoperator?pivots=azuremonitor) and [summarize](/azure/kusto/query/summarizeoperator) commands may cause high CPU utilization when they are processing a large set of data. Their complexity is directly related to the number of possible values, referred to as *cardinality*, of the columns that are using as the `by` in summarize or as the join attributes. For explanation and optimization of join and summarize, see their documentation articles and optimization tips.\n@@ -150,9 +156,12 @@ Heartbeat\n ## Data used for processed query\n \n A critical factor in the processing of the query is the volume of data that is scanned and used for the query processing. Azure Data Explorer uses aggressive optimizations that dramatically reduce the data volume compared to other data platforms. Still, there are critical factors in the query that can impact the data volume that is used.\n+\n In Azure Monitor Logs, the **TimeGenerated** column is used as a way to index the data. Restricting the **TimeGenerated** values to as narrow a range as possible will make a significant improvement to query performance by significantly limiting the amount of data that has to be processed.\n \n-Another factor that increases the data that is process is the use of large number of tables. This usually happens when `search *` and `union *` commands are used. These commands force the system to evaluate and scan data from all tables in the workspace. In some cases, there might be hundreds of tables in the workspace. Try to avoid as much as possible using “search *” or any search without scoping it to a specific table.\n+### Avoid unnecessary use of search and union operators\n+\n+Another factor that increases the data that is process is the use of large number of tables. This usually happens when `search *` and `union *` commands are used. These commands force the system to evaluate and scan data from all tables in the workspace. In some cases, there might be hundreds of tables in the workspace. Try to avoid as much as possible using \"search *\" or any search without scoping it to a specific table.\n \n For example, the following queries produce exactly the same result but the last one is by far the most efficient:\n \n@@ -174,6 +183,8 @@ Perf\n | summarize count(), avg(CounterValue)  by Computer\n ```\n \n+### Add early filters to the query\n+\n Another method to reduce the data volume is to have [where](/azure/kusto/query/whereoperator) conditions early in the query. The Azure Data Explorer platform includes a cache that lets it know which partitions include data that is relevant for a specific where condition. For example, if a query contains `where EventID == 4624` then it would distribute the query only to nodes that handle partitions with matching events.\n \n The following example queries produce exactly the same result but the second one is more efficient:\n@@ -190,7 +201,9 @@ SecurityEvent\n | summarize LoginSessions = dcount(LogonGuid) by Account\n ```\n \n-Since Azure Data Explorer is a columnar data store, retrieval of every column is independent of the others. The number of columns that are retrieved directly influences the overall data volume. You should only include the columns in the output that are needed by [summarizing](/azure/kusto/query/summarizeoperator) the results or [projecting](/azure/kusto/query/projectoperator) the specific columns. Azure Data Explorer has several optimizations to reduce the number of retrieved columns. If it determines that a column isn’t needed, for example if it's not referenced in the [summarize](/azure/kusto/query/summarizeoperator) command, it won’t retrieve it.\n+### Reduce the number of columns that is retrieved\n+\n+Since Azure Data Explorer is a columnar data store, retrieval of every column is independent of the others. The number of columns that are retrieved directly influences the overall data volume. You should only include the columns in the output that are needed by [summarizing](/azure/kusto/query/summarizeoperator) the results or [projecting](/azure/kusto/query/projectoperator) the specific columns. Azure Data Explorer has several optimizations to reduce the number of retrieved columns. If it determines that a column isn't needed, for example if it's not referenced in the [summarize](/azure/kusto/query/summarizeoperator) command, it won't retrieve it.\n \n For example, the second query may process three times more data since it needs to fetch not one column but three:\n \n@@ -214,6 +227,8 @@ The time range can be set using the time range selector in the Log Analytics scr\n An alternative method is to explicitly include a [where](/azure/kusto/query/whereoperator) condition on **TimeGenerated** in the query. You should use this method as it assures that the time span is fixed, even when the query is used from a different interface.\n You should ensure that all parts of the query have **TimeGenerated** filters. When a query has sub-queries fetching data from various tables or the same table, each has to include its own [where](/azure/kusto/query/whereoperator) condition.\n \n+### Make sure all sub-queries have TimeGenerated filter\n+\n For example, in the following query, while the **Perf** table will be scanned only for the last day, the **Heartbeat** table will be scanned for all of its history, which might be up to two years:\n \n ```Kusto\n@@ -283,6 +298,8 @@ Heartbeat\n | summarize min(TimeGenerated) by Computer\n ```\n \n+### Time span measurement limitations\n+\n The measurement is always larger than the actual time specified. For example, if the filter on the query is 7 days, the system might scan 7.5 or 8.1 days. This is because the system is partitioning the data into chunks in variable size. To assure that all relevant records are scanned, it scans the entire partition that might cover several hours and even more than a day.\n \n There are several cases where the system cannot provide an accurate measurement of the time range. This happens in most of the cases where the query's span less than a day or in multi-workspace queries.\n@@ -298,9 +315,9 @@ While some queries require usage of old data, there are cases where old data is\n \n Such cases can be for example:\n \n-- Not setting the time range in Log Analytics with a sub-query that isn’t limited. See example above.\n+- Not setting the time range in Log Analytics with a sub-query that isn't limited. See example above.\n - Using the API without the time range optional parameters.\n-- Using a client that doesn’t force a time range such as the Power BI connector.\n+- Using a client that doesn't force a time range such as the Power BI connector.\n \n See examples and notes in the pervious section as they are also relevant in this case.\n \n@@ -337,10 +354,10 @@ To efficiently execute a query, it is partitioned and distributed to compute nod\n Query behaviors that can reduce parallelism include:\n \n - Use of serialization and window functions such as the [serialize operator](/azure/kusto/query/serializeoperator), [next()](/azure/kusto/query/nextfunction), [prev()](/azure/kusto/query/prevfunction), and the [row](/azure/kusto/query/rowcumsumfunction) functions. Time series and user analytics functions can be used in some of these cases. Inefficient serialization may also happen if the following operators are used not at the end of the query: [range](/azure/kusto/query/rangeoperator), [sort](/azure/kusto/query/sortoperator), [order](/azure/kusto/query/orderoperator), [top](/azure/kusto/query/topoperator), [top-hitters](/azure/kusto/query/tophittersoperator), [getschema](/azure/kusto/query/getschemaoperator).\n--\tUsage of [dcount()](/azure/kusto/query/dcount-aggfunction) aggregation function force the system to have central copy of the distinct values. When the scale of data is high, consider using the dcount function optional parameters to reduced accuracy.\n--\tIn many cases, the [join](/azure/kusto/query/joinoperator?pivots=azuremonitor) operator lowers overall parallelism. Examine shuffle join as an alternative when performance is problematic.\n--\tIn resource-scope queries, the pre-execution RBAC checks may linger in situations where there is very large number of RBAC assignments. This may lead to longer checks that would result in lower parallelism. For example, a query is executed on a subscription where there are thousands of resources and each resource has many role assignments in the resource level, not on the subscription or resource group.\n--\tIf a query is processing small chunks of data, its parallelism will be low as the system will not spread it across many compute nodes.\n+-    Usage of [dcount()](/azure/kusto/query/dcount-aggfunction) aggregation function force the system to have central copy of the distinct values. When the scale of data is high, consider using the dcount function optional parameters to reduced accuracy.\n+-    In many cases, the [join](/azure/kusto/query/joinoperator?pivots=azuremonitor) operator lowers overall parallelism. Examine shuffle join as an alternative when performance is problematic.\n+-    In resource-scope queries, the pre-execution RBAC checks may linger in situations where there is very large number of RBAC assignments. This may lead to longer checks that would result in lower parallelism. For example, a query is executed on a subscription where there are thousands of resources and each resource has many role assignments in the resource level, not on the subscription or resource group.\n+-    If a query is processing small chunks of data, its parallelism will be low as the system will not spread it across many compute nodes.\n \n \n "
  },
  {
    "Number": 107761,
    "Title": "Azure Monitor data source link fix",
    "ClosedAt": "2020-03-16T13:04:46Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/platform/data-platform-logs.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -80,7 +80,7 @@ Azure Monitor can collect log data from a variety of sources both within Azure a\n \n | Data | Description |\n |:---|:---|\n-| Resource diagnostics | Configure Diagnostic settings to write to diagnostic data, including metrics to a Log Analytics workspace. See [Stream Azure resource logs to Log Analytics](resource-logs-collect-storage.md). |\n+| Resource diagnostics | Configure Diagnostic settings to write to diagnostic data, including metrics to a Log Analytics workspace. See [Stream Azure resource logs to Log Analytics](resource-logs-collect-workspace.md). |\n | Monitoring solutions | Monitoring solutions write data they collect to their Log Analytics workspace. See [Data collection details for management solutions in Azure](../insights/solutions-inventory.md) for a list of solutions. See [Monitoring solutions in Azure Monitor](../insights/solutions.md) for details on installing and using solutions. |\n | Metrics | Send platform metrics for Azure Monitor resources to a Log Analytics workspace to retain log data for longer periods and to perform complex analysis with other data types using the [Kusto query language](/azure/kusto/query/). See [Stream Azure resource logs to Log Analytics](resource-logs-collect-storage.md). |\n | Azure table storage | Collect data from Azure storage where some Azure resources write monitoring data. See  [Use Azure blob storage for IIS and Azure table storage for events with Log Analytics](diagnostics-extension-logs.md). |"
  },
  {
    "Number": 107796,
    "Title": "Key rotation clarification",
    "ClosedAt": "2020-03-16T09:58:19Z",
    "User": "yossi-y",
    "FileName": "articles/azure-monitor/platform/customer-managed-keys.md",
    "Addition": 3,
    "Delections": 13,
    "Changes": 16,
    "Patch": "@@ -90,9 +90,7 @@ The following rules apply:\n - The AEK is used to derive DEKs, which are the keys that are used to\n     encrypt each block of data written to disk.\n \n-- When you configure your key in Key Vault and reference it in the\n-    *Cluster* resource, Azure Storage wraps the AEK with your KEK in\n-    Azure Key Vault.\n+- When you configure your key in Key Vault and reference it in the *Cluster* resource, the Azure Storage sends requests to your Azure Key Vault to wrap and unwrap the AEK to perform data encryption and decryption operations.\n \n - Your KEK never leaves your Key Vault and in the case of an HSM key,\n     it never leaves the hardware.\n@@ -101,10 +99,6 @@ The following rules apply:\n     *Cluster* resource to authenticate and access to Azure Key Vault via\n     Azure Active Directory.\n \n-- For read/write operations, Azure Storage sends requests to Azure Key\n-    Vault to wrap and unwrap the AEK to perform encryption\n-    and decryption operations.\n-\n ## CMK provisioning procedure\n \n For Application Insights CMK configuration, follow the Appendix content for steps 3 and 6.\n@@ -403,12 +397,8 @@ encryption key and once accessed, data ingestion and query resume within\n \n ## CMK (KEK) rotation\n \n-Rotation of CMK requires explicit update of the *Cluster* resource with\n-the new Azure Key Vault Key version. To update Azure Monitor with your\n-new key version, follow the instructions in \"Update *Cluster* resource\n-with *Key identifier* details\" step.\n-\n-If you update your key in Key Vault and don't update the new *Key identifier* details in the *Cluster* resource*, Azure Monitor Storage will keep using your previous key.\n+Rotation of CMK requires explicit update of the *Cluster* resource with the new key version in Azure Key Vault. To update Azure Monitor with your new key version, follow the instructions in \"Update *Cluster* resource with Key identifier details\" step. If you update your key version in Key Vault and don't update the new Key identifier details in the *Cluster* resource, Azure Monitor Storage will keep using your previous key.\n+All your data is accessible after the key rotation operation including data ingested before the rotation and after it, since all data remains encrypted by the Account Encryption Key (AEK) while it’s now being encrypted by your new Key Encryption Key (KEK) version.\n \n ## Limitations and constraints\n "
  },
  {
    "Number": 107783,
    "Title": "App Insights Access Role update to AI Component Contributor",
    "ClosedAt": "2020-03-16T04:49:47Z",
    "User": "lgayhardt",
    "FileName": "articles/azure-monitor/app/resources-roles-access-control.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -72,7 +72,7 @@ Where applicable we link to the associated official reference documentation.\n | --- | --- |\n | [Owner](https://docs.microsoft.com/azure/role-based-access-control/built-in-roles#owner) |Can change anything, including user access. |\n | [Contributor](https://docs.microsoft.com/azure/role-based-access-control/built-in-roles#contributor) |Can edit anything, including all resources. |\n-| [Application Insights Component contributor](https://docs.microsoft.com/azure/role-based-access-control/built-in-roles#application-insights-component-contributor) |Can edit Application Insights resources, web tests and alerts. |\n+| [Application Insights Component contributor](https://docs.microsoft.com/azure/role-based-access-control/built-in-roles#application-insights-component-contributor) |Can edit Application Insights resources. |\n | [Reader](https://docs.microsoft.com/azure/role-based-access-control/built-in-roles#reader) |Can view but not change anything. |\n | [Application Insights Snapshot Debugger](https://docs.microsoft.com/azure/role-based-access-control/built-in-roles#application-insights-snapshot-debugger) | Gives the user permission to use Application Insights Snapshot Debugger features. Note that this role is included in neither the Owner nor Contributor roles. |\n | Azure Service Deploy Release Management Contributor | Contributor role for services deploying through Azure Service Deploy. |\n@@ -81,7 +81,7 @@ Where applicable we link to the associated official reference documentation.\n | [Log Analytics Contributor](https://docs.microsoft.com/azure/role-based-access-control/built-in-roles#log-analytics-contributor) | Log Analytics Contributor can read all monitoring data and edit monitoring settings. Editing monitoring settings includes adding the VM extension to VMs; reading storage account keys to be able to configure collection of logs from Azure Storage; creating and configuring Automation accounts; adding solutions; and configuring Azure diagnostics on all Azure resources.  |\n | [Log Analytics Reader](https://docs.microsoft.com/azure/role-based-access-control/built-in-roles#log-analytics-reader) | Log Analytics Reader can view and search all monitoring data as well as and view monitoring settings, including viewing the configuration of Azure diagnostics on all Azure resources. |\n | masterreader | Allows a user to view everything but not make changes. |\n-| [Monitoring Contributor](https://docs.microsoft.com/azure/role-based-access-control/built-in-roles#monitoring-contributor) | Can read all monitoring data and update monitoring settings. |\n+| [Monitoring Contributor](https://docs.microsoft.com/azure/role-based-access-control/built-in-roles#monitoring-contributor) | Can read all monitoring data and update monitoring settings.|\n | [Monitoring Metrics Publisher](https://docs.microsoft.com/azure/role-based-access-control/built-in-roles#monitoring-metrics-publisher) | Enables publishing metrics against Azure resources. |\n | [Monitoring Reader](https://docs.microsoft.com/azure/role-based-access-control/built-in-roles#monitoring-reader) | Can read all monitoring data. |\n | Resource Policy Contributor (Preview) | Backfilled users from EA, with rights to create/modify resource policy, create support ticket and read resource/hierarchy.  |"
  },
  {
    "Number": 107540,
    "Title": "Small change to the parsing sample",
    "ClosedAt": "2020-03-16T02:36:14Z",
    "User": "MeirMen",
    "FileName": "articles/azure-monitor/log-query/query-optimization.md",
    "Addition": 2,
    "Delections": 0,
    "Changes": 2,
    "Patch": "@@ -79,6 +79,8 @@ SecurityEvent\n | extend FilePath = tostring(Details.UserData.RuleAndFileData.FilePath)\n | extend FileHash = tostring(Details.UserData.RuleAndFileData.FileHash)\n | summarize count() by FileHash, FilePath\n+| where FileHash != \"\" // No need to filter out %SYSTEM32 here as it was removed before\n+\n ```\n \n Queries that contain [where](/azure/kusto/query/whereoperator) clauses on an evaluated column rather than on columns that are physically present in the dataset lose efficiency. Filtering on evaluated columns prevents some system optimizations when large sets of data are handled."
  },
  {
    "Number": 107746,
    "Title": "(AzureCXP) Microsoftdocs/azure-docs#49954",
    "ClosedAt": "2020-03-14T19:40:17Z",
    "User": "KrishnaG-MSFT",
    "FileName": "articles/azure-monitor/platform/powershell-workspace-configuration.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -154,7 +154,7 @@ New-AzOperationalInsightsComputerGroup -ResourceGroupName $ResourceGroup -Worksp\n Enable-AzOperationalInsightsIISLogCollection -ResourceGroupName $ResourceGroup -WorkspaceName $WorkspaceName\n \n # Linux Perf\n-New-AzOperationalInsightsLinuxPerformanceObjectDataSource -ResourceGroupName $ResourceGroup -WorkspaceName $WorkspaceName -ObjectName \"Logical Disk\" -InstanceName \"*\"  -CounterNames @(\"% Used Inodes\", \"Free Megabytes\", \"% Used Space\", \"Disk Transfers/sec\", \"Disk Reads/sec\", \"Disk Reads/sec\", \"Disk Writes/sec\") -IntervalSeconds 20  -Name \"Example Linux Disk Performance Counters\"\n+New-AzOperationalInsightsLinuxPerformanceObjectDataSource -ResourceGroupName $ResourceGroup -WorkspaceName $WorkspaceName -ObjectName \"Logical Disk\" -InstanceName \"*\"  -CounterNames @(\"% Used Inodes\", \"Free Megabytes\", \"% Used Space\", \"Disk Transfers/sec\", \"Disk Reads/sec\", \"Disk Writes/sec\") -IntervalSeconds 20  -Name \"Example Linux Disk Performance Counters\"\n Enable-AzOperationalInsightsLinuxPerformanceCollection -ResourceGroupName $ResourceGroup -WorkspaceName $WorkspaceName\n \n # Linux Syslog"
  },
  {
    "Number": 107744,
    "Title": "Azure Monitor what's new fix",
    "ClosedAt": "2020-03-14T18:15:48Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/whats-new.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -12,7 +12,7 @@ ms.date: 03/05/2020\n # What's new in Azure Monitor documentation?\n This article provides lists Azure Monitor articles that are either new or have been significantly updated. It will be refreshed the first week of each month to include article updates from the previous month.\n \n-## March 2020\n+## February 2020\n \n ### Agents\n Multiple updates as part of rewrite of diagnostics extension content."
  },
  {
    "Number": 107650,
    "Title": "Small change. ",
    "ClosedAt": "2020-03-13T18:50:33Z",
    "User": "rboucher",
    "FileName": "articles/azure-monitor/log-query/query-optimization.md",
    "Addition": 1,
    "Delections": 0,
    "Changes": 1,
    "Patch": "@@ -79,6 +79,7 @@ SecurityEvent\n | extend FilePath = tostring(Details.UserData.RuleAndFileData.FilePath)\n | extend FileHash = tostring(Details.UserData.RuleAndFileData.FileHash)\n | summarize count() by FileHash, FilePath\n+| where FileHash != \"\" // No need to filter out %SYSTEM32 here as it was removed before\n ```\n \n Queries that contain [where](/azure/kusto/query/whereoperator) clauses on an evaluated column rather than on columns that are physically present in the dataset lose efficiency. Filtering on evaluated columns prevents some system optimizations when large sets of data are handled."
  },
  {
    "Number": 107599,
    "Title": "Link change",
    "ClosedAt": "2020-03-13T13:08:25Z",
    "User": "harelbr",
    "FileName": "articles/azure-monitor/platform/alerts-metric.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -4,7 +4,7 @@ description: Learn how to use Azure portal or CLI to create, view, and manage me\n author: harelbr\n ms.author: harelbr\n ms.topic: conceptual\n-ms.date: 02/16/2020\n+ms.date: 03/13/2020\n ms.subservice: alerts\n ---\n # Create, view, and manage metric alerts using Azure Monitor\n@@ -130,7 +130,7 @@ The previous sections described how to create, view, and manage metric alert rul\n \n ## Next steps\n \n-- [Create metric alerts using Azure Resource Manager Templates](../../azure-monitor/platform/alerts-enable-template.md).\n+- [Create metric alerts using Azure Resource Manager Templates](../../azure-monitor/platform/alerts-metric-create-templates.md).\n - [Understand how metric alerts work](alerts-metric-overview.md).\n - [Understand how metric alerts with Dynamic Thresholds condition work](alerts-dynamic-thresholds.md).\n - [Understand the web hook schema for metric alerts](../../azure-monitor/platform/alerts-metric-near-real-time.md#payload-schema)"
  },
  {
    "Number": 107598,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-13T11:01:58Z",
    "User": "PMEds28",
    "FileName": "articles/active-directory-domain-services/tutorial-configure-ldaps.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -66,7 +66,7 @@ The certificate you request or create must meet the following requirements. Your\n * **Key usage** - The certificate must be configured for *digital signatures* and *key encipherment*.\n * **Certificate purpose** - The certificate must be valid for SSL server authentication.\n \n-In this tutorial, let's create a self-signed certificate for secure LDAP using the [New-SelfSignedCertificate][New-SelfSignedCertificate] cmdlet. Open a PowerShell window as **Administrator** and run the following commands. Replace the *$dnsName* variable with the DNS name used by your own managed domain, such as *aaddscontoso.com*:\n+There are several tools available to create self-signed certificate such as OpenSSL, Keytool, MakeCert, [New-SelfSignedCertificate][New-SelfSignedCertificate] cmdlet etc. In this tutorial, let's create a self-signed certificate for secure LDAP using the [New-SelfSignedCertificate][New-SelfSignedCertificate] cmdlet. Open a PowerShell window as **Administrator** and run the following commands. Replace the *$dnsName* variable with the DNS name used by your own managed domain, such as *aaddscontoso.com*:\n \n ```powershell\n # Define your own DNS name used by your Azure AD DS managed domain\n@@ -140,7 +140,7 @@ Before you can use the digital certificate created in the previous step with you\n 1. As this certificate is used to decrypt data, you should carefully control access. A password can be used to protect the use of the certificate. Without the correct password, the certificate can't be applied to a service.\n \n     On the **Security** page, choose the option for **Password** to protect the *.PFX* certificate file. Enter and confirm a password, then select **Next**. This password is used in the next section to enable secure LDAP for your Azure AD DS managed domain.\n-1. On the **File to Export** page, specify the file name and location where you'd like to export the certificate, such as *C:\\Users\\accountname\\azure-ad-ds.pfx*.\n+1. On the **File to Export** page, specify the file name and location where you'd like to export the certificate, such as *C:\\Users\\accountname\\azure-ad-ds.pfx*. Keep a note of the password and location of the *.PFX* file as this information would be required in next steps.\n 1. On the review page, select **Finish** to export the certificate to a *.PFX* certificate file. A confirmation dialog is displayed when the certificate has been successfully exported.\n 1. Leave the MMC open for use in the following section.\n "
  },
  {
    "Number": 107598,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-13T11:01:58Z",
    "User": "PMEds28",
    "FileName": "articles/active-directory/fundamentals/active-directory-ops-guide-iam.md",
    "Addition": 15,
    "Delections": 15,
    "Changes": 30,
    "Patch": "@@ -36,7 +36,7 @@ Managing Azure Active Directory requires the continuous execution of key operati\n | Troubleshoot and remediate license assignment errors | IAM Operations Team |\r\n | Provision identities to applications in Azure AD | IAM Operations Team |\r\n \r\n-As you review your list, you may find you need to either assign an owner for tasks that are missing an owner or adjust ownership for tasks with owners that aren’t aligned with the recommendations above.\r\n+As you review your list, you may find you need to either assign an owner for tasks that are missing an owner or adjust ownership for tasks with owners that aren't aligned with the recommendations above.\r\n \r\n #### Assigning owners recommended reading\r\n \r\n@@ -53,25 +53,25 @@ Microsoft recommends you have a good baseline and understanding of the issues in\n \r\n To enable all hybrid experiences, device-based security posture, and integration with Azure AD, it is required that you synchronize user accounts that your employees use to login to their desktops.\r\n \r\n-If you don’t synchronize the forest users log into, then you should change the synchronization to come from the proper forest.\r\n+If you don't synchronize the forest users log into, then you should change the synchronization to come from the proper forest.\r\n \r\n #### Synchronization scope and object filtering\r\n \r\n Removing known buckets of objects that aren't required to be synchronized has the following operational benefits:\r\n \r\n - Fewer sources of sync errors\r\n - Faster sync cycles\r\n-- Less \"garbage\" to carry forward from on-premises, for example, pollution of the global address list for on-premises service accounts that aren’t relevant in the cloud\r\n+- Less \"garbage\" to carry forward from on-premises, for example, pollution of the global address list for on-premises service accounts that aren't relevant in the cloud\r\n \r\n > [!NOTE]\r\n-> If you find you are importing many objects that aren’t being exported to the cloud, you should filter by OU or specific attributes.\r\n+> If you find you are importing many objects that aren't being exported to the cloud, you should filter by OU or specific attributes.\r\n \r\n Examples of objects to exclude are:\r\n \r\n-- Service Accounts that aren’t used for cloud applications\r\n-- Groups that aren’t meant to be used in cloud scenarios such as those used to grant access to resources\r\n+- Service Accounts that aren't used for cloud applications\r\n+- Groups that aren't meant to be used in cloud scenarios such as those used to grant access to resources\r\n - Users or contacts that are external identities that are meant to be represented with Azure AD B2B Collaboration\r\n-- Computer Accounts where employees aren’t meant to access cloud applications from, for example, servers\r\n+- Computer Accounts where employees aren't meant to access cloud applications from, for example, servers\r\n \r\n > [!NOTE]\r\n > If a single human identity has multiple accounts provisioned from something such as a legacy domain migration, merger, or acquisition, you should only synchronize the account used by the user on a day-to-day basis, for example, what they use to log in to their computer.\r\n@@ -88,7 +88,7 @@ Azure AD Connect plays a key role in the provisioning process. If the Sync Serve\n - **Deploy Azure AD Connect Server(s) in Staging Mode** - allows an administrator to \"promote\" the staging server to production by a simple configuration switch.\r\n - **Use Virtualization** - If the Azure AD connect is deployed in a virtual machine (VM), admins can leverage their virtualization stack to live migrate or quickly redeploy the VM and therefore resume synchronization.\r\n \r\n-If your organization is lacking a disaster recovery and failover strategy for Sync, you shouldn’t hesitate to deploy Azure AD Connect in Staging Mode. Likewise, if there is a mismatch between your production and staging configuration, you should re-baseline Azure AD Connect staging mode to match the production configuration, including software versions and configurations.\r\n+If your organization is lacking a disaster recovery and failover strategy for Sync, you shouldn't hesitate to deploy Azure AD Connect in Staging Mode. Likewise, if there is a mismatch between your production and staging configuration, you should re-baseline Azure AD Connect staging mode to match the production configuration, including software versions and configurations.\r\n \r\n ![A screenshot of Azure AD Connect staging mode configuration](./media/active-directory-ops-guide/active-directory-ops-img1.png)\r\n \r\n@@ -102,7 +102,7 @@ If your Azure AD Connect version is more than six months behind, you should upgr\n \r\n Using **ms-DS-consistencyguid** as the [source anchor](https://docs.microsoft.com/azure/active-directory/hybrid/plan-connect-design-concepts) allows an easier migration of objects across forests and domains, which is common in AD Domain consolidation/cleanup, mergers, acquisitions, and divestitures.\r\n \r\n-If you’re currently using **ObjectGuid** as the source anchor, we recommend you switch to using **ms-DS-ConsistencyGuid**.\r\n+If you're currently using **ObjectGuid** as the source anchor, we recommend you switch to using **ms-DS-ConsistencyGuid**.\r\n \r\n #### Custom rules\r\n \r\n@@ -113,7 +113,7 @@ Azure AD Connect custom rules provide the ability to control the flow of attribu\n - Higher probability of divergence of configuration between the production server and staging server\r\n - Additional overhead when upgrading Azure AD Connect if custom rules are created within the precedence greater than 100 (used by built-in rules)\r\n \r\n-If you are using overly complex rules, you should investigate the reasons for the complexity and find opportunities for simplification. Likewise, if you have created custom rules with precedence value over 100, you should fix the rules so they aren’t at risk or conflict with the default set.\r\n+If you are using overly complex rules, you should investigate the reasons for the complexity and find opportunities for simplification. Likewise, if you have created custom rules with precedence value over 100, you should fix the rules so they aren't at risk or conflict with the default set.\r\n \r\n Examples of misusing custom rules include:\r\n \r\n@@ -143,7 +143,7 @@ Azure Active Directory streamlines the management of licenses through [group-bas\n \r\n If you are currently using a manual process to assign licenses and components to users, we recommend you implement group-based licensing. If your current process does not monitor licensing errors or what is Assigned versus Available, you should define improvements to the process to address licensing errors and monitor licensing assignment.\r\n \r\n-Another aspect of license management is the definition of service plans (components of the license) that should be enabled based on job functions in the organization. Granting access to service plans that aren’t necessary, can result in users seeing tools in the Office portal that they have not been trained for or should not be using. It can drive additional help desk volume, unnecessary provisioning, and put your compliance and governance at risk, for example, when provisioning OneDrive for Business to individuals that might not be allowed to share content.\r\n+Another aspect of license management is the definition of service plans (components of the license) that should be enabled based on job functions in the organization. Granting access to service plans that aren't necessary, can result in users seeing tools in the Office portal that they have not been trained for or should not be using. It can drive additional help desk volume, unnecessary provisioning, and put your compliance and governance at risk, for example, when provisioning OneDrive for Business to individuals that might not be allowed to share content.\r\n \r\n Use the following guidelines to define service plans to users:\r\n \r\n@@ -158,26 +158,26 @@ Use the following guidelines to define service plans to users:\n \r\n #### Lifecycle management\r\n \r\n-If you are currently using a tool, such as [Microsoft Identity Manager](https://docs.microsoft.com/microsoft-identity-manager/) or third-party system, that relies on an on-premises infrastructure, we recommend you offload assignment from the existing tool, implement group-based licensing and define a group lifecycle management based on [groups](https://docs.microsoft.com/azure/active-directory/users-groups-roles/licensing-group-advanced#use-group-based-licensing-with-dynamic-groups). Likewise, if your existing process doesn’t account for new employees or employees that leave the organization, you should deploy group-based licensing based on dynamic groups and define a group membership lifecycle. Finally, if group-based licensing is deployed against on-premises groups that lack lifecycle management, consider using cloud groups to enable capabilities such as delegated ownership or attribute-based dynamic membership.\r\n+If you are currently using a tool, such as [Microsoft Identity Manager](https://docs.microsoft.com/microsoft-identity-manager/) or third-party system, that relies on an on-premises infrastructure, we recommend you offload assignment from the existing tool, implement group-based licensing and define a group lifecycle management based on [groups](https://docs.microsoft.com/azure/active-directory/users-groups-roles/licensing-group-advanced#use-group-based-licensing-with-dynamic-groups). Likewise, if your existing process doesn't account for new employees or employees that leave the organization, you should deploy group-based licensing based on dynamic groups and define a group membership lifecycle. Finally, if group-based licensing is deployed against on-premises groups that lack lifecycle management, consider using cloud groups to enable capabilities such as delegated ownership or attribute-based dynamic membership.\r\n \r\n ### Assignment of apps with \"All users\" group\r\n \r\n Resource owners may believe that the **All users** group contains only **Enterprise Employees** when they may actually contain both **Enterprise Employees** and **Guests**. As a result, you should take special care when using the **All users** group for application assignment and granting access to resources such as SharePoint content or applications.\r\n \r\n > [!IMPORTANT]\r\n-> If the **All users** group is enabled and used for conditional access policies, app or resource assignment, make sure to [secure the group](https://docs.microsoft.com/azure/active-directory/b2b/use-dynamic-groups#hardening-the-all-users-dynamic-group) if you don't want it to include guest users. Furthermore, you should fix your licensing assignments by creating and assigning to groups that contain **Enterprise Employees** only. On the other hand, if you find that the **All users** group is enabled but not being used to grant access to resources, make sure your organization’s operational guidance is to intentionally use that group (which includes both **Enterprise Employees** and **Guests**).\r\n+> If the **All users** group is enabled and used for conditional access policies, app or resource assignment, make sure to [secure the group](https://docs.microsoft.com/azure/active-directory/b2b/use-dynamic-groups) if you don't want it to include guest users. Furthermore, you should fix your licensing assignments by creating and assigning to groups that contain **Enterprise Employees** only. On the other hand, if you find that the **All users** group is enabled but not being used to grant access to resources, make sure your organization's operational guidance is to intentionally use that group (which includes both **Enterprise Employees** and **Guests**).\r\n \r\n ### Automated user provisioning to apps\r\n \r\n [Automated user provisioning](https://docs.microsoft.com/azure/active-directory/manage-apps/user-provisioning) to applications is the best way to create a consistent provisioning, deprovisioning, and lifecycle of identities across multiple systems.\r\n \r\n-If you are currently provisioning apps in an ad-hoc manner or using things like CSV files, JIT, or an on-premises solution that does not address lifecycle management, we recommend you [implement application provisioning](https://docs.microsoft.com/azure/active-directory/manage-apps/user-provisioning#how-do-i-set-up-automatic-provisioning-to-an-application) with Azure AD for supported applications and define a consistent pattern for applications that aren’t yet supported by Azure AD.\r\n+If you are currently provisioning apps in an ad-hoc manner or using things like CSV files, JIT, or an on-premises solution that does not address lifecycle management, we recommend you [implement application provisioning](https://docs.microsoft.com/azure/active-directory/manage-apps/user-provisioning#how-do-i-set-up-automatic-provisioning-to-an-application) with Azure AD for supported applications and define a consistent pattern for applications that aren't yet supported by Azure AD.\r\n \r\n ![Azure AD provisioning service](./media/active-directory-ops-guide/active-directory-ops-img3.png)\r\n \r\n ### Azure AD Connect delta sync cycle baseline\r\n \r\n-It is important to understand the volume of changes in your organization and make sure that it isn’t taking too long to have a predictable synchronization time.\r\n+It is important to understand the volume of changes in your organization and make sure that it isn't taking too long to have a predictable synchronization time.\r\n \r\n The [default delta sync](https://docs.microsoft.com/azure/active-directory/hybrid/how-to-connect-sync-feature-scheduler) frequency is 30 minutes. If the delta sync is taking longer than 30 minutes consistently, or there are significant discrepancies between the delta sync performance of staging and production, you should investigate and review the [factors influencing the performance of Azure AD Connect](https://docs.microsoft.com/azure/active-directory/hybrid/plan-connect-performance-factors).\r\n \r"
  },
  {
    "Number": 107598,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-13T11:01:58Z",
    "User": "PMEds28",
    "FileName": "articles/active-directory/managed-identities-azure-resources/how-to-view-managed-identity-service-principal-cli.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -28,7 +28,7 @@ In this article, you learn how to view the service principal of a managed identi\n \n - If you're unfamiliar with managed identities for Azure resources, check out the [overview section](overview.md).\n - If you don't already have an Azure account, [sign up for a free account](https://azure.microsoft.com/free/).\n-- Enable [system assigned identity on a virtual machine](/azure/active-directory/managed-identities-azure-resources/qs-configure-portal-windows-vm#system-assigned-managed-identity) or [application](/azure/app-service/overview-managed-identity#adding-a-system-assigned-identity).\n+- Enable [system assigned identity on a virtual machine](/azure/active-directory/managed-identities-azure-resources/qs-configure-portal-windows-vm#system-assigned-managed-identity) or [application](/azure/app-service/overview-managed-identity#add-a-system-assigned-identity).\n - To run the CLI script examples, you have three options:\n     - Use [Azure Cloud Shell](../../cloud-shell/overview.md) from the Azure portal (see next section).\n     - Use the embedded Azure Cloud Shell via the \"Try It\" button, located in the top right corner of each code block."
  },
  {
    "Number": 107598,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-13T11:01:58Z",
    "User": "PMEds28",
    "FileName": "articles/active-directory/managed-identities-azure-resources/how-to-view-managed-identity-service-principal-portal.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -31,7 +31,7 @@ In this article, you learn how to view the service principal of a managed identi\n \n - If you're unfamiliar with managed identities for Azure resources, check out the [overview section](overview.md).\n - If you don't already have an Azure account, [sign up for a free account](https://azure.microsoft.com/free/).\n-- Enable [system assigned identity on a virtual machine](/azure/active-directory/managed-identities-azure-resources/qs-configure-portal-windows-vm#system-assigned-managed-identity) or [application](/azure/app-service/overview-managed-identity#adding-a-system-assigned-identity).\n+- Enable [system assigned identity on a virtual machine](/azure/active-directory/managed-identities-azure-resources/qs-configure-portal-windows-vm#system-assigned-managed-identity) or [application](/azure/app-service/overview-managed-identity#add-a-system-assigned-identity).\n \n ## View the service principal\n "
  },
  {
    "Number": 107598,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-13T11:01:58Z",
    "User": "PMEds28",
    "FileName": "articles/active-directory/managed-identities-azure-resources/how-to-view-managed-identity-service-principal-powershell.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -30,7 +30,7 @@ In this article, you learn how to view the service principal of a managed identi\n \n - If you're unfamiliar with managed identities for Azure resources, check out the [overview section](overview.md).\n - If you don't already have an Azure account, [sign up for a free account](https://azure.microsoft.com/free/).\n-- Enable [system assigned identity on a virtual machine](/azure/active-directory/managed-identities-azure-resources/qs-configure-portal-windows-vm#system-assigned-managed-identity) or [application](/azure/app-service/overview-managed-identity#adding-a-system-assigned-identity).\n+- Enable [system assigned identity on a virtual machine](/azure/active-directory/managed-identities-azure-resources/qs-configure-portal-windows-vm#system-assigned-managed-identity) or [application](/azure/app-service/overview-managed-identity#add-a-system-assigned-identity).\n - Install the latest version of [Azure PowerShell](/powershell/azure/install-az-ps)\n \n ## View the service principal"
  },
  {
    "Number": 107598,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-13T11:01:58Z",
    "User": "PMEds28",
    "FileName": "articles/active-directory/saas-apps/invision-tutorial.md",
    "Addition": 206,
    "Delections": 0,
    "Changes": 206,
    "Patch": "@@ -0,0 +1,206 @@\n+---\n+title: 'Tutorial: Azure Active Directory single sign-on (SSO) integration with InVision | Microsoft Docs'\n+description: Learn how to configure single sign-on between Azure Active Directory and InVision.\n+services: active-directory\n+documentationCenter: na\n+author: jeevansd\n+manager: mtillman\n+ms.reviewer: barbkess\n+\n+ms.assetid: 02487206-30b0-4b1d-ae99-573c3d2ef9b0\n+ms.service: active-directory\n+ms.subservice: saas-app-tutorial\n+ms.workload: identity\n+ms.tgt_pltfrm: na\n+ms.topic: tutorial\n+ms.date: 03/09/2020\n+ms.author: jeedes\n+\n+ms.collection: M365-identity-device-management\n+---\n+\n+# Tutorial: Azure Active Directory single sign-on (SSO) integration with InVision\n+\n+In this tutorial, you'll learn how to integrate InVision with Azure Active Directory (Azure AD). When you integrate InVision with Azure AD, you can:\n+\n+* Control in Azure AD who has access to InVision.\n+* Enable your users to be automatically signed-in to InVision with their Azure AD accounts.\n+* Manage your accounts in one central location - the Azure portal.\n+\n+To learn more about SaaS app integration with Azure AD, see [What is application access and single sign-on with Azure Active Directory](https://docs.microsoft.com/azure/active-directory/manage-apps/what-is-single-sign-on).\n+\n+## Prerequisites\n+\n+To get started, you need the following items:\n+\n+* An Azure AD subscription. If you don't have a subscription, you can get a [free account](https://azure.microsoft.com/free/).\n+* InVision single sign-on (SSO) enabled subscription.\n+\n+## Scenario description\n+\n+In this tutorial, you configure and test Azure AD SSO in a test environment.\n+\n+* InVision supports **SP and IDP** initiated SSO\n+* Once you configure InVision you can enforce session control, which protect exfiltration and infiltration of your organization’s sensitive data in real-time. Session control extend from Conditional Access. [Learn how to enforce session control with Microsoft Cloud App Security](https://docs.microsoft.com/cloud-app-security/proxy-deployment-any-app).\n+\n+## Adding InVision from the gallery\n+\n+To configure the integration of InVision into Azure AD, you need to add InVision from the gallery to your list of managed SaaS apps.\n+\n+1. Sign in to the [Azure portal](https://portal.azure.com) using either a work or school account, or a personal Microsoft account.\n+1. On the left navigation pane, select the **Azure Active Directory** service.\n+1. Navigate to **Enterprise Applications** and then select **All Applications**.\n+1. To add new application, select **New application**.\n+1. In the **Add from the gallery** section, type **InVision** in the search box.\n+1. Select **InVision** from results panel and then add the app. Wait a few seconds while the app is added to your tenant.\n+\n+## Configure and test Azure AD single sign-on for InVision\n+\n+Configure and test Azure AD SSO with InVision using a test user called **B.Simon**. For SSO to work, you need to establish a link relationship between an Azure AD user and the related user in InVision.\n+\n+To configure and test Azure AD SSO with InVision, complete the following building blocks:\n+\n+1. **[Configure Azure AD SSO](#configure-azure-ad-sso)** - to enable your users to use this feature.\n+    * **[Create an Azure AD test user](#create-an-azure-ad-test-user)** - to test Azure AD single sign-on with B.Simon.\n+    * **[Assign the Azure AD test user](#assign-the-azure-ad-test-user)** - to enable B.Simon to use Azure AD single sign-on.\n+1. **[Configure InVision SSO](#configure-invision-sso)** - to configure the single sign-on settings on application side.\n+    * **[Create InVision test user](#create-invision-test-user)** - to have a counterpart of B.Simon in InVision that is linked to the Azure AD representation of user.\n+1. **[Test SSO](#test-sso)** - to verify whether the configuration works.\n+\n+## Configure Azure AD SSO\n+\n+Follow these steps to enable Azure AD SSO in the Azure portal.\n+\n+1. In the [Azure portal](https://portal.azure.com/), on the **InVision** application integration page, find the **Manage** section and select **single sign-on**.\n+1. On the **Select a single sign-on method** page, select **SAML**.\n+1. On the **Set up single sign-on with SAML** page, click the edit/pen icon for **Basic SAML Configuration** to edit the settings.\n+\n+   ![Edit Basic SAML Configuration](common/edit-urls.png)\n+\n+1. On the **Basic SAML Configuration** section, if you wish to configure the application in **IDP** initiated mode, enter the values for the following fields:\n+\n+    a. In the **Identifier** text box, type a URL using the following pattern:\n+    `https://<SUBDOMAIN>.invisionapp.com`\n+\n+    b. In the **Reply URL** text box, type a URL using the following pattern:\n+    `https://<SUBDOMAIN>.invisionapp.com//sso/auth`\n+\n+1. Click **Set additional URLs** and perform the following step if you wish to configure the application in **SP** initiated mode:\n+\n+    In the **Sign-on URL** text box, type a URL using the following pattern:\n+    `https://<SUBDOMAIN>.invisionapp.com`\n+\n+\t> [!NOTE]\n+\t> These values are not real. Update these values with the actual Identifier, Reply URL and Sign-on URL. Contact [InVision Client support team](mailto:support@invisionapp.com) to get these values. You can also refer to the patterns shown in the **Basic SAML Configuration** section in the Azure portal.\n+\n+1. On the **Set up single sign-on with SAML** page, in the **SAML Signing Certificate** section,  find **Certificate (Base64)** and select **Download** to download the certificate and save it on your computer.\n+\n+\t![The Certificate download link](common/certificatebase64.png)\n+\n+1. On the **Set up InVision** section, copy the appropriate URL(s) based on your requirement.\n+\n+\t![Copy configuration URLs](common/copy-configuration-urls.png)\n+\n+### Create an Azure AD test user\n+\n+In this section, you'll create a test user in the Azure portal called B.Simon.\n+\n+1. From the left pane in the Azure portal, select **Azure Active Directory**, select **Users**, and then select **All users**.\n+1. Select **New user** at the top of the screen.\n+1. In the **User** properties, follow these steps:\n+   1. In the **Name** field, enter `B.Simon`.  \n+   1. In the **User name** field, enter the username@companydomain.extension. For example, `B.Simon@contoso.com`.\n+   1. Select the **Show password** check box, and then write down the value that's displayed in the **Password** box.\n+   1. Click **Create**.\n+\n+### Assign the Azure AD test user\n+\n+In this section, you'll enable B.Simon to use Azure single sign-on by granting access to InVision.\n+\n+1. In the Azure portal, select **Enterprise Applications**, and then select **All applications**.\n+1. In the applications list, select **InVision**.\n+1. In the app's overview page, find the **Manage** section and select **Users and groups**.\n+\n+   ![The \"Users and groups\" link](common/users-groups-blade.png)\n+\n+1. Select **Add user**, then select **Users and groups** in the **Add Assignment** dialog.\n+\n+\t![The Add User link](common/add-assign-user.png)\n+\n+1. In the **Users and groups** dialog, select **B.Simon** from the Users list, then click the **Select** button at the bottom of the screen.\n+1. If you're expecting any role value in the SAML assertion, in the **Select Role** dialog, select the appropriate role for the user from the list and then click the **Select** button at the bottom of the screen.\n+1. In the **Add Assignment** dialog, click the **Assign** button.\n+\n+## Configure InVision SSO\n+\n+1. In a different web browser window, sign into InVision site as an administrator.\n+\n+1. Click on **Team** and select **Settings**.\n+\n+    ![InVision Configuration](./media/invision-tutorial/config1.png)\n+\n+1. Scroll down to **Single sign-on** and then click **Change**.\n+\n+    ![InVision Configuration](./media/invision-tutorial/config3.png)\n+\n+1. On the **Single sign-on** page, perform the following steps:\n+\n+    ![InVision Configuration](./media/invision-tutorial/config4.png)\n+\n+    a. Change **Require SSO for every member of < account name >** to **On**.\n+\n+    b. In the **name** textbox, enter the name for example like `azureadsso`.\n+\n+    c. Enter the Sign-on URL value in the **Sign-in URL** textbox.\n+\n+    d. In the **Sign-out URL** textbox, paste the **Log out** URL value, which you have copied from the Azure portal.\n+\n+    e. In the **SAML Certificate** textbox, open the downloaded **Certificate (Base64)** into Notepad, copy the content and paste it into SAML Certificate textbox.\n+\n+    f. Select **SHA-256** from the dropdown for the **HASH Algorithm**.\n+\n+    g. Enter appropriate name for the **SSO Button Label**.\n+\n+    h. Make **Allow Just-in-Time provisioning** On.\n+\n+    i. Click **Update**.\n+\n+### Create InVision test user\n+\n+1. In a different web browser window, sign into InVision site as an administrator.\n+\n+1. Click on **Team** and select **People**.\n+\n+    ![InVision Configuration](./media/invision-tutorial/config2.png)\n+\n+1. Click on the **+ ICON** to add new user.\n+\n+    ![InVision Configuration](./media/invision-tutorial/user1.png)\n+\n+1. Enter the email address of the user and click **Next**.\n+\n+    ![InVision Configuration](./media/invision-tutorial/user2.png)\n+\n+1. Once verify the email address and then click **Invite**.\n+\n+    ![InVision Configuration](./media/invision-tutorial/user3.png)\n+\n+## Test SSO\n+\n+In this section, you test your Azure AD single sign-on configuration using the Access Panel.\n+\n+When you click the InVision tile in the Access Panel, you should be automatically signed in to the InVision for which you set up SSO. For more information about the Access Panel, see [Introduction to the Access Panel](https://docs.microsoft.com/azure/active-directory/active-directory-saas-access-panel-introduction).\n+\n+## Additional resources\n+\n+- [ List of Tutorials on How to Integrate SaaS Apps with Azure Active Directory ](https://docs.microsoft.com/azure/active-directory/active-directory-saas-tutorial-list)\n+\n+- [What is application access and single sign-on with Azure Active Directory? ](https://docs.microsoft.com/azure/active-directory/manage-apps/what-is-single-sign-on)\n+\n+- [What is conditional access in Azure Active Directory?](https://docs.microsoft.com/azure/active-directory/conditional-access/overview)\n+\n+- [Try InVision with Azure AD](https://aad.portal.azure.com/)\n+\n+- [What is session control in Microsoft Cloud App Security?](https://docs.microsoft.com/cloud-app-security/proxy-intro-aad)\n+\n+- [How to protect InVision with advanced visibility and controls](https://docs.microsoft.com/cloud-app-security/proxy-intro-aad)\n\\ No newline at end of file"
  },
  {
    "Number": 107598,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-13T11:01:58Z",
    "User": "PMEds28",
    "FileName": "articles/active-directory/users-groups-roles/groups-self-service-management.md",
    "Addition": 12,
    "Delections": 12,
    "Changes": 24,
    "Patch": "@@ -11,7 +11,7 @@ ms.service: active-directory\n ms.workload: identity\n ms.subservice: users-groups-roles\n ms.topic: conceptual\n-ms.date: 03/18/2019\n+ms.date: 03/10/2020\n ms.author: curtand\n \n ms.reviewer: krbain\n@@ -25,7 +25,7 @@ You can enable users to create and manage their own security groups or Office 36\n \n ## Self-service group membership defaults\n \n-When security groups are created in the Azure portal or using Azure AD PowerShell, only the group's owners can update membership. Security groups created in the [Access panel](https://account.activedirectory.windowsazure.com/r#/joinGroups) and all Office 365 groups are available to join for all users, whether owner-approved or auto-approved. In the Access panel, you can change membership options when you create the group.\n+When security groups are created in the Azure portal or using Azure AD PowerShell, only the group's owners can update membership. Security groups created by self-service in the [Access panel](https://account.activedirectory.windowsazure.com/r#/joinGroups) and all Office 365 groups are available to join for all users, whether owner-approved or auto-approved. In the Access panel, you can change membership options when you create the group.\n \n Groups created in | Security group default behavior | Office 365 group default behavior\n ------------------ | ------------------------------- | ---------------------------------\n@@ -36,24 +36,24 @@ Groups created in | Security group default behavior | Office 365 group default b\n ## Self-service group management scenarios\n \n * **Delegated group management**\n-    An example is an administrator who is managing access to a SaaS application that the company is using. Managing these access rights is becoming cumbersome, so this administrator asks the business owner to create a new group. The administrator assigns access for the application to the new group, and adds to the group all people already accessing the application. The business owner then can add more users, and those users are automatically provisioned to the application. The business owner doesn't need to wait for the administrator to manage access for users. If the administrator grants the same permission to a manager in a different business group, then that person can also manage access for their own group members. Neither the business owner nor the manager can view or manage each other’s group memberships. The administrator can still see all users who have access to the application and block access rights if needed.\n+    An example is an administrator who is managing access to a SaaS application that the company is using. Managing these access rights is becoming cumbersome, so this administrator asks the business owner to create a new group. The administrator assigns access for the application to the new group, and adds to the group all people already accessing the application. The business owner then can add more users, and those users are automatically provisioned to the application. The business owner doesn't need to wait for the administrator to manage access for users. If the administrator grants the same permission to a manager in a different business group, then that person can also manage access for their own group members. Neither the business owner nor the manager can view or manage each other's group memberships. The administrator can still see all users who have access to the application and block access rights if needed.\n * **Self-service group management**\n-    An example of this scenario is two users who both have SharePoint Online sites that they set up independently. They want to give each other’s teams access to their sites. To accomplish this, they can create one group in Azure AD, and in SharePoint Online each of them selects that group to provide access to their sites. When someone wants access, they request it from the Access Panel, and after approval they get access to both SharePoint Online sites automatically. Later, one of them decides that all people accessing the site should also get access to a particular SaaS application. The administrator of the SaaS application can add access rights for the  application to the SharePoint Online site. From then on, any requests that get approved gives access to the two SharePoint Online sites and also to this SaaS application.\n+    An example of this scenario is two users who both have SharePoint Online sites that they set up independently. They want to give each other's teams access to their sites. To accomplish this, they can create one group in Azure AD, and in SharePoint Online each of them selects that group to provide access to their sites. When someone wants access, they request it from the Access Panel, and after approval they get access to both SharePoint Online sites automatically. Later, one of them decides that all people accessing the site should also get access to a particular SaaS application. The administrator of the SaaS application can add access rights for the  application to the SharePoint Online site. From then on, any requests that get approved gives access to the two SharePoint Online sites and also to this SaaS application.\n \n ## Make a group available for user self-service\n \n 1. Sign in to the [Azure AD admin center](https://aad.portal.azure.com) with an account that's a global admin for the directory.\n-2. Select **Users and groups**, and then select **Group settings**.\n-3. Set **Self-service group management enabled** to **Yes**.\n-4. Set **Users can create security groups** or **Users can create Office 365 groups** to **Yes**.\n-   * When these settings are enabled, all users in your directory are allowed to create new security groups and add members to these groups. These new groups would also show up in the Access Panel for all other users. If the policy setting on the group allows it, other users can create requests to join these groups. \n-   * When these settings are disabled, users can't create groups and can't change existing groups for which they are an owner. However, they can still manage the memberships of those groups and approve requests from other users to join their groups.\n+1. Select **Groups**, and then select **General** settings.\n+1. Set **Owners can manage group membership requests in the Access Panel** to **Yes**.\n+1. Set **Restrict access to Groups in the Access Panel** to **No**.\n+1. If you set **Users can create security groups in Azure portals** or **Users can create Office 365 groups in Azure portals** to\n \n-You can also use **Users who can manage security groups** and **Users who can manage Office 365 groups** to achieve more granular access control over self-service group management for your users. When **Users can create groups** is enabled, all users in your tenant are allowed to create new groups and add members to these groups. You can't specify individuals who can create their own groups. You can specify individuals only for making another group member a group owner.\n+    - **Yes**: All users in your Azure AD organization are allowed to create new security groups and add members to these groups. These new groups would also show up in the Access Panel for all other users. If the policy setting on the group allows it, other users can create requests to join these groups\n+    - **No**: Users can't create groups and can't change existing groups for which they are an owner. However, they can still manage the memberships of those groups and approve requests from other users to join their groups.\n \n-By setting **Users who can use self-service for security groups** and **Users who can manage Office 365 groups** to **Yes**, you enable all users in your tenant to create new groups.\n+You can also use **Owners who can assign members as group owners in Azure portals** and **Owners who can assign members as group owners in Azure portals** to achieve more granular access control over self-service group management for your users.\n \n-You can also use **Group that can manage security groups** or **Group that can manage Office 365 groups** to specify a single group whose members can use self-service.\n+When users can create groups, all users in your organization are allowed to create new groups and then can, as the default owner, add members to these groups. You can't specify individuals who can create their own groups. You can specify individuals only for making another group member a group owner.\n \n ## Next steps\n "
  },
  {
    "Number": 107598,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-13T11:01:58Z",
    "User": "PMEds28",
    "FileName": "articles/api-management/api-management-howto-developer-portal.md",
    "Addition": 6,
    "Delections": 0,
    "Changes": 6,
    "Patch": "@@ -153,6 +153,12 @@ The interactive console makes a client-side API request from the browser. You ca\n </cors>\n ```\n \n+Apply the CORS on the global scope to ensure it's enabled for all APIs.\n+\n+1. Navigate to **All APIs** in the **APIs** section of your API Management service in the Azure portal.\n+2. Click on the **</>** icon in the **Inbound processing** section.\n+3. Insert the policy in the **<inbound>** section of the XML file. Make sure the **<origin>** value matches your developer portal's domain.\n+\n > [!NOTE]\n > \n > If you apply the CORS policy in the Product scope, instead of the API(s) scope, and your API uses subscription key authentication through a header, your console won't work."
  },
  {
    "Number": 107598,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-13T11:01:58Z",
    "User": "PMEds28",
    "FileName": "articles/api-management/api-management-howto-integrate-internal-vnet-appgateway.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -80,7 +80,7 @@ In the first setup example all your APIs are managed only from within your Virtu\n In this guide we will also expose the **developer portal** to external audiences through the Application Gateway. It requires additional steps to create developer portal's listener, probe, settings and rules. All details are provided in respective steps.\n \n > [!WARNING]\n-> If you use Azure AD or third party authentication, please enable [cookie-based session affinity](https://docs.microsoft.com/azure/application-gateway/overview#session-affinity) feature in Application Gateway.\n+> If you use Azure AD or third party authentication, please enable [cookie-based session affinity](../application-gateway/features.md#session-affinity) feature in Application Gateway.\n \n > [!WARNING]\n > To prevent Application Gateway WAF from breaking the download of OpenAPI specification in the developer portal, you need to disable the firewall rule `942200 - \"Detects MySQL comment-/space-obfuscated injections and backtick termination\"`."
  },
  {
    "Number": 107598,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-13T11:01:58Z",
    "User": "PMEds28",
    "FileName": "articles/api-management/api-management-using-with-vnet.md",
    "Addition": 3,
    "Delections": 1,
    "Changes": 4,
    "Patch": "@@ -175,10 +175,12 @@ When an API Management service instance is hosted in a VNET, the ports in the fo\n ## <a name=\"subnet-size\"> </a> Subnet Size Requirement\n Azure reserves some IP addresses within each subnet, and these addresses can't be used. The first and last IP addresses of the subnets are reserved for protocol conformance, along with three more addresses used for Azure services. For more information, see [Are there any restrictions on using IP addresses within these subnets?](../virtual-network/virtual-networks-faq.md#are-there-any-restrictions-on-using-ip-addresses-within-these-subnets)\n \n-In addition to the IP addresses used by the Azure VNET infrastructure, each Api Management instance in the subnet uses two IP addresses per unit of Premium SKU or one IP address for the Developer SKU. Each instance reserves an additional IP address for the external load balancer. When deploying into Internal vnet, it requires an additional IP address for the internal load balancer.\n+In addition to the IP addresses used by the Azure VNET infrastructure, each Api Management instance in the subnet uses two IP addresses per unit of Premium SKU or one IP address for the Developer SKU. Each instance reserves an additional IP address for the external load balancer. When deploying into Internal virtual network, it requires an additional IP address for the internal load balancer.\n \n Given the calculation above the minimum size of the subnet, in which API Management can be deployed is /29 that gives three usable IP addresses.\n \n+Each additional scale unit of API Management requires two more IP addresses.\n+\n ## <a name=\"routing\"> </a> Routing\n + A load balanced public IP address (VIP) will be reserved to provide access to all service endpoints.\n + An IP address from a subnet IP range (DIP) will be used to access resources within the vnet and a public IP address (VIP) will be used to access resources outside the vnet."
  },
  {
    "Number": 107598,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-13T11:01:58Z",
    "User": "PMEds28",
    "FileName": "articles/app-service/containers/how-to-serve-content-from-azure-storage.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -21,7 +21,7 @@ This guide shows how to attach Azure Storage to App Service on Linux. Benefits i\n - [Azure CLI](/cli/azure/install-azure-cli) (2.0.46 or later).\n - An existing [App Service on Linux app](https://docs.microsoft.com/azure/app-service/containers/).\n - An [Azure Storage Account](https://docs.microsoft.com/azure/storage/common/storage-quickstart-create-account?tabs=azure-cli)\n-- An [Azure file share and directory](https://docs.microsoft.com/azure/storage/common/storage-azure-cli#create-and-manage-file-shares).\n+- An [Azure file share and directory](../../storage/files/storage-how-to-use-files-cli.md).\n \n \n ## Limitations of Azure Storage with App Service"
  },
  {
    "Number": 107598,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-13T11:01:58Z",
    "User": "PMEds28",
    "FileName": "articles/application-gateway/configuration-overview.md",
    "Addition": 3,
    "Delections": 3,
    "Changes": 6,
    "Patch": "@@ -123,7 +123,7 @@ Choose HTTP or HTTPS:\n \n - If you choose HTTP, the traffic between the client and the application gateway is unencrypted.\n \n-- Choose HTTPS if you want [SSL termination](https://docs.microsoft.com/azure/application-gateway/overview#secure-sockets-layer-ssltls-termination) or [end-to-end SSL encryption](https://docs.microsoft.com/azure/application-gateway/ssl-overview). The traffic between the client and the application gateway is encrypted. And the SSL connection terminates at the application gateway. If you want end-to-end SSL encryption, you must choose HTTPS and configure the **back-end HTTP** setting. This ensures that traffic is re-encrypted when it travels from the application gateway to the back end.\n+- Choose HTTPS if you want [SSL termination](features.md#secure-sockets-layer-ssltls-termination) or [end-to-end SSL encryption](https://docs.microsoft.com/azure/application-gateway/ssl-overview). The traffic between the client and the application gateway is encrypted. And the SSL connection terminates at the application gateway. If you want end-to-end SSL encryption, you must choose HTTPS and configure the **back-end HTTP** setting. This ensures that traffic is re-encrypted when it travels from the application gateway to the back end.\n \n To configure SSL termination and end-to-end SSL encryption, you must add a certificate to the listener to enable the application gateway to derive a symmetric key. This is dictated by the SSL protocol specification. The symmetric key is used to encrypt and decrypt the traffic that's sent to the gateway. The gateway certificate must be in Personal Information Exchange (PFX) format. This format lets you export the private key that the gateway uses to encrypt and decrypt traffic.\n \n@@ -305,7 +305,7 @@ This setting lets you configure an optional custom forwarding path to use when t\n \n ### Use for app service\n \n-This is a UI only shortcut that selects the two required settings for the Azure App Service back end. It enables **pick host name from back-end address**, and it creates a new custom probe if you don't have one already. (For more information, see the [Pick host name from back-end address](#pick) setting section of this article.) A new probe is created, and the probe header is picked from the back-end member’s address.\n+This is a UI only shortcut that selects the two required settings for the Azure App Service back end. It enables **pick host name from back-end address**, and it creates a new custom probe if you don't have one already. (For more information, see the [Pick host name from back-end address](#pick) setting section of this article.) A new probe is created, and the probe header is picked from the back-end member's address.\n \n ### Use custom probe\n \n@@ -322,7 +322,7 @@ This feature helps when the domain name of the back end is different from the DN\n \n An example case is multi-tenant services as the back end. An app service is a multi-tenant service that uses a shared space with a single IP address. So, an app service can only be accessed through the hostnames that are configured in the custom domain settings.\n \n-By default, the custom domain name is *example.azurewebsites.net*. To access your app service by using an application gateway through a hostname that's not explicitly registered in the app service or through the application gateway’s FQDN, you override the hostname in the original request to the app service’s hostname. To do this, enable the **pick host name from backend address** setting.\n+By default, the custom domain name is *example.azurewebsites.net*. To access your app service by using an application gateway through a hostname that's not explicitly registered in the app service or through the application gateway's FQDN, you override the hostname in the original request to the app service's hostname. To do this, enable the **pick host name from backend address** setting.\n \n For a custom domain whose existing custom DNS name is mapped to the app service, you don't have to enable this setting.\n "
  },
  {
    "Number": 107598,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-13T11:01:58Z",
    "User": "PMEds28",
    "FileName": "articles/application-gateway/ingress-controller-expose-websocket-server.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -11,7 +11,7 @@ ms.author: caya\n \n # Expose a WebSocket server to Application Gateway\n \n-As outlined in the Application Gateway v2 documentation - it [provides native support for the WebSocket and HTTP/2 protocols](https://docs.microsoft.com/azure/application-gateway/overview#websocket-and-http2-traffic). Please note, that for both Application Gateway and the Kubernetes Ingress - there is no user-configurable setting to selectively enable or disable WebSocket support.\n+As outlined in the Application Gateway v2 documentation - it [provides native support for the WebSocket and HTTP/2 protocols](features.md#websocket-and-http2-traffic). Please note, that for both Application Gateway and the Kubernetes Ingress - there is no user-configurable setting to selectively enable or disable WebSocket support.\n \n The Kubernetes deployment YAML below shows the minimum configuration used to deploy a WebSocket server, which is the same as deploying a regular web server:\n ```yaml"
  },
  {
    "Number": 107598,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-13T11:01:58Z",
    "User": "PMEds28",
    "FileName": "articles/automation/troubleshoot/runbooks.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -604,7 +604,7 @@ Run As accounts might not have the same permissions against Azure resources as y\n \n ### Issues passing parameters into webhooks\n \n-For help with passing parameters into webhooks, see [Start a runbook from a webhook](https://docs.microsoft.com/azure/automation/automation-webhooks#parameters).\n+For help with passing parameters into webhooks, see [Start a runbook from a webhook](../automation-webhooks.md#parameters-used-when-the-webhook-starts-a-runbook).\n \n ### Issues using Az modules\n \n@@ -625,7 +625,7 @@ Run As accounts might not have the same permissions against Azure resources as y\n \n ### Passing parameters into webhooks\n \n-For help with passing parameters into webhooks, see [Start a runbook from a webhook](https://docs.microsoft.com/azure/automation/automation-webhooks#parameters).\n+For help with passing parameters into webhooks, see [Start a runbook from a webhook](https://docs.microsoft.com/azure/automation/automation-webhooks#parameters-used-when-the-webhook-starts-a-runbook).\n \n ### Using Az modules\n "
  },
  {
    "Number": 107598,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-13T11:01:58Z",
    "User": "PMEds28",
    "FileName": "articles/azure-app-configuration/concept-private-endpoint.md",
    "Addition": 80,
    "Delections": 0,
    "Changes": 80,
    "Patch": "@@ -0,0 +1,80 @@\n+---\n+title: Using private endpoints for Azure App Configuration\n+description: Secure your App Configuration store using private endpoints\n+services: azure-app-configuration\n+author: lisaguthrie\n+ms.service: azure-app-configuration\n+ms.topic: conceptual\n+ms.date: 3/12/2020\n+ms.author: lcozzens\n+\n+#Customer intent: As a developer using Azure App Configuration, I want to understand how to use private endpoints to enable secure communication with my App Configuration instance.\n+---\n+# Using private endpoints for Azure App Configuration\n+\n+You can use [private endpoints](../private-link/private-endpoint-overview.md) for Azure App Configuration to allow clients on a virtual network (VNet) to securely access data over a [private link](../private-link/private-link-overview.md). The private endpoint uses an IP address from the VNet address space for your App Configuration store. Network traffic between the clients on the VNet and the App Configuration store traverses over the VNet using a private link on the Microsoft backbone network, eliminating exposure to the public internet.\n+\n+Using private endpoints for your App Configuration store enables you to:\n+- Secure your application configuration details by configuring the firewall to block all connections to App Configuration on the public endpoint.\n+- Increase security for the virtual network (VNet) ensuring data doesn't escape from the VNet.\n+- Securely connect to the App Configuration store from on-premises networks that connect to the VNet using [VPN](../vpn-gateway/vpn-gateway-about-vpngateways.md) or [ExpressRoutes](../expressroute/expressroute-locations.md) with private-peering.\n+\n+> [!NOTE]\n+> Azure App Configuration offers the use of private endpoints as a public preview. Public preview offerings allow customers to experiment with new features prior to their official release.  Public preview features and services are not meant for production use.\n+\n+## Conceptual Overview\n+\n+A private endpoint is a special network interface for an Azure service in your [Virtual Network](../virtual-network/virtual-networks-overview.md) (VNet). When you create a private endpoint for your App Config store, it provides secure connectivity between clients on your VNet and your configuration store. The private endpoint is assigned an IP address from the IP address range of your VNet. The connection between the private endpoint and the configuration store uses a secure private link.\n+\n+Applications in the VNet can connect to the configuration store over the private endpoint **using the same connection strings and authorization mechanisms that they would use otherwise**. Private endpoints can be used with all protocols supported by the App Configuration store.\n+\n+While App Configuration doesn't support service endpoints, private endpoints can be created in subnets that use [Service Endpoints](../virtual-network/virtual-network-service-endpoints-overview.md). Clients in a subnet can connect securely to an App Configuration store using the private endpoint while using service endpoints to access others.  \n+\n+When you create a private endpoint for a service in your VNet, a consent request is sent for approval to the service account owner. If the user requesting the creation of the private endpoint is also an owner of the account, this consent request is automatically approved.\n+\n+Service account owners can manage consent requests and private endpoints through the `Private Endpoints` tab of the config store in the [Azure portal](https://portal.azure.com).\n+\n+### Private Endpoints for App Configuration \n+\n+When creating a private endpoint, you must specify the App Configuration store to which it connects. If you have multiple App Configuration instances within an account, you need a separate private endpoint for each store.\n+\n+#### Resources for creating private endpoints\n+\n+For more detailed information on creating a private endpoint for your App Configuration store, refer to the following articles:\n+\n+- [Create a private endpoint using the Private Link Center in the Azure portal](../private-link/create-private-endpoint-portal.md)\n+- [Create a private endpoint using Azure CLI](../private-link/create-private-endpoint-cli.md)\n+- [Create a private endpoint using Azure PowerShell](../private-link/create-private-endpoint-powershell.md)\n+\n+### Connecting to Private Endpoints\n+\n+Azure relies upon DNS resolution to route connections from the VNet to the configuration store over a private link. You can quickly find connections strings in the Azure portal by selecting your App Configuration store, then selecting **Settings** > **Access Keys**.  \n+\n+> [!IMPORTANT]\n+> Use the same connection string to connect to your App Configuration store using private endpoints as you would use for a public endpoint. Don't connect to the storage account using its `privatelink` subdomain URL.\n+\n+## DNS changes for Private Endpoints\n+\n+When you create a private endpoint, the DNS CNAME resource record for the configuration store is updated to an alias in a subdomain with the prefix `privatelink`. Azure also creates a [private DNS zone](../dns/private-dns-overview.md) corresponding to the `privatelink` subdomain, with the DNS A resource records for the private endpoints.\n+\n+When you resolve the endpoint URL from outside the VNet, it resolves to the public endpoint of the store. When resolved from within the VNet hosting the private endpoint, the endpoint URL resolves to the private endpoint.\n+\n+You can control access for clients outside the VNet through the public endpoint using the Azure Firewall service.\n+\n+This approach enables access to the store **using the same connection string** for clients on the VNet hosting the private endpoints as well as clients outside the VNet.\n+\n+If you are using a custom DNS server on your network, clients must be able to resolve the fully qualified domain name (FQDN) for the service endpoint to the private endpoint IP address. Configure your DNS server to delegate your private link subdomain to the private DNS zone for the VNet, or configure the A records for `AppConfigInstanceA.privatelink.azconfig.io` with the private endpoint IP address.\n+\n+> [!TIP]\n+> When using a custom or on-premises DNS server, you should configure your DNS server to resolve the store name in the `privatelink` subdomain to the private endpoint IP address. You can do this by delegating the `privatelink` subdomain to the private DNS zone of the VNet, or configuring the DNS zone on your DNS server and adding the DNS A records.\n+\n+#### Resources for configuring your DNS server with private endpoints\n+\n+For more information, see:\n+\n+- [Name resolution for resources in Azure virtual networks](/azure/virtual-network/virtual-networks-name-resolution-for-vms-and-role-instances#name-resolution-that-uses-your-own-dns-server)\n+- [DNS configuration for Private Endpoints](/azure/private-link/private-endpoint-overview#dns-configuration)\n+\n+## Pricing\n+\n+Enabling private endpoints requires a [Standard tier](https://azure.microsoft.com/pricing/details/app-configuration/) App Configuration store.  To learn about private link pricing details, see [Azure Private Link pricing](https://azure.microsoft.com/pricing/details/private-link).\n\\ No newline at end of file"
  },
  {
    "Number": 107598,
    "Title": "Merge Master to Live, 4 AM",
    "ClosedAt": "2020-03-13T11:01:58Z",
    "User": "PMEds28",
    "FileName": "articles/azure-government/compliance/azure-services-in-fedramp-auditscope.md",
    "Addition": 5,
    "Delections": 2,
    "Changes": 7,
    "Patch": "@@ -26,7 +26,7 @@ This article provides a detailed list of in-scope cloud services across Azure Pu\n * Planned 2020 = indicates the service will be reviewed by 3PAO and JAB in 2020. Once the service is authorized, status will be updated \n \n ## Azure public services by audit scope\n-| _Last Updated: February 2020_ |\n+| _Last Updated: March 2020_ |\n \n | Azure Service| DoD CC SRG IL 2 | FedRAMP Moderate | FedRAMP High | Planned 2020 |\n | ------------ |:---------------:|:----------------:|:------------:|:------------:|\n@@ -166,7 +166,7 @@ This article provides a detailed list of in-scope cloud services across Azure Pu\n \n \n ## Azure Government services by audit scope\n-| _Last Updated: February 2020_ |\n+| _Last Updated: March 2020_ |\n \n | Azure Service | DoD CC SRG IL 2 | DoD CC SRG IL 4 | DoD CC SRG IL 5 | FedRAMP High | Planned 2020\n | ------------- |:---------------:|:---------------:|:---------------:|:------------:|:------------:\n@@ -205,8 +205,10 @@ This article provides a detailed list of in-scope cloud services across Azure Pu\n | [Azure Information Protection](https://azure.microsoft.com/services/information-protection/) | :heavy_check_mark: | :heavy_check_mark: |  | :heavy_check_mark: | \n | [Azure Intune](https://docs.microsoft.com/intune/what-is-intune) | :heavy_check_mark: | :heavy_check_mark: |  | :heavy_check_mark: | \n | [Azure Kubernetes Service (AKS)](https://azure.microsoft.com/services/kubernetes-service/) |  |  |  |  | :heavy_check_mark:\n+| [Azure Lighthouse](https://azure.microsoft.com/services/azure-lighthouse/)| :heavy_check_mark: | :heavy_check_mark: |  | :heavy_check_mark: | \n | [Azure Lab Services](https://azure.microsoft.com/services/lab-services/) | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n | [Azure Managed Applications](https://azure.microsoft.com/services/managed-applications/) | :heavy_check_mark: | :heavy_check_mark: |  | :heavy_check_mark: | :heavy_check_mark:\n+| [Azure Maps](https://azure.microsoft.com/services/azure-maps/)| :heavy_check_mark: | :heavy_check_mark: |  | :heavy_check_mark: |\n | [Azure Migrate](https://azure.microsoft.com/services/azure-migrate/) | :heavy_check_mark: | :heavy_check_mark: |  | :heavy_check_mark: | \n | [Azure Monitor](https://azure.microsoft.com/services/monitor/) | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n | [Azure Policy](https://azure.microsoft.com/services/azure-policy/) | :heavy_check_mark: | :heavy_check_mark: |  | :heavy_check_mark: | \n@@ -226,6 +228,7 @@ This article provides a detailed list of in-scope cloud services across Azure Pu\n | [Cognitive Services: Speech Services](https://azure.microsoft.com/services/cognitive-services/directory/speech/)  | :heavy_check_mark: | :heavy_check_mark: |  | :heavy_check_mark: | \n | [Cognitive Services: Text Analytics](https://azure.microsoft.com/services/cognitive-services/text-analytics/) | :heavy_check_mark: | :heavy_check_mark: |  | :heavy_check_mark: | \n | [Cognitive Services: Translator Text](https://azure.microsoft.com/services/cognitive-services/speech-translation/) | :heavy_check_mark: | :heavy_check_mark: |  | :heavy_check_mark: | \n+| [Container Instances](https://azure.microsoft.com/services/container-instances/)| :heavy_check_mark: | :heavy_check_mark: |  | :heavy_check_mark: | \n | [D365 Integrator App](https://docs.microsoft.com/power-platform/admin/data-integrator) | :heavy_check_mark: | :heavy_check_mark: |  | :heavy_check_mark: | \n | [Dynamics 365 Service Omni-Channel Engagement Hub](https://docs.microsoft.com/dynamics365/omnichannel/introduction-omnichannel) | :heavy_check_mark: | :heavy_check_mark: |  | :heavy_check_mark: | \n | [Dynamics 365 Forms Pro](https://docs.microsoft.com/forms-pro/get-started) | :heavy_check_mark: | :heavy_check_mark: |  | :heavy_check_mark: | "
  },
  {
    "Number": 107476,
    "Title": "fixed indent and image extensions",
    "ClosedAt": "2020-03-12T18:59:49Z",
    "User": "MGoedtel",
    "FileName": "articles/azure-monitor/insights/container-insights-livedata-metrics.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -29,7 +29,7 @@ This feature performs a polling operation against the metrics endpoints (includi\n \n The polling interval is configured from the **Set interval** drop-down allowing you to set polling for new data every 1, 5, 15 and 30 seconds. \n \n-![Go Live drop-down polling interval](./media/container-insights-livedata-metrics/cluster-view-polling-interval-dropdown.ping.png)\n+![Go Live drop-down polling interval](./media/container-insights-livedata-metrics/cluster-view-polling-interval-dropdown.png)\n \n >[!IMPORTANT]\n >We recommend setting the polling interval to one second while troubleshooting an issue for a short period of time. These requests may impact the availability and throttling of the Kubernetes API on your cluster. Afterwards, reconfigure to a longer polling interval. "
  },
  {
    "Number": 107476,
    "Title": "fixed indent and image extensions",
    "ClosedAt": "2020-03-12T18:59:49Z",
    "User": "MGoedtel",
    "FileName": "articles/azure-monitor/insights/container-insights-livedata-overview.md",
    "Addition": 4,
    "Delections": 4,
    "Changes": 8,
    "Patch": "@@ -52,7 +52,7 @@ You can view real-time log data as they are generated by the container engine fr\n 4. Select an object from the performance grid, and on the properties pane found on the right side, select **View live data (preview)** option. If the AKS cluster is configured with single sign-on using Azure AD, you are prompted to authenticate on first use during that browser session. Select your account and complete authentication with Azure.  \n \n     >[!NOTE]\n-    >When viewing the data from your Log Analytics workspace by selecting the **View in analytics** option from the properties pane, the log search results will potentially show **Nodes**, **Daemon Sets**, **Replica Sets**, **Jobs**, **Cron Jobs**, **Pods**, and **Containers** which may no longer exist. Attempting to search logs for a container which isn’t available in `kubectl` will also fail here. Review the [View in analytics](container-insights-log-search.md#search-logs-to-analyze-data) feature to learn more about viewing historical logs, events and metrics.  \n+    >When viewing the data from your Log Analytics workspace by selecting the **View in analytics** option from the properties pane, the log search results will potentially show **Nodes**, **Daemon Sets**, **Replica Sets**, **Jobs**, **Cron Jobs**, **Pods**, and **Containers** which may no longer exist. Attempting to search logs for a container which isn't available in `kubectl` will also fail here. Review the [View in analytics](container-insights-log-search.md#search-logs-to-analyze-data) feature to learn more about viewing historical logs, events and metrics.  \n \n After successfully authenticating, the Live Data (preview) console pane will appear below the performance data grid where you can view log data in a continuous stream. If the fetch status indicator shows a green check mark, which is on the far right of the pane, it means data can be retrieved and it begins streaming to your console.  \n \n@@ -73,13 +73,13 @@ You can view real-time event data as they are generated by the container engine\n 4. Select an object from the performance grid, and on the properties pane found on the right side, select **View live data (preview)** option. If the AKS cluster is configured with single sign-on using Azure AD, you are prompted to authenticate on first use during that browser session. Select your account and complete authentication with Azure.  \n \n     >[!NOTE]\n-    >When viewing the data from your Log Analytics workspace by selecting the **View in analytics** option from the properties pane, the log search results will potentially show **Nodes**, **Daemon Sets**, **Replica Sets**, **Jobs**, **Cron Jobs**, **Pods**, and **Containers** which may no longer exist. Attempting to search logs for a container which isn’t available in `kubectl` will also fail here. Review the [View in analytics](container-insights-log-search.md#search-logs-to-analyze-data) feature to learn more about viewing historical logs, events and metrics.  \n+    >When viewing the data from your Log Analytics workspace by selecting the **View in analytics** option from the properties pane, the log search results will potentially show **Nodes**, **Daemon Sets**, **Replica Sets**, **Jobs**, **Cron Jobs**, **Pods**, and **Containers** which may no longer exist. Attempting to search logs for a container which isn't available in `kubectl` will also fail here. Review the [View in analytics](container-insights-log-search.md#search-logs-to-analyze-data) feature to learn more about viewing historical logs, events and metrics.  \n \n After successfully authenticating, the Live Data (preview) console pane will appear below the performance data grid. If the fetch status indicator shows a green check mark, which is on the far right of the pane, it means data can be retrieved and it begins streaming to your console. \n     \n If the object you selected was a container, select the **Events** option in the pane. If you selected a Node, Pod, or controller, viewing events is automatically selected. \n \n-    ![Controller properties pane view events](./media/container-insights-livedata-overview/controller-properties-live-events.png)  \n+![Controller properties pane view events](./media/container-insights-livedata-overview/controller-properties-live-event.png)  \n \n The pane title shows the name of the Pod the container is grouped with.\n \n@@ -100,7 +100,7 @@ You can view real-time metric data as they are generated by the container engine\n 4. Select a **Pod** object from the performance grid, and on the properties pane found on the right side, select **View live data (preview)** option. If the AKS cluster is configured with single sign-on using Azure AD, you are prompted to authenticate on first use during that browser session. Select your account and complete authentication with Azure.  \n \n     >[!NOTE]\n-    >When viewing the data from your Log Analytics workspace by selecting the **View in analytics** option from the properties pane, the log search results will potentially show **Nodes**, **Daemon Sets**, **Replica Sets**, **Jobs**, **Cron Jobs**, **Pods**, and **Containers** which may no longer exist. Attempting to search logs for a container which isn’t available in `kubectl` will also fail here. Review the [View in analytics](container-insights-log-search.md#search-logs-to-analyze-data) feature to learn more about viewing historical logs, events and metrics.  \n+    >When viewing the data from your Log Analytics workspace by selecting the **View in analytics** option from the properties pane, the log search results will potentially show **Nodes**, **Daemon Sets**, **Replica Sets**, **Jobs**, **Cron Jobs**, **Pods**, and **Containers** which may no longer exist. Attempting to search logs for a container which isn't available in `kubectl` will also fail here. Review the [View in analytics](container-insights-log-search.md#search-logs-to-analyze-data) feature to learn more about viewing historical logs, events and metrics.  \n \n After successfully authenticating, the Live Data (preview) console pane will appear below the performance data grid. Metric data is retrieved and begins streaming to your console for presentation in the two charts. The pane title shows the name of the pod the container is grouped with.\n "
  },
  {
    "Number": 107381,
    "Title": "Remove preview from OpenCensus Python",
    "ClosedAt": "2020-03-12T06:27:56Z",
    "User": "lzchen",
    "FileName": "articles/azure-monitor/app/app-insights-overview.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -98,7 +98,7 @@ There are several ways to get started. Begin with whichever works best for you.\n   * [.NET Console Applications](../../azure-monitor/app/console.md)\n   * [Java](../../azure-monitor/app/java-get-started.md)\n   * [Node.js](../../azure-monitor/app/nodejs.md)\n-  * [Python (preview)](../../azure-monitor/app/opencensus-python.md)\n+  * [Python](../../azure-monitor/app/opencensus-python.md)\n   * [Other platforms](../../azure-monitor/app/platforms.md)\n * **[Instrument your web pages](../../azure-monitor/app/javascript.md)** for page view, AJAX, and other client-side telemetry.\n * **[Analyze mobile app usage](../../azure-monitor/learn/mobile-center-quickstart.md)** by integrating with Visual Studio App Center.\n@@ -117,7 +117,7 @@ Get started at development time with:\n * [ASP.NET Core](../../azure-monitor/app/asp-net-core.md)\n * [Java](../../azure-monitor/app/java-get-started.md)\n * [Node.js](../../azure-monitor/app/nodejs.md)\n-* [Python (preview)](../../azure-monitor/app/opencensus-python.md)\n+* [Python](../../azure-monitor/app/opencensus-python.md)\n \n \n ## Support and feedback"
  },
  {
    "Number": 107381,
    "Title": "Remove preview from OpenCensus Python",
    "ClosedAt": "2020-03-12T06:27:56Z",
    "User": "lzchen",
    "FileName": "articles/azure-monitor/app/distributed-tracing.md",
    "Addition": 5,
    "Delections": 5,
    "Changes": 10,
    "Patch": "@@ -11,11 +11,11 @@ ms.reviewer: mbullwin\n \n # What is Distributed Tracing?\n \n-The advent of modern cloud and [microservices](https://azure.com/microservices) architectures has given rise to simple, independently deployable services that can help reduce costs while increasing availability and throughput. But while these movements have made individual services easier to understand as a whole, they’ve made overall systems more difficult to reason about and debug.\n+The advent of modern cloud and [microservices](https://azure.com/microservices) architectures has given rise to simple, independently deployable services that can help reduce costs while increasing availability and throughput. But while these movements have made individual services easier to understand as a whole, they've made overall systems more difficult to reason about and debug.\n \n-In monolithic architectures, we’ve gotten used to debugging with call stacks. Call stacks are brilliant tools for showing the flow of execution (Method A called Method B, which called Method C), along with details and parameters about each of those calls. This is great for monoliths or services running on a single process, but how do we debug when the call is across a process boundary, not simply a reference on the local stack? \n+In monolithic architectures, we've gotten used to debugging with call stacks. Call stacks are brilliant tools for showing the flow of execution (Method A called Method B, which called Method C), along with details and parameters about each of those calls. This is great for monoliths or services running on a single process, but how do we debug when the call is across a process boundary, not simply a reference on the local stack? \n \n-That’s where distributed tracing comes in.  \n+That's where distributed tracing comes in.  \n \n Distributed tracing is the equivalent of call stacks for modern cloud and microservices architectures, with the addition of a simplistic performance profiler thrown in. In Azure Monitor, we provide two experiences for consuming distributed trace data. The first is our [transaction diagnostics](https://docs.microsoft.com/azure/application-insights/app-insights-transaction-diagnostics) view, which is like a call stack with a time dimension added in. The transaction diagnostics view provides visibility into one single transaction/request, and is helpful for finding the root cause of reliability issues and performance bottlenecks on a per request basis.\n \n@@ -34,7 +34,7 @@ The Application Insights SDKs for .NET, .NET Core, Java, Node.js, and JavaScript\n * [Java](https://docs.microsoft.com/azure/application-insights/app-insights-java-get-started)\n * [Node.js](https://docs.microsoft.com/azure/application-insights/app-insights-nodejs-quick-start)\n * [JavaScript](https://docs.microsoft.com/azure/application-insights/app-insights-javascript)\n-* [Python (preview)](opencensus-python.md)\n+* [Python](opencensus-python.md)\n \n With the proper Application Insights SDK installed and configured, tracing information is automatically collected for popular frameworks, libraries, and technologies by SDK dependency auto-collectors. The full list of supported technologies is available in [the Dependency auto-collection documentation](https://docs.microsoft.com/azure/application-insights/auto-collect-dependencies).\n \n@@ -44,7 +44,7 @@ With the proper Application Insights SDK installed and configured, tracing infor\n \n In addition to the Application Insights SDKs, Application Insights also supports distributed tracing through [OpenCensus](https://opencensus.io/). OpenCensus is an open source, vendor-agnostic, single distribution of libraries to provide metrics collection and distributed tracing for services. It also enables the open source community to enable distributed tracing with popular technologies like Redis, Memcached, or MongoDB. [Microsoft collaborates on OpenCensus with several other monitoring and cloud partners](https://open.microsoft.com/2018/06/13/microsoft-joins-the-opencensus-project/).\n \n-[Python (preview)](opencensus-python.md) \n+[Python](opencensus-python.md) \n \n The OpenCensus website maintains API reference documentation for [Python](https://opencensus.io/api/python/trace/usage.html) and [Go](https://godoc.org/go.opencensus.io), as well as various different guides for using OpenCensus. \n "
  },
  {
    "Number": 107381,
    "Title": "Remove preview from OpenCensus Python",
    "ClosedAt": "2020-03-12T06:27:56Z",
    "User": "lzchen",
    "FileName": "articles/azure-monitor/app/platforms.md",
    "Addition": 3,
    "Delections": 3,
    "Changes": 6,
    "Patch": "@@ -13,7 +13,7 @@ ms.reviewer: olegan\n * [Java](../../azure-monitor/app/java-get-started.md)\n * [JavaScript](../../azure-monitor/app/javascript.md)\n * [Node.JS](../../azure-monitor/app/nodejs.md)\n-* [Python (preview)](../../azure-monitor/app/opencensus-python.md)\n+* [Python](../../azure-monitor/app/opencensus-python.md)\n \n ## Supported platforms and frameworks\n \n@@ -30,7 +30,7 @@ ms.reviewer: olegan\n * [iOS](../../azure-monitor/learn/mobile-center-quickstart.md) (App Center)\n * [Java EE](../../azure-monitor/app/java-get-started.md)\n * [Node.JS](https://www.npmjs.com/package/applicationinsights)\n-* [Python (preview)](../../azure-monitor/app/opencensus-python.md)\n+* [Python](../../azure-monitor/app/opencensus-python.md)\n * [Universal Windows app](../../azure-monitor/learn/mobile-center-quickstart.md) (App Center)\n * [Windows desktop applications, services, and worker roles](../../azure-monitor/app/windows-desktop.md)\n \n@@ -46,4 +46,4 @@ ms.reviewer: olegan\n * [Stream Analytics](../../azure-monitor/app/export-power-bi.md)\n \n ## Unsupported SDKs\n-We're aware that several other community-supported SDKs exist. However, Azure Monitor only provides support when using the supported SDKs listed on this page. We’re constantly assessing opportunities to expand our support for other languages, so follow our [GitHub Announcements](https://github.com/microsoft/ApplicationInsights-Announcements/issues) page to receive the latest SDK news. \n+We're aware that several other community-supported SDKs exist. However, Azure Monitor only provides support when using the supported SDKs listed on this page. We're constantly assessing opportunities to expand our support for other languages, so follow our [GitHub Announcements](https://github.com/microsoft/ApplicationInsights-Announcements/issues) page to receive the latest SDK news. "
  },
  {
    "Number": 107381,
    "Title": "Remove preview from OpenCensus Python",
    "ClosedAt": "2020-03-12T06:27:56Z",
    "User": "lzchen",
    "FileName": "articles/azure-monitor/app/sdk-connection-string.md",
    "Addition": 4,
    "Delections": 4,
    "Changes": 8,
    "Patch": "@@ -102,7 +102,7 @@ See also: https://docs.microsoft.com/azure/azure-monitor/app/custom-endpoints#re\n \n In this example, only the Instrumentation Key has been set.\n \n-- Authorization scheme defaults to “ikey” \n+- Authorization scheme defaults to \"ikey\" \n - Instrumentation Key: 00000000-0000-0000-0000-000000000000\n - The regional service URIs are based on the [SDK defaults](https://github.com/microsoft/ApplicationInsights-dotnet/blob/e50d569cebf485e72e98f4a08a0bc0e30cdf42bc/BASE/src/Microsoft.ApplicationInsights/Extensibility/Implementation/Endpoints/Constants.cs#L6) and will connect to the public global Azure:\n    - Ingestion: https://dc.services.visualstudio.com/\n@@ -118,7 +118,7 @@ In this example, only the Instrumentation Key has been set.\n \n In this example, this connection string specifies the endpoint suffix and the SDK will construct service endpoints.\n \n-- Authorization scheme defaults to “ikey” \n+- Authorization scheme defaults to \"ikey\" \n - Instrumentation Key: 00000000-0000-0000-0000-000000000000\n - The regional service URIs are based on provided endpoint suffix: \n    - Ingestion: https://dc.ai.contoso.com\n@@ -134,7 +134,7 @@ In this example, this connection string specifies the endpoint suffix and the SD\n \n In this example, this connection string specifies explicit overrides for every service. The SDK will use the exact endpoints provided without modification.\n \n-- Authorization scheme defaults to “ikey” \n+- Authorization scheme defaults to \"ikey\" \n - Instrumentation Key: 00000000-0000-0000-0000-000000000000\n - The regional service URIs are based on the explicit override values: \n    - Ingestion: https:\\//custom.com:111/\n@@ -275,4 +275,4 @@ Get started at development time with:\n * [ASP.NET Core](../../azure-monitor/app/asp-net-core.md)\n * [Java](../../azure-monitor/app/java-get-started.md)\n * [Node.js](../../azure-monitor/app/nodejs.md)\n-* [Python (preview)](../../azure-monitor/app/opencensus-python.md)\n+* [Python](../../azure-monitor/app/opencensus-python.md)"
  },
  {
    "Number": 107418,
    "Title": "Remove preview from OpenCensus Python",
    "ClosedAt": "2020-03-12T06:07:32Z",
    "User": "lzchen",
    "FileName": "articles/azure-monitor/app/app-insights-overview.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -98,7 +98,7 @@ There are several ways to get started. Begin with whichever works best for you.\n   * [.NET Console Applications](../../azure-monitor/app/console.md)\n   * [Java](../../azure-monitor/app/java-get-started.md)\n   * [Node.js](../../azure-monitor/app/nodejs.md)\n-  * [Python (preview)](../../azure-monitor/app/opencensus-python.md)\n+  * [Python](../../azure-monitor/app/opencensus-python.md)\n   * [Other platforms](../../azure-monitor/app/platforms.md)\n * **[Instrument your web pages](../../azure-monitor/app/javascript.md)** for page view, AJAX, and other client-side telemetry.\n * **[Analyze mobile app usage](../../azure-monitor/learn/mobile-center-quickstart.md)** by integrating with Visual Studio App Center.\n@@ -117,7 +117,7 @@ Get started at development time with:\n * [ASP.NET Core](../../azure-monitor/app/asp-net-core.md)\n * [Java](../../azure-monitor/app/java-get-started.md)\n * [Node.js](../../azure-monitor/app/nodejs.md)\n-* [Python (preview)](../../azure-monitor/app/opencensus-python.md)\n+* [Python](../../azure-monitor/app/opencensus-python.md)\n \n \n ## Support and feedback"
  },
  {
    "Number": 107418,
    "Title": "Remove preview from OpenCensus Python",
    "ClosedAt": "2020-03-12T06:07:32Z",
    "User": "lzchen",
    "FileName": "articles/azure-monitor/app/distributed-tracing.md",
    "Addition": 5,
    "Delections": 5,
    "Changes": 10,
    "Patch": "@@ -11,11 +11,11 @@ ms.reviewer: mbullwin\n \n # What is Distributed Tracing?\n \n-The advent of modern cloud and [microservices](https://azure.com/microservices) architectures has given rise to simple, independently deployable services that can help reduce costs while increasing availability and throughput. But while these movements have made individual services easier to understand as a whole, they’ve made overall systems more difficult to reason about and debug.\n+The advent of modern cloud and [microservices](https://azure.com/microservices) architectures has given rise to simple, independently deployable services that can help reduce costs while increasing availability and throughput. But while these movements have made individual services easier to understand as a whole, they've made overall systems more difficult to reason about and debug.\n \n-In monolithic architectures, we’ve gotten used to debugging with call stacks. Call stacks are brilliant tools for showing the flow of execution (Method A called Method B, which called Method C), along with details and parameters about each of those calls. This is great for monoliths or services running on a single process, but how do we debug when the call is across a process boundary, not simply a reference on the local stack? \n+In monolithic architectures, we've gotten used to debugging with call stacks. Call stacks are brilliant tools for showing the flow of execution (Method A called Method B, which called Method C), along with details and parameters about each of those calls. This is great for monoliths or services running on a single process, but how do we debug when the call is across a process boundary, not simply a reference on the local stack? \n \n-That’s where distributed tracing comes in.  \n+That's where distributed tracing comes in.  \n \n Distributed tracing is the equivalent of call stacks for modern cloud and microservices architectures, with the addition of a simplistic performance profiler thrown in. In Azure Monitor, we provide two experiences for consuming distributed trace data. The first is our [transaction diagnostics](https://docs.microsoft.com/azure/application-insights/app-insights-transaction-diagnostics) view, which is like a call stack with a time dimension added in. The transaction diagnostics view provides visibility into one single transaction/request, and is helpful for finding the root cause of reliability issues and performance bottlenecks on a per request basis.\n \n@@ -34,7 +34,7 @@ The Application Insights SDKs for .NET, .NET Core, Java, Node.js, and JavaScript\n * [Java](https://docs.microsoft.com/azure/application-insights/app-insights-java-get-started)\n * [Node.js](https://docs.microsoft.com/azure/application-insights/app-insights-nodejs-quick-start)\n * [JavaScript](https://docs.microsoft.com/azure/application-insights/app-insights-javascript)\n-* [Python (preview)](opencensus-python.md)\n+* [Python](opencensus-python.md)\n \n With the proper Application Insights SDK installed and configured, tracing information is automatically collected for popular frameworks, libraries, and technologies by SDK dependency auto-collectors. The full list of supported technologies is available in [the Dependency auto-collection documentation](https://docs.microsoft.com/azure/application-insights/auto-collect-dependencies).\n \n@@ -44,7 +44,7 @@ With the proper Application Insights SDK installed and configured, tracing infor\n \n In addition to the Application Insights SDKs, Application Insights also supports distributed tracing through [OpenCensus](https://opencensus.io/). OpenCensus is an open source, vendor-agnostic, single distribution of libraries to provide metrics collection and distributed tracing for services. It also enables the open source community to enable distributed tracing with popular technologies like Redis, Memcached, or MongoDB. [Microsoft collaborates on OpenCensus with several other monitoring and cloud partners](https://open.microsoft.com/2018/06/13/microsoft-joins-the-opencensus-project/).\n \n-[Python (preview)](opencensus-python.md) \n+[Python](opencensus-python.md) \n \n The OpenCensus website maintains API reference documentation for [Python](https://opencensus.io/api/python/trace/usage.html) and [Go](https://godoc.org/go.opencensus.io), as well as various different guides for using OpenCensus. \n "
  },
  {
    "Number": 107418,
    "Title": "Remove preview from OpenCensus Python",
    "ClosedAt": "2020-03-12T06:07:32Z",
    "User": "lzchen",
    "FileName": "articles/azure-monitor/app/platforms.md",
    "Addition": 3,
    "Delections": 3,
    "Changes": 6,
    "Patch": "@@ -13,7 +13,7 @@ ms.reviewer: olegan\n * [Java](../../azure-monitor/app/java-get-started.md)\n * [JavaScript](../../azure-monitor/app/javascript.md)\n * [Node.JS](../../azure-monitor/app/nodejs.md)\n-* [Python (preview)](../../azure-monitor/app/opencensus-python.md)\n+* [Python](../../azure-monitor/app/opencensus-python.md)\n \n ## Supported platforms and frameworks\n \n@@ -30,7 +30,7 @@ ms.reviewer: olegan\n * [iOS](../../azure-monitor/learn/mobile-center-quickstart.md) (App Center)\n * [Java EE](../../azure-monitor/app/java-get-started.md)\n * [Node.JS](https://www.npmjs.com/package/applicationinsights)\n-* [Python (preview)](../../azure-monitor/app/opencensus-python.md)\n+* [Python](../../azure-monitor/app/opencensus-python.md)\n * [Universal Windows app](../../azure-monitor/learn/mobile-center-quickstart.md) (App Center)\n * [Windows desktop applications, services, and worker roles](../../azure-monitor/app/windows-desktop.md)\n \n@@ -46,4 +46,4 @@ ms.reviewer: olegan\n * [Stream Analytics](../../azure-monitor/app/export-power-bi.md)\n \n ## Unsupported SDKs\n-We're aware that several other community-supported SDKs exist. However, Azure Monitor only provides support when using the supported SDKs listed on this page. We’re constantly assessing opportunities to expand our support for other languages, so follow our [GitHub Announcements](https://github.com/microsoft/ApplicationInsights-Announcements/issues) page to receive the latest SDK news. \n+We're aware that several other community-supported SDKs exist. However, Azure Monitor only provides support when using the supported SDKs listed on this page. We're constantly assessing opportunities to expand our support for other languages, so follow our [GitHub Announcements](https://github.com/microsoft/ApplicationInsights-Announcements/issues) page to receive the latest SDK news. "
  },
  {
    "Number": 107418,
    "Title": "Remove preview from OpenCensus Python",
    "ClosedAt": "2020-03-12T06:07:32Z",
    "User": "lzchen",
    "FileName": "articles/azure-monitor/app/sdk-connection-string.md",
    "Addition": 4,
    "Delections": 4,
    "Changes": 8,
    "Patch": "@@ -102,7 +102,7 @@ See also: https://docs.microsoft.com/azure/azure-monitor/app/custom-endpoints#re\n \n In this example, only the Instrumentation Key has been set.\n \n-- Authorization scheme defaults to “ikey” \n+- Authorization scheme defaults to \"ikey\" \n - Instrumentation Key: 00000000-0000-0000-0000-000000000000\n - The regional service URIs are based on the [SDK defaults](https://github.com/microsoft/ApplicationInsights-dotnet/blob/e50d569cebf485e72e98f4a08a0bc0e30cdf42bc/BASE/src/Microsoft.ApplicationInsights/Extensibility/Implementation/Endpoints/Constants.cs#L6) and will connect to the public global Azure:\n    - Ingestion: https://dc.services.visualstudio.com/\n@@ -118,7 +118,7 @@ In this example, only the Instrumentation Key has been set.\n \n In this example, this connection string specifies the endpoint suffix and the SDK will construct service endpoints.\n \n-- Authorization scheme defaults to “ikey” \n+- Authorization scheme defaults to \"ikey\" \n - Instrumentation Key: 00000000-0000-0000-0000-000000000000\n - The regional service URIs are based on provided endpoint suffix: \n    - Ingestion: https://dc.ai.contoso.com\n@@ -134,7 +134,7 @@ In this example, this connection string specifies the endpoint suffix and the SD\n \n In this example, this connection string specifies explicit overrides for every service. The SDK will use the exact endpoints provided without modification.\n \n-- Authorization scheme defaults to “ikey” \n+- Authorization scheme defaults to \"ikey\" \n - Instrumentation Key: 00000000-0000-0000-0000-000000000000\n - The regional service URIs are based on the explicit override values: \n    - Ingestion: https:\\//custom.com:111/\n@@ -275,4 +275,4 @@ Get started at development time with:\n * [ASP.NET Core](../../azure-monitor/app/asp-net-core.md)\n * [Java](../../azure-monitor/app/java-get-started.md)\n * [Node.js](../../azure-monitor/app/nodejs.md)\n-* [Python (preview)](../../azure-monitor/app/opencensus-python.md)\n+* [Python](../../azure-monitor/app/opencensus-python.md)"
  },
  {
    "Number": 107267,
    "Title": "M47969: Fixing typo public/private",
    "ClosedAt": "2020-03-11T18:34:12Z",
    "User": "v-maudel",
    "FileName": "articles/azure-monitor/platform/diagnostics-extension-schema-windows.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -502,7 +502,7 @@ The *PublicConfig* and *PrivateConfig* are separated because in most JSON usage\n ```\n \n > [!NOTE]\n-> The public config Azure Monitor sink definition has two properties, *PrincipalId* and *Secret*. These are only required for Classic VMs and Classic Cloud services. These properties should not be used for other resources.\n+> The private config Azure Monitor sink definition has two properties, *PrincipalId* and *Secret*. These are only required for Classic VMs and Classic Cloud services. These properties should not be used for other resources.\n \n \n ```json\n@@ -670,4 +670,4 @@ The *PublicConfig* and *PrivateConfig* are separated because in most JSON usage\n > [!NOTE]\n > The public config Azure Monitor sink definition has two properties, resourceId and region. These are only required for Classic VMs and Classic Cloud services. These properties should not be used for Resource Manager Virtual Machines or Virtual Machine Scale sets.\n > There is also an additional Private Config element for the Azure Monitor sink, that passes in a Principal Id and Secret. This is only required for Classic VMs and Classic Cloud Services. For Resource Manager VMs and VMSS the Azure Monitor definition in the private config element can be excluded.\n->\n\\ No newline at end of file\n+>"
  },
  {
    "Number": 106970,
    "Title": "Add description for SEO.",
    "ClosedAt": "2020-03-10T17:55:39Z",
    "User": "TimShererWithAquent",
    "FileName": "articles/azure-monitor/app/custom-data-correlation.md",
    "Addition": 6,
    "Delections": 6,
    "Changes": 12,
    "Patch": "@@ -1,6 +1,6 @@\n ---\n title: Azure Application Insights | Microsoft Docs\n-description: \n+description: Correlate data from Application Insights to other datasets, such as data enrichment or lookup tables, non-Application Insights data sources, and custom data.\n ms.topic: conceptual\n author: eternovsky\n ms.author: evternov\n@@ -11,7 +11,7 @@ ms.reviewer: mbullwin\n \n # Correlating Application Insights data with custom data sources\n \n-Application Insights collects several different data types: exceptions, traces, page views, and others. While this is often sufficient to investigate your application’s performance, reliability, and usage, there are cases when it is useful to correlate the data stored in Application Insights to other completely custom datasets.\n+Application Insights collects several different data types: exceptions, traces, page views, and others. While this is often sufficient to investigate your application's performance, reliability, and usage, there are cases when it is useful to correlate the data stored in Application Insights to other completely custom datasets.\n \n Some situations where you might want custom data include:\n \n@@ -21,17 +21,17 @@ Some situations where you might want custom data include:\n \n ## How to correlate custom data with Application Insights data \n \n-Since Application Insights is backed by the powerful Azure Monitor log platform, we are able to use the full power of Azure Monitor to ingest the data. Then, we will write queries using the “join” operator that will correlate this custom data with the data available to us in Azure Monitor logs. \n+Since Application Insights is backed by the powerful Azure Monitor log platform, we are able to use the full power of Azure Monitor to ingest the data. Then, we will write queries using the \"join\" operator that will correlate this custom data with the data available to us in Azure Monitor logs. \n \n ## Ingesting data\n \n In this section, we will review how to get your data into Azure Monitor logs.\n \n-If you don’t already have one, provision a new Log Analytics workspace by following [these instructions](../learn/quick-collect-azurevm.md) through and including the “create a workspace” step.\n+If you don't already have one, provision a new Log Analytics workspace by following [these instructions](../learn/quick-collect-azurevm.md) through and including the \"create a workspace\" step.\n \n To start sending log data into Azure Monitor. Several options exist:\n \n-- For a synchronous mechanism, you can either directly call the [data collector API](https://docs.microsoft.com/azure/log-analytics/log-analytics-data-collector-api) or use our Logic App connector – simply look for “Azure Log Analytics” and pick the “Send Data” option:\n+- For a synchronous mechanism, you can either directly call the [data collector API](https://docs.microsoft.com/azure/log-analytics/log-analytics-data-collector-api) or use our Logic App connector – simply look for \"Azure Log Analytics\" and pick the \"Send Data\" option:\n \n   ![Screenshot choose and action](./media/custom-data-correlation/01-logic-app-connector.png)  \n \n@@ -41,7 +41,7 @@ To start sending log data into Azure Monitor. Several options exist:\n \n Application Insights is based on the Azure Monitor log platform. We can therefore use [cross-resource joins](https://docs.microsoft.com/azure/log-analytics/log-analytics-cross-workspace-search) to correlate any data we ingested into Azure Monitor with our Application Insights data.\n \n-For example, we can ingest our lab inventory and locations into a table called “LabLocations_CL” in a Log Analytics workspace called “myLA”. If we then wanted to review our requests tracked in Application Insights app called “myAI” and correlate the machine names that served the requests to the locations of these machines stored in the previously mentioned custom table, we can run the following query from either the Application Insights or Azure Monitor context:\n+For example, we can ingest our lab inventory and locations into a table called \"LabLocations_CL\" in a Log Analytics workspace called \"myLA\". If we then wanted to review our requests tracked in Application Insights app called \"myAI\" and correlate the machine names that served the requests to the locations of these machines stored in the previously mentioned custom table, we can run the following query from either the Application Insights or Azure Monitor context:\n \n ```\n app('myAI').requests"
  },
  {
    "Number": 106819,
    "Title": "Azure Monitor activity log export links",
    "ClosedAt": "2020-03-11T14:08:44Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/platform/data-sources.md",
    "Addition": 3,
    "Delections": 3,
    "Changes": 6,
    "Patch": "@@ -69,8 +69,8 @@ The [Azure Activity log](platform-logs-overview.md) includes service health reco\n |:---|:---|\n | Activity log | The Activity log is collected into its own data store that you can view from the Azure Monitor menu or use to create Activity log alerts. | [Query the Activity log in the Azure portal](activity-log-view.md#azure-portal) |\n | Azure Monitor Logs | Configure Azure Monitor Logs to collect the Activity log to analyze it with other monitoring data. | [Collect and analyze Azure activity logs in Log Analytics workspace in Azure Monitor](activity-log-collect.md) |\n-| Azure Storage | Export the Activity log to Azure Storage for archiving. | [Archive Activity log](activity-log-export.md#archive-activity-log)  |\n-| Event Hubs | Stream the Activity log to other locations using Event Hubs | [Stream Activity log to Event Hub](activity-log-export.md#stream-activity-log-to-event-hub). |\n+| Azure Storage | Export the Activity log to Azure Storage for archiving. | [Archive Activity log](resource-logs-collect-storage.md)  |\n+| Event Hubs | Stream the Activity log to other locations using Event Hubs | [Stream Activity log to Event Hub](resource-logs-stream-event-hubs.md). |\n \n ### Azure Service Health\n [Azure Service Health](../../service-health/service-health-overview.md) provides information about the health of the Azure services in your subscription that your application and resources rely on.\n@@ -98,7 +98,7 @@ Most Azure services will send [platform metrics](data-platform-metrics.md) that\n ### Resource logs\n [Resource logs](platform-logs-overview.md) provide insights into the _internal_ operation of an Azure resource.  Resource logs are created automatically, but you must create a diagnostic setting to specify a destination for them to collected for each resource.\n \n-The configuration requirements and content of resource logs vary by resource type, and not all services yet create them. See [Supported services, schemas, and categories for Azure resource logs](diagnostic-logs-schema.md) for details on each service and links to detailed configuration procedures. If the service isn’t listed in this article, then that service doesn’t currently create resource logs.\n+The configuration requirements and content of resource logs vary by resource type, and not all services yet create them. See [Supported services, schemas, and categories for Azure resource logs](diagnostic-logs-schema.md) for details on each service and links to detailed configuration procedures. If the service isn't listed in this article, then that service doesn't currently create resource logs.\n \n | Destination | Description | Reference |\n |:---|:---|:---|"
  },
  {
    "Number": 106819,
    "Title": "Azure Monitor activity log export links",
    "ClosedAt": "2020-03-11T14:08:44Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/platform/diagnostic-settings-legacy.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Collect Azure Activity log with diagnostic settings (preview) - Azure Monitor | Microsoft Docs\n+title: Collect Azure Activity log with diagnostic settings - Azure Monitor | Microsoft Docs\n description: Use diagnostic settings to forward Azure Activity logs to Azure Monitor Logs, Azure storage, or Azure Event Hubs.\n author: bwren\n ms.service: azure-monitor"
  },
  {
    "Number": 106819,
    "Title": "Azure Monitor activity log export links",
    "ClosedAt": "2020-03-11T14:08:44Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/platform/resource-logs-blob-format.md",
    "Addition": 54,
    "Delections": 54,
    "Changes": 108,
    "Patch": "@@ -12,7 +12,7 @@ ms.subservice: logs\n # Prepare for format change to Azure Monitor platform logs archived to a storage account\n \n > [!WARNING]\n-> If you are sending [Azure resource logs or metrics to a storage account using diagnostic settings](resource-logs-collect-storage.md) or [activity logs to a storage account using log profiles](activity-log-export.md), the format of the data in the storage account changed to JSON Lines on Nov. 1, 2018. The instructions below describe the impact and how to update your tooling to handle the new format.\n+> If you are sending [Azure resource logs or metrics to a storage account using diagnostic settings](resource-logs-collect-storage.md) or [activity logs to a storage account using log profiles](resource-logs-collect-storage.md), the format of the data in the storage account changed to JSON Lines on Nov. 1, 2018. The instructions below describe the impact and how to update your tooling to handle the new format.\n >\n \n ## What changed\n@@ -54,60 +54,60 @@ The current format of the PT1H.json file in Azure blob storage uses a JSON array\n \n ```json\n {\n-\t\"records\": [\n-\t\t{\n-\t\t\t\"time\": \"2016-01-05T01:32:01.2691226Z\",\n-\t\t\t\"resourceId\": \"/SUBSCRIPTIONS/361DA5D4-A47A-4C79-AFDD-XXXXXXXXXXXX/RESOURCEGROUPS/CONTOSOGROUP/PROVIDERS/MICROSOFT.KEYVAULT/VAULTS/CONTOSOKEYVAULT\",\n-\t\t\t\"operationName\": \"VaultGet\",\n-\t\t\t\"operationVersion\": \"2015-06-01\",\n-\t\t\t\"category\": \"AuditEvent\",\n-\t\t\t\"resultType\": \"Success\",\n-\t\t\t\"resultSignature\": \"OK\",\n-\t\t\t\"resultDescription\": \"\",\n-\t\t\t\"durationMs\": \"78\",\n-\t\t\t\"callerIpAddress\": \"104.40.82.76\",\n-\t\t\t\"correlationId\": \"\",\n-\t\t\t\"identity\": {\n-\t\t\t\t\"claim\": {\n-\t\t\t\t\t\"http://schemas.microsoft.com/identity/claims/objectidentifier\": \"d9da5048-2737-4770-bd64-XXXXXXXXXXXX\",\n-\t\t\t\t\t\"http://schemas.xmlsoap.org/ws/2005/05/identity/claims/upn\": \"live.com#username@outlook.com\",\n-\t\t\t\t\t\"appid\": \"1950a258-227b-4e31-a9cf-XXXXXXXXXXXX\"\n-\t\t\t\t}\n-\t\t\t},\n-\t\t\t\"properties\": {\n-\t\t\t\t\"clientInfo\": \"azure-resource-manager/2.0\",\n-\t\t\t\t\"requestUri\": \"https://control-prod-wus.vaultcore.azure.net/subscriptions/361da5d4-a47a-4c79-afdd-XXXXXXXXXXXX/resourcegroups/contosoresourcegroup/providers/Microsoft.KeyVault/vaults/contosokeyvault?api-version=2015-06-01\",\n-\t\t\t\t\"id\": \"https://contosokeyvault.vault.azure.net/\",\n-\t\t\t\t\"httpStatusCode\": 200\n-\t\t\t}\n-\t\t},\n+    \"records\": [\n         {\n-\t\t\t\"time\": \"2016-01-05T01:33:56.5264523Z\",\n-\t\t\t\"resourceId\": \"/SUBSCRIPTIONS/361DA5D4-A47A-4C79-AFDD-XXXXXXXXXXXX/RESOURCEGROUPS/CONTOSOGROUP/PROVIDERS/MICROSOFT.KEYVAULT/VAULTS/CONTOSOKEYVAULT\",\n-\t\t\t\"operationName\": \"VaultGet\",\n-\t\t\t\"operationVersion\": \"2015-06-01\",\n-\t\t\t\"category\": \"AuditEvent\",\n-\t\t\t\"resultType\": \"Success\",\n-\t\t\t\"resultSignature\": \"OK\",\n-\t\t\t\"resultDescription\": \"\",\n-\t\t\t\"durationMs\": \"83\",\n-\t\t\t\"callerIpAddress\": \"104.40.82.76\",\n-\t\t\t\"correlationId\": \"\",\n-\t\t\t\"identity\": {\n-\t\t\t\t\"claim\": {\n-\t\t\t\t\t\"http://schemas.microsoft.com/identity/claims/objectidentifier\": \"d9da5048-2737-4770-bd64-XXXXXXXXXXXX\",\n-\t\t\t\t\t\"http://schemas.xmlsoap.org/ws/2005/05/identity/claims/upn\": \"live.com#username@outlook.com\",\n-\t\t\t\t\t\"appid\": \"1950a258-227b-4e31-a9cf-XXXXXXXXXXXX\"\n-\t\t\t\t}\n-\t\t\t},\n-\t\t\t\"properties\": {\n-\t\t\t\t\"clientInfo\": \"azure-resource-manager/2.0\",\n-\t\t\t\t\"requestUri\": \"https://control-prod-wus.vaultcore.azure.net/subscriptions/361da5d4-a47a-4c79-afdd-XXXXXXXXXXXX/resourcegroups/contosoresourcegroup/providers/Microsoft.KeyVault/vaults/contosokeyvault?api-version=2015-06-01\",\n-\t\t\t\t\"id\": \"https://contosokeyvault.vault.azure.net/\",\n-\t\t\t\t\"httpStatusCode\": 200\n-\t\t\t}\n-\t\t}\n-\t]\n+            \"time\": \"2016-01-05T01:32:01.2691226Z\",\n+            \"resourceId\": \"/SUBSCRIPTIONS/361DA5D4-A47A-4C79-AFDD-XXXXXXXXXXXX/RESOURCEGROUPS/CONTOSOGROUP/PROVIDERS/MICROSOFT.KEYVAULT/VAULTS/CONTOSOKEYVAULT\",\n+            \"operationName\": \"VaultGet\",\n+            \"operationVersion\": \"2015-06-01\",\n+            \"category\": \"AuditEvent\",\n+            \"resultType\": \"Success\",\n+            \"resultSignature\": \"OK\",\n+            \"resultDescription\": \"\",\n+            \"durationMs\": \"78\",\n+            \"callerIpAddress\": \"104.40.82.76\",\n+            \"correlationId\": \"\",\n+            \"identity\": {\n+                \"claim\": {\n+                    \"http://schemas.microsoft.com/identity/claims/objectidentifier\": \"d9da5048-2737-4770-bd64-XXXXXXXXXXXX\",\n+                    \"http://schemas.xmlsoap.org/ws/2005/05/identity/claims/upn\": \"live.com#username@outlook.com\",\n+                    \"appid\": \"1950a258-227b-4e31-a9cf-XXXXXXXXXXXX\"\n+                }\n+            },\n+            \"properties\": {\n+                \"clientInfo\": \"azure-resource-manager/2.0\",\n+                \"requestUri\": \"https://control-prod-wus.vaultcore.azure.net/subscriptions/361da5d4-a47a-4c79-afdd-XXXXXXXXXXXX/resourcegroups/contosoresourcegroup/providers/Microsoft.KeyVault/vaults/contosokeyvault?api-version=2015-06-01\",\n+                \"id\": \"https://contosokeyvault.vault.azure.net/\",\n+                \"httpStatusCode\": 200\n+            }\n+        },\n+        {\n+            \"time\": \"2016-01-05T01:33:56.5264523Z\",\n+            \"resourceId\": \"/SUBSCRIPTIONS/361DA5D4-A47A-4C79-AFDD-XXXXXXXXXXXX/RESOURCEGROUPS/CONTOSOGROUP/PROVIDERS/MICROSOFT.KEYVAULT/VAULTS/CONTOSOKEYVAULT\",\n+            \"operationName\": \"VaultGet\",\n+            \"operationVersion\": \"2015-06-01\",\n+            \"category\": \"AuditEvent\",\n+            \"resultType\": \"Success\",\n+            \"resultSignature\": \"OK\",\n+            \"resultDescription\": \"\",\n+            \"durationMs\": \"83\",\n+            \"callerIpAddress\": \"104.40.82.76\",\n+            \"correlationId\": \"\",\n+            \"identity\": {\n+                \"claim\": {\n+                    \"http://schemas.microsoft.com/identity/claims/objectidentifier\": \"d9da5048-2737-4770-bd64-XXXXXXXXXXXX\",\n+                    \"http://schemas.xmlsoap.org/ws/2005/05/identity/claims/upn\": \"live.com#username@outlook.com\",\n+                    \"appid\": \"1950a258-227b-4e31-a9cf-XXXXXXXXXXXX\"\n+                }\n+            },\n+            \"properties\": {\n+                \"clientInfo\": \"azure-resource-manager/2.0\",\n+                \"requestUri\": \"https://control-prod-wus.vaultcore.azure.net/subscriptions/361da5d4-a47a-4c79-afdd-XXXXXXXXXXXX/resourcegroups/contosoresourcegroup/providers/Microsoft.KeyVault/vaults/contosokeyvault?api-version=2015-06-01\",\n+                \"id\": \"https://contosokeyvault.vault.azure.net/\",\n+                \"httpStatusCode\": 200\n+            }\n+        }\n+    ]\n }\n ```\n "
  },
  {
    "Number": 106819,
    "Title": "Azure Monitor activity log export links",
    "ClosedAt": "2020-03-11T14:08:44Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/platform/stream-monitoring-data-event-hubs.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -30,7 +30,7 @@ Before you configure streaming for any data source, you need to [create an Event\n | Tier | Data | Method |\n |:---|:---|:---|\n | [Azure tenant](data-sources.md#azure-tenant) | Azure Active Directory audit logs | Configure a tenant diagnostic setting on your AAD tenant. See  [Tutorial: Stream Azure Active Directory logs to an Azure event hub](../../active-directory/reports-monitoring/tutorial-azure-monitor-stream-logs-to-event-hub.md) for details. |\n-| [Azure subscription](data-sources.md#azure-subscription) | Azure Activity Log | Create a log profile to export Activity Log events to Event Hubs.  See [Export Azure Activity log to storage or Azure Event Hubs](activity-log-export.md) for details. |\n+| [Azure subscription](data-sources.md#azure-subscription) | Azure Activity Log | Create a log profile to export Activity Log events to Event Hubs.  See [Stream Azure platform logs to Azure Event Hubs](resource-logs-stream-event-hubs.md) for details. |\n | [Azure resources](data-sources.md#azure-resources) | Platform metrics<br> Resource logs |Both types of data are sent to an event hub using a resource diagnostic setting. See [Stream Azure resource  logs to an event hub](resource-logs-stream-event-hubs.md) for details. |\n | [Operating system (guest)](data-sources.md#operating-system-guest) | Azure virtual machines | Install the [Azure Diagnostics Extension](diagnostics-extension-overview.md) on Windows and Linux virtual machines in Azure. See [Streaming Azure Diagnostics data in the hot path by using Event Hubs](diagnostics-extension-stream-event-hubs.md) for details on Windows VMs and [Use Linux Diagnostic Extension to monitor metrics and logs](../../virtual-machines/extensions/diagnostics-linux.md#protected-settings) for details on Linux VMs. |\n | [Application code](data-sources.md#application-code) | Application Insights | Application Insights doesn't provide a direct method to stream data to event hubs. You can [set up continuous export](../../azure-monitor/app/export-telemetry.md) of the Application Insights data to a storage account and then use a Logic App to send the data to an event hub as described in [Manual streaming with Logic App](#manual-streaming-with-logic-app). |"
  },
  {
    "Number": 107107,
    "Title": "update for new video",
    "ClosedAt": "2020-03-10T20:14:02Z",
    "User": "rboucher",
    "FileName": "articles/azure-monitor/learn/tutorial-metrics-explorer.md",
    "Addition": 6,
    "Delections": 4,
    "Changes": 10,
    "Patch": "@@ -2,12 +2,10 @@\n title: Tutorial - Create a metrics chart in Azure Monitor\n description: Learn how to create your first metric chart with Azure metrics explorer.\n author: bwren\n-services: azure-monitor\n-\n+ms.author: bwren\n ms.subservice: metrics\n ms.topic: tutorial\n-ms.date: 12/16/2019\n-ms.author: bwren\n+ms.date: 03/09/2020\n ---\n \n # Tutorial: Create a metrics chart in Azure Monitor\n@@ -20,6 +18,10 @@ In this tutorial, you learn how to:\n > * Perform different aggregations of metric values\n > * Modify the time range and granularity for the chart\n \n+Following is a video that shows a more extensive scenario than the procedure outlined in this article. If you are new to metrics, we suggest you read through this article first and then view the video to see more specifics. \n+\n+> [!VIDEO https://www.microsoft.com/videoplayer/embed/RE4qO59]\n+\n ## Prerequisites\n \n To complete this tutorial you need an Azure resource to monitor. You can use any resource in your Azure subscription that supports metrics. To determine whether a resource supports metrics, go to its menu in the Azure portal and verify that there's a **Metrics** option in the **Monitoring** section of the menu."
  },
  {
    "Number": 107135,
    "Title": "update js snippet",
    "ClosedAt": "2020-03-10T18:09:33Z",
    "User": "xiao-lix",
    "FileName": "articles/azure-monitor/app/javascript.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -47,11 +47,11 @@ If your app does not use npm, you can directly instrument your webpages with App\n \n ```html\n <script type=\"text/javascript\">\n-var sdkInstance=\"appInsightsSDK\";window[sdkInstance]=\"appInsights\";var aiName=window[sdkInstance],aisdk=window[aiName]||function(e){function n(e){t[e]=function(){var n=arguments;t.queue.push(function(){t[e].apply(t,n)})}}var t={config:e};t.initialize=!0;var i=document,a=window;setTimeout(function(){var n=i.createElement(\"script\");n.src=e.url||\"https://az416426.vo.msecnd.net/scripts/b/ai.2.min.js\",i.getElementsByTagName(\"script\")[0].parentNode.appendChild(n)});try{t.cookie=i.cookie}catch(e){}t.queue=[],t.version=2;for(var r=[\"Event\",\"PageView\",\"Exception\",\"Trace\",\"DependencyData\",\"Metric\",\"PageViewPerformance\"];r.length;)n(\"track\"+r.pop());n(\"startTrackPage\"),n(\"stopTrackPage\");var s=\"Track\"+r[0];if(n(\"start\"+s),n(\"stop\"+s),n(\"addTelemetryInitializer\"),n(\"setAuthenticatedUserContext\"),n(\"clearAuthenticatedUserContext\"),n(\"flush\"),t.SeverityLevel={Verbose:0,Information:1,Warning:2,Error:3,Critical:4},!(!0===e.disableExceptionTracking||e.extensionConfig&&e.extensionConfig.ApplicationInsightsAnalytics&&!0===e.extensionConfig.ApplicationInsightsAnalytics.disableExceptionTracking)){n(\"_\"+(r=\"onerror\"));var o=a[r];a[r]=function(e,n,i,a,s){var c=o&&o(e,n,i,a,s);return!0!==c&&t[\"_\"+r]({message:e,url:n,lineNumber:i,columnNumber:a,error:s}),c},e.autoExceptionInstrumented=!0}return t}(\n+var sdkInstance=\"appInsightsSDK\";window[sdkInstance]=\"appInsights\";var aiName=window[sdkInstance],aisdk=window[aiName]||function(n){var o={config:n,initialize:!0},t=document,e=window,i=\"script\";setTimeout(function(){var e=t.createElement(i);e.src=n.url||\"https://az416426.vo.msecnd.net/scripts/b/ai.2.min.js\",t.getElementsByTagName(i)[0].parentNode.appendChild(e)});try{o.cookie=t.cookie}catch(e){}function a(n){o[n]=function(){var e=arguments;o.queue.push(function(){o[n].apply(o,e)})}}o.queue=[],o.version=2;for(var s=[\"Event\",\"PageView\",\"Exception\",\"Trace\",\"DependencyData\",\"Metric\",\"PageViewPerformance\"];s.length;)a(\"track\"+s.pop());var r=\"Track\",c=r+\"Page\";a(\"start\"+c),a(\"stop\"+c);var u=r+\"Event\";if(a(\"start\"+u),a(\"stop\"+u),a(\"addTelemetryInitializer\"),a(\"setAuthenticatedUserContext\"),a(\"clearAuthenticatedUserContext\"),a(\"flush\"),o.SeverityLevel={Verbose:0,Information:1,Warning:2,Error:3,Critical:4},!(!0===n.disableExceptionTracking||n.extensionConfig&&n.extensionConfig.ApplicationInsightsAnalytics&&!0===n.extensionConfig.ApplicationInsightsAnalytics.disableExceptionTracking)){a(\"_\"+(s=\"onerror\"));var p=e[s];e[s]=function(e,n,t,i,a){var r=p&&p(e,n,t,i,a);return!0!==r&&o[\"_\"+s]({message:e,url:n,lineNumber:t,columnNumber:i,error:a}),r},n.autoExceptionInstrumented=!0}return o}(\n {\n   instrumentationKey:\"INSTRUMENTATION_KEY\"\n }\n-);window[aiName]=aisdk,aisdk.queue&&0===aisdk.queue.length&&aisdk.trackPageView({});\n+);(window[aiName]=aisdk).queue&&0===aisdk.queue.length&&aisdk.trackPageView({});\n </script>\n ```\n "
  },
  {
    "Number": 107067,
    "Title": "Update per feedback in Github issue 49688",
    "ClosedAt": "2020-03-10T15:57:38Z",
    "User": "rboucher",
    "FileName": "articles/azure-monitor/platform/alerts-enable-template.md",
    "Addition": 4,
    "Delections": 5,
    "Changes": 9,
    "Patch": "@@ -4,17 +4,16 @@ description: Learn how to use a Resource Manager template to create a classic me\n author: rboucher\n ms.author: robb\n ms.topic: conceptual\n-ms.date: 4/27/2018\n+ms.date: 03/09/2020\n ms.subservice: alerts\n ---\n # Create a classic metric alert with a Resource Manager template\n-This article shows how you can use an [Azure Resource Manager template](../../azure-resource-manager/templates/template-syntax.md) to configure Azure metric alerts. This enables you to automatically set up alerts on your resources when they are created to ensure that all resources are monitored correctly.\n-\n-> [!NOTE]\n+> [!WARNING]\n > \n-> This article describes creating **classic metric alerts** using Resource Manager templates. If you are looking for creating [newer metric alerts](../../azure-monitor/platform/alerts-metric-near-real-time.md) using templates, [this article](alerts-metric-create-templates.md) provides the details.\n+> This article describes creating **classic metric alerts** using Resource Manager templates. Classic alerts were retired in August 2019 and set to be fully deprecated in June 2020. You cannot create new classic alerts public Azure. Some regional versions of Azure may still have the option, but we suggest you instead create [newer metric alerts](../../azure-monitor/platform/alerts-metric-near-real-time.md) using templates if at all possible. [This article](alerts-metric-create-templates.md) provides the details.\n >\n \n+This article shows how you can use an [Azure Resource Manager template](../../azure-resource-manager/templates/template-syntax.md) to configure Azure classic metric alerts. This enables you to automatically set up alerts on your resources when they are created to ensure that all resources are monitored correctly.\n \n The basic steps are as follows:\n "
  },
  {
    "Number": 107064,
    "Title": "App Insights Pricing update from Dale part 2",
    "ClosedAt": "2020-03-10T07:02:11Z",
    "User": "lgayhardt",
    "FileName": "articles/azure-monitor/app/pricing.md",
    "Addition": 8,
    "Delections": 1,
    "Changes": 9,
    "Patch": "@@ -121,7 +121,6 @@ dependencies\n | render barchart  \n ```\n \n-\n ## Viewing Application Insights usage on your Azure bill\n \n Azure provides a great deal of useful functionality in the [Azure Cost Management + Billing](https://docs.microsoft.com/azure/cost-management/quick-acm-cost-analysis?toc=/azure/billing/TOC.json) hub. For instance, the \"Cost analysis\" functionality enables you to view your spends for Azure resources. Adding a filter by resource type (to microsoft.insights/components for Application Insights) will allow you to track your spending.\n@@ -171,6 +170,14 @@ To change the daily cap, in the **Configure** section of your Application Insigh\n \n To [change the daily cap via Azure Resource Manager](../../azure-monitor/app/powershell.md), the property to change is the `dailyQuota`.  Via Azure Resource Manager you can also set the `dailyQuotaResetTime` and the daily cap's `warningThreshold`.\n \n+### Create alerts for the Daily Cap\n+\n+The Application Insights Daily Cap creates an event in the Azure activity kog when the ingested data volumes hits the warning level or the daily cap level.  You can [create an alert based on these activity log events](https://docs.microsoft.com/azure/azure-monitor/platform/alerts-activity-log#create-with-the-azure-portal). The signal names for these events are:\n+\n+* Application Insights component daily cap warning threshold reached\n+\n+* Application Insights component daily cap reached\n+\n ## Sampling\n [Sampling](../../azure-monitor/app/sampling.md) is a method of reducing the rate at which telemetry is sent to your app, while retaining the ability to find related events during diagnostic searches. You also retain correct event counts.\n "
  },
  {
    "Number": 106590,
    "Title": "App Insights Source Map for JS",
    "ClosedAt": "2020-03-09T22:05:06Z",
    "User": "lgayhardt",
    "FileName": "articles/azure-monitor/app/javascript.md",
    "Addition": 23,
    "Delections": 15,
    "Changes": 38,
    "Patch": "@@ -60,17 +60,17 @@ var sdkInstance=\"appInsightsSDK\";window[sdkInstance]=\"appInsights\";var aiName=wi\n By default the Application Insights JavaScript SDK autocollects a number of telemetry items that are helpful in determining the health of your application and the underlying user experience. These include:\n \n - **Uncaught exceptions** in your app, including information on\n-\t- Stack trace\n-\t- Exception details and message accompanying the error\n-\t- Line & column number of error\n-\t- URL where error was raised\n+    - Stack trace\n+    - Exception details and message accompanying the error\n+    - Line & column number of error\n+    - URL where error was raised\n - **Network Dependency Requests** made by your app **XHR** and **Fetch** (fetch collection is disabled by default) requests, include information on\n-\t- Url of dependency source\n-\t- Command & Method used to request the dependency\n-\t- Duration of the request\n-\t- Result code and success status of the request\n-\t- ID (if any) of user making the request\n-\t- Correlation context (if any) where request is made\n+    - Url of dependency source\n+    - Command & Method used to request the dependency\n+    - Duration of the request\n+    - Result code and success status of the request\n+    - ID (if any) of user making the request\n+    - Correlation context (if any) where request is made\n - **User information** (for example, Location, network, IP)\n - **Device information** (for example, Browser, OS, version, language, resolution, model)\n - **Session information**\n@@ -91,6 +91,7 @@ appInsights.trackTrace({message: 'This message will use a telemetry initializer'\n appInsights.addTelemetryInitializer(() => false); // Nothing is sent after this is executed\n appInsights.trackTrace({message: 'this message will not be sent'}); // Not sent\n ```\n+\n ## Configuration\n Most configuration fields are named such that they can be defaulted to false. All fields are optional except for `instrumentationKey`.\n \n@@ -151,7 +152,7 @@ Currently, we offer a separate [React plugin](#react-extensions) which you can i\n \n ## Explore browser/client-side data\n \n-Browser/client-side data can be viewed by going to **Metrics** and adding individual metrics you are interested in: \n+Browser/client-side data can be viewed by going to **Metrics** and adding individual metrics you are interested in:\n \n ![](./media/javascript/page-view-load-time.png)\n \n@@ -161,15 +162,15 @@ Select **Browser** and then choose **Failures** or **Performance**.\n \n ![](./media/javascript/browser.png)\n \n-### Performance \n+### Performance\n \n ![](./media/javascript/performance-operations.png)\n \n ### Dependencies\n \n ![](./media/javascript/performance-dependencies.png)\n \n-### Analytics \n+### Analytics\n \n To query your telemetry collected by the JavaScript SDK, select the **View in Logs (Analytics)** button. By adding a `where` statement of `client_Type == \"Browser\"`, you will only see data from the JavaScript SDK and any server-side telemetry collected by other SDKs will be excluded.\n  \n@@ -190,7 +191,14 @@ dataset\n \n ### Source Map Support\n \n-The minified callstack of your exception telemetry can be unminified in the Azure portal. All existing integrations on the Exception Details panel will work with the newly unminified callstack. Drag and drop source map unminifying supports all existing and future JS SDKs (+Node.JS), so you do not need to upgrade your SDK version. To view your unminified callstack,\n+The minified callstack of your exception telemetry can be unminified in the Azure portal. All existing integrations on the Exception Details panel will work with the newly unminified callstack.\n+\n+#### Link to Blob storage account\n+\n+You can link your Application Insights resource to your own Azure Blob Storage container to automatically unminify call stacks. To get started, see [automatic source map support](./source-map-support.md).\n+\n+### Drag and drop\n+\n 1. Select an Exception Telemetry item in the Azure portal to view its \"End-to-end transaction details\"\n 2. Identify which source maps correspond to this call stack. The source map must match a stack frame's source file, but suffixed with `.map`\n 3. Drag and drop the source maps onto the call stack in the Azure portal\n@@ -208,7 +216,7 @@ This version comes with the bare minimum number of features and functionalities\n \n For runnable examples, see [Application Insights JavaScript SDK Samples](https://github.com/topics/applicationinsights-js-demo)\n \n-## Upgrading from the old Version of Application Insights\n+## Upgrading from the old version of Application Insights\n \n Breaking changes in the SDK V2 version:\n - To allow for better API signatures, some of the API calls, such as trackPageView and trackException, have been updated. Running in Internet Explorer 8 and earlier versions of the browser is not supported."
  },
  {
    "Number": 106590,
    "Title": "App Insights Source Map for JS",
    "ClosedAt": "2020-03-09T22:05:06Z",
    "User": "lgayhardt",
    "FileName": "articles/azure-monitor/app/source-map-support.md",
    "Addition": 88,
    "Delections": 0,
    "Changes": 88,
    "Patch": "@@ -0,0 +1,88 @@\n+---\n+title: Source map support for JavaScript applications - Azure Monitor Application Insights\n+description: Learn how to upload source maps to your own storage account Blob container using Application Insights.\n+ms.topic: conceptual\n+author: markwolff\n+ms.author: marwolff\n+ms.date: 03/04/2020\n+\n+---\n+\n+# Source map support for JavaScript applications\n+\n+Application Insights supports the uploading of source maps to your own Storage Account Blob Container.\n+Source maps can be used to unminify call stacks found on the end to end transaction details page. Any exception sent by the [JavaScript SDK][ApplicationInsights-JS] or the [Node.js SDK][ApplicationInsights-Node.js] can be unminified with source maps.\n+\n+![Unminify a Call Stack by linking with a Storage Account](./media/source-map-support/details-unminify.gif)\n+\n+## Create a new storage account and Blob container\n+\n+If you already have an existing storage account or blob container, you can skip this step.\n+\n+1. [Create a new storage account][create storage account]\n+2. [Create a blob container][create blob container] inside your storage account. Be sure to set the \"Public access level\" to `Private`, to ensure that your source maps are not publicly accessible.\n+\n+> [!div class=\"mx-imgBorder\"]\n+>![Your container access level must be set to Private](./media/source-map-support/container-access-level.png)\n+\n+## Push your source maps to your Blob container\n+\n+You should integrate your continuous deployment pipeline with your storage account by configuring it to automatically upload your source maps to the configured Blob container. You should not upload your source maps to a subfolder in the Blob container; currently the source map will only be fetched from the root folder.\n+\n+### Upload source maps via Azure Pipelines (recommended)\n+\n+If you are using Azure Pipelines to continuously build and deploy your application, add an [Azure File Copy][azure file copy] task to your pipeline to automatically upload your source maps.\n+\n+> [!div class=\"mx-imgBorder\"]\n+> ![Add an Azure File Copy task to your Pipeline to upload your source maps to Azure Blob Storage](./media/source-map-support/azure-file-copy.png)\n+\n+## Configure your Application Insights resource with a Source Map storage account\n+\n+### From the end-to-end transaction details page\n+\n+From the end-to-end transaction details tab, you can click on *Unminify* and it will display a prompt to configure if your resource is unconfigured.\n+\n+1. In the Portal, view the details of an exception that is minified.\n+2. Click on *Unminify*\n+3. If your resource has not been configured, a message will appear, prompting you to configure.\n+\n+### From the properties page\n+\n+If you would like to configure or change the storage account or Blob container that is linked to your Application Insights Resource, you can do it by viewing the Application Insights resource's *Properties* tab.\n+\n+1. Navigate to the *Properties* tab of your Application Insights resource.\n+2. Click on *Change source map blob container*.\n+3. Select a different Blob container as your source maps container.\n+4. Click `Apply`.\n+\n+> [!div class=\"mx-imgBorder\"]\n+> ![Reconfigure your selected Azure Blob Container by navigating to the Properties Blade](./media/source-map-support/reconfigure.png)\n+\n+## Troubleshooting\n+\n+### Required role-based access control (RBAC) settings on your Blob container\n+\n+Any user on the Portal using this feature must be at least assigned as a [Storage Blob Data Reader][storage blob data reader] to your Blob container. You must assign this role to anyone else that will be using the source maps through this feature.\n+\n+> [!NOTE]\n+> Depending on how the container was created, this may not have been automatically assigned to you or your team.\n+\n+### Source map not found\n+\n+1. Verify that the corresponding source map is uploaded to the correct blob container\n+2. Verify that the source map file is named after the JavaScript file it maps to, suffixed with `.map`.\n+    - For example, `/static/js/main.4e2ca5fa.chunk.js` will search for the blob named `main.4e2ca5fa.chunk.js.map`\n+3. Check your browser's console to see if any errors are being logged. Include this in any support ticket.\n+\n+## Next Steps\n+\n+* [Azure File Copy task](https://docs.microsoft.com/azure/devops/pipelines/tasks/deploy/azure-file-copy?view=azure-devops)\n+\n+\n+<!-- Remote URLs -->\n+[create storage account]: https://docs.microsoft.com/azure/storage/common/storage-account-create?toc=%2Fazure%2Fstorage%2Fblobs%2Ftoc.json&tabs=azure-portal\n+[create blob container]: https://docs.microsoft.com/azure/storage/blobs/storage-quickstart-blobs-portal\n+[storage blob data reader]: https://docs.microsoft.com/azure/role-based-access-control/built-in-roles#storage-blob-data-reader\n+[ApplicationInsights-JS]: \"https://github.com/microsoft/applicationinsights-js\"\n+[ApplicationInsights-Node.js]: \"https://github.com/microsoft/applicationinsights-node.js\"\n+[azure file copy]: \"https://aka.ms/azurefilecopyreadme\"\n\\ No newline at end of file"
  },
  {
    "Number": 107008,
    "Title": "App Insights Pricing update from Dale",
    "ClosedAt": "2020-03-09T21:35:51Z",
    "User": "lgayhardt",
    "FileName": "articles/azure-monitor/app/pricing.md",
    "Addition": 50,
    "Delections": 43,
    "Changes": 93,
    "Patch": "@@ -24,9 +24,9 @@ The pricing for [Azure Application Insights][start] is a **Pay-As-You-Go** model\n \n [Multi-step web tests](../../azure-monitor/app/availability-multistep.md) incur an additional charge. Multi-step web tests are web tests that perform a sequence of actions. There's no separate charge for *ping tests* of a single page. Telemetry from ping tests and multi-step tests is charged the same as other telemetry from your app.\n \n-## Estimating the costs to manage your application \n+## Estimating the costs to manage your application\n \n-If you're not yet using Application Insights, you can use the [Azure Monitor pricing calculator](https://azure.microsoft.com/pricing/calculator/?service=monitor) to estimate the cost of using Application Insights. Start by entering \"Azure Monitor\" in the Search box, and clicking on the resulting Azure Monitor tile. Scroll down the page to Azure Monitor, and select Application Insights from the Type dropdown.  Here you can enter the number of GB of data you expect to collect per month, so the question is how much data will Application Insights collect monitoring your application. \n+If you're not yet using Application Insights, you can use the [Azure Monitor pricing calculator](https://azure.microsoft.com/pricing/calculator/?service=monitor) to estimate the cost of using Application Insights. Start by entering \"Azure Monitor\" in the Search box, and clicking on the resulting Azure Monitor tile. Scroll down the page to Azure Monitor, and select Application Insights from the Type dropdown.  Here you can enter the number of GB of data you expect to collect per month, so the question is how much data will Application Insights collect monitoring your application.\n \n There are two approaches to address this: use of default monitoring and adaptive sampling, which is available in the ASP.NET SDK, or estimate your likely data ingestion based on what other similar customers have seen.\n \n@@ -38,25 +38,25 @@ For SDKs that don't support adaptive sampling, you can employ [ingestion samplin\n \n ### Learn from what similar customers collect\n \n-In the Azure Monitoring Pricing calculator for Application Insights, if you enable the \"Estimate data volume based on application activity\" functionality, you can provide inputs about your application (requests per month and page views per month, in case you will collect client-side telemetry), and then the calculator will tell you the median and 90th percentile amount of data collected by similar applications. These applications span the range of Application Insights configuration (e.g some have default [sampling](../../azure-monitor/app/sampling.md), some have no sampling etc.), so you still have the control to reduce the volume of data you ingest far below the median level using sampling. But this is a starting point to understand what other, similar customers are seeing. \n+In the Azure Monitoring Pricing calculator for Application Insights, if you enable the \"Estimate data volume based on application activity\" functionality, you can provide inputs about your application (requests per month and page views per month, in case you will collect client-side telemetry), and then the calculator will tell you the median and 90th percentile amount of data collected by similar applications. These applications span the range of Application Insights configuration (e.g some have default [sampling](../../azure-monitor/app/sampling.md), some have no sampling etc.), so you still have the control to reduce the volume of data you ingest far below the median level using sampling. But this is a starting point to understand what other, similar customers are seeing.\n \n ## Understand your usage and estimate costs\n \n-Application Insights makes it easy to understand what your costs are likely to be based on recent usage patterns. To get started, in the Azure portal, for the Application Insights resource, go to the **Usage and estimated costs** page: \n+Application Insights makes it easy to understand what your costs are likely to be based on recent usage patterns. To get started, in the Azure portal, for the Application Insights resource, go to the **Usage and estimated costs** page:\n \n ![Choose pricing](./media/pricing/pricing-001.png)\n \n A. Review your data volume for the month. This includes all the data that's received and retained (after any [sampling](../../azure-monitor/app/sampling.md)) from your server and client apps, and from availability tests.  \n B. A separate charge is made for [multi-step web tests](../../azure-monitor/app/availability-multistep.md). (This doesn't include simple availability tests, which are included in the data volume charge.)  \n C. View data volume trends for the past month.  \n-D. Enable data ingestion [sampling](../../azure-monitor/app/sampling.md).   \n+D. Enable data ingestion [sampling](../../azure-monitor/app/sampling.md).\n E. Set the daily data volume cap.  \n \n (Note that all prices displayed in screenshots in this article are for example purposes only. For current prices in your currency and region, see [Application Insights pricing][pricing].)\n \n-To investigate your Application Insights usage more deeply, open the **Metrics** page, add the metric named \"Data point volume\", and then select the *Apply splitting* option to split the data by \"Telemetry item type\". \n+To investigate your Application Insights usage more deeply, open the **Metrics** page, add the metric named \"Data point volume\", and then select the *Apply splitting* option to split the data by \"Telemetry item type\".\n \n-Application Insights charges are added to your Azure bill. You can see details of your Azure bill in the **Billing** section of the Azure portal, or in the [Azure billing portal](https://account.windowsazure.com/Subscriptions). \n+Application Insights charges are added to your Azure bill. You can see details of your Azure bill in the **Billing** section of the Azure portal, or in the [Azure billing portal](https://account.windowsazure.com/Subscriptions).\n \n ![In the left menu, select Billing](./media/pricing/02-billing.png)\n \n@@ -69,12 +69,16 @@ To learn more about your data volumes, selecting **Metrics** for your Applicatio\n \n ### Queries to understand data volume details\n \n+There are two approaches to investigating data volumes for Application Insights. The first uses aggregated information in the `systemEvents` table, and the second uses the `_BilledSize` property, which is available on each ingested event.\n+\n+#### Using aggregated data volume information\n+\n For instance, you can use the `systemEvents` table to see the data volume ingested in the last 24 hours with the query:\n \n ```kusto\n-systemEvents \n+systemEvents\n | where timestamp >= ago(24h)\n-| where type == \"Billing\" \n+| where type == \"Billing\"\n | extend BillingTelemetryType = tostring(dimensions[\"BillingTelemetryType\"])\n | extend BillingTelemetrySizeInBytes = todouble(measurements[\"BillingTelemetrySize\"])\n | summarize sum(BillingTelemetrySizeInBytes)\n@@ -83,45 +87,49 @@ systemEvents\n Or to see a chart of data volume (in bytes) by data type for the last 30 days, you can use:\n \n ```kusto\n-systemEvents \n+systemEvents\n | where timestamp >= startofday(ago(30d))\n-| where type == \"Billing\" \n+| where type == \"Billing\"\n | extend BillingTelemetryType = tostring(dimensions[\"BillingTelemetryType\"])\n | extend BillingTelemetrySizeInBytes = todouble(measurements[\"BillingTelemetrySize\"])\n | summarize sum(BillingTelemetrySizeInBytes) by BillingTelemetryType, bin(timestamp, 1d) | render barchart  \n ```\n \n Note that this query can be used in an [Azure Log Alert](https://docs.microsoft.com/azure/azure-monitor/platform/alerts-unified-log) to set up alerting on data volumes.  \n \n-To learn more about your telemetry data changes, let's check the count of events by type using the query:\n+To learn more about your telemetry data changes, we can get the count of events by type using the query:\n \n ```kusto\n-systemEvents \n+systemEvents\n | where timestamp >= startofday(ago(30d))\n-| where type == \"Billing\" \n+| where type == \"Billing\"\n | extend BillingTelemetryType = tostring(dimensions[\"BillingTelemetryType\"])\n-| summarize count() by BillingTelemetryType, bin(timestamp, 1d) | render barchart  \n+| summarize count() by BillingTelemetryType, bin(timestamp, 1d)\n+| render barchart  \n ```\n \n-If a similar changes are seen in the counts as is seen in the volume in bytes, then we can focus on the data types of events, which show increased counts.  For instance, if it is observed that the number of dependencies increased, here's a query to understand which operations are responsible for the increase:\n+#### Using data size per event information\n+\n+To learn more details about the source of your data volumes, you can use the `_BilledSize` property that is present on each ingested event.\n+\n+For example, to look at which operations generate the most data volume in the last 30 days, we can sum `_BilledSize` for all dependency events:\n \n ```kusto\n-dependencies \n+dependencies\n | where timestamp >= startofday(ago(30d))\n-| summarize count() by operation_Name, bin(timestamp, 1d)  \n+| summarize sum(_BilledSize) by operation_Name\n | render barchart  \n ```\n \n \n-## Viewing Application Insights usage on your Azure bill \n+## Viewing Application Insights usage on your Azure bill\n \n Azure provides a great deal of useful functionality in the [Azure Cost Management + Billing](https://docs.microsoft.com/azure/cost-management/quick-acm-cost-analysis?toc=/azure/billing/TOC.json) hub. For instance, the \"Cost analysis\" functionality enables you to view your spends for Azure resources. Adding a filter by resource type (to microsoft.insights/components for Application Insights) will allow you to track your spending.\n \n More understanding of your usage can be gained by [downloading your usage from the Azure portal](https://docs.microsoft.com/azure/billing/billing-download-azure-invoice-daily-usage-date#download-usage-in-azure-portal).\n-In the downloaded spreadsheet, you can see usage per Azure resource per day. In this Excel spreadsheet, usage from your Application Insights resources can be found by first filtering on the \"Meter Category\" column to show \"Application Insights\" and \"Log Analytics\", and then adding a filter on the \"Instance ID\" column which is \"contains microsoft.insights/components\".  Most Application Insights usage is reported on meters with the Meter Category of Log Analytics, since there is a single logs backend for all Azure Monitor components.  Only Application Insights resources on legacy pricing tiers and multi-step web tests are reported with a Meter Category of Application Insights.  The usage is shown in the \"Consumed Quantity\" column and the unit for each entry is shown in the \"Unit of Measure\" column.  More details are available to help you [understand your Microsoft Azure bill](https://docs.microsoft.com/azure/billing/billing-understand-your-bill). \n-\n+In the downloaded spreadsheet, you can see usage per Azure resource per day. In this Excel spreadsheet, usage from your Application Insights resources can be found by first filtering on the \"Meter Category\" column to show \"Application Insights\" and \"Log Analytics\", and then adding a filter on the \"Instance ID\" column which is \"contains microsoft.insights/components\".  Most Application Insights usage is reported on meters with the Meter Category of Log Analytics, since there is a single logs backend for all Azure Monitor components.  Only Application Insights resources on legacy pricing tiers and multi-step web tests are reported with a Meter Category of Application Insights.  The usage is shown in the \"Consumed Quantity\" column and the unit for each entry is shown in the \"Unit of Measure\" column.  More details are available to help you [understand your Microsoft Azure bill](https://docs.microsoft.com/azure/billing/billing-understand-your-bill).\n \n-## Managing your data volume \n+## Managing your data volume\n \n The volume of data you send can be managed using the following techniques:\n \n@@ -135,14 +143,14 @@ The volume of data you send can be managed using the following techniques:\n  \n * **Daily cap**: When you create an Application Insights resource in the Azure portal, the daily cap is set to 100 GB/day. When you create an Application Insights resource in Visual Studio, the default is small (only 32.3 MB/day). The daily cap default is set to facilitate testing. It's intended that the user will raise the daily cap before deploying the app into production. \n \n-    The maximum cap is 1,000 GB/day unless you request a higher maximum for a high-traffic application. \n-\t\n-\tWarning emails about the daily cap are sent to account that are members of these roles for your Application Insights resource: \"ServiceAdmin\", \"AccountAdmin\", \"CoAdmin\", \"Owner\".\n+    The maximum cap is 1,000 GB/day unless you request a higher maximum for a high-traffic application.\n+    \n+    Warning emails about the daily cap are sent to account that are members of these roles for your Application Insights resource: \"ServiceAdmin\", \"AccountAdmin\", \"CoAdmin\", \"Owner\".\n \n     Use care when you set the daily cap. Your intent should be to *never hit the daily cap*. If you hit the daily cap, you lose data for the remainder of the day, and you can't monitor your application. To change the daily cap, use the **Daily volume cap** option. You can access this option in the **Usage and estimated costs** pane (this is described in more detail later in the article).\n-\t\n+    \n     We've removed the restriction on some subscription types that have credit that couldn't be used for Application Insights. Previously, if the subscription has a spending limit, the daily cap dialog has instructions to remove the spending limit and enable the daily cap to be raised beyond 32.3 MB/day.\n-\t\n+    \n * **Throttling**: Throttling limits the data rate to 32,000 events per second, averaged over 1 minute per instrumentation key. The volume of data that your app sends is assessed every minute. If it exceeds the per-second rate averaged over the minute, the server refuses some requests. The SDK buffers the data and then tries to resend it. It spreads out a surge over several minutes. If your app consistently sends data at more than the throttling rate, some data will be dropped. (The ASP.NET, Java, and JavaScript SDKs try to resend data this way; other SDKs might simply drop throttled data.) If throttling occurs, a notification warning alerts you that this has occurred.\n \n ## Manage your maximum daily data volume\n@@ -153,15 +161,15 @@ Instead of using the daily volume cap, use [sampling](../../azure-monitor/app/sa\n \n ### Identify what daily data limit to define\n \n-Review Application Insights Usage and estimated costs to understand the data ingestion trend and what is the daily volume cap to define. It should be considered with care, since you won’t be able to monitor your resources after the limit is reached. \n+Review Application Insights Usage and estimated costs to understand the data ingestion trend and what is the daily volume cap to define. It should be considered with care, since you won't be able to monitor your resources after the limit is reached.\n \n ### Set the Daily Cap\n \n To change the daily cap, in the **Configure** section of your Application Insights resource, in the **Usage and estimated costs** page, select  **Daily Cap**.\n \n ![Adjust the daily telemetry volume cap](./media/pricing/pricing-003.png)\n \n-To [change the daily cap via Azure Resource Manager](../../azure-monitor/app/powershell.md), the property to change is the `dailyQuota`.  Via Azure Resource Manager you can also set the `dailyQuotaResetTime` and the daily cap's `warningThreshold`. \n+To [change the daily cap via Azure Resource Manager](../../azure-monitor/app/powershell.md), the property to change is the `dailyQuota`.  Via Azure Resource Manager you can also set the `dailyQuotaResetTime` and the daily cap's `warningThreshold`.\n \n ## Sampling\n [Sampling](../../azure-monitor/app/sampling.md) is a method of reducing the rate at which telemetry is sent to your app, while retaining the ability to find related events during diagnostic searches. You also retain correct event counts.\n@@ -187,21 +195,21 @@ To discover the actual sampling rate, no matter where it's been applied, use an\n     | summarize 100/avg(itemCount) by bin(timestamp, 1h)\n     | render areachart\n \n-In each retained record, `itemCount` indicates the number of original records that it represents. It's equal to 1 + the number of previous discarded records. \n+In each retained record, `itemCount` indicates the number of original records that it represents. It's equal to 1 + the number of previous discarded records.\n \n ## Change the data retention period\n \n-The default retention for Application Insights resources is 90 days. Different retention periods can be selected for each Application Insights resource. The full set of available retention periods is 30, 60, 90, 120, 180, 270, 365, 550 or 730 days. \n+The default retention for Application Insights resources is 90 days. Different retention periods can be selected for each Application Insights resource. The full set of available retention periods is 30, 60, 90, 120, 180, 270, 365, 550 or 730 days.\n \n To change the retention, from your Application Insights resource, go to the **Usage and Estimated Costs** page and select the **Data Retention** option:\n \n ![Adjust the daily telemetry volume cap](./media/pricing/pricing-005.png)\n \n-The retention can also be [set programatically using Powershell](powershell.md#set-the-data-retention) using the `retentionInDays` parameter. Additionally, if you set the data retention to 30 days, you can trigger an immediate purge of older data using the `immediatePurgeDataOn30Days` parameter, which may be useful for compliance-related scenarios. This purge functionality is only exposed via Azure Resource Manager and should be used with extreme care. The daily reset time for the data volume cap can be configured using Azure Resource Manager to set the `dailyQuotaResetTime` parameter. \n+The retention can also be [set programatically using PowerShell](powershell.md#set-the-data-retention) using the `retentionInDays` parameter. Additionally, if you set the data retention to 30 days, you can trigger an immediate purge of older data using the `immediatePurgeDataOn30Days` parameter, which may be useful for compliance-related scenarios. This purge functionality is only exposed via Azure Resource Manager and should be used with extreme care. The daily reset time for the data volume cap can be configured using Azure Resource Manager to set the `dailyQuotaResetTime` parameter.\n \n ## Data transfer charges using Application Insights\n \n-Sending data to Application Insights might incur data bandwidth charges. As described in the [Azure Bandwidth pricing page](https://azure.microsoft.com/pricing/details/bandwidth/), data transfer between Azure services located in two regions charged as outbound data transfer at the normal rate. Inbound data transfer is free. However, this charge is very small (few %) compared to the costs for Application Insights log data ingestion. Consequently controlling costs for Log Analytics needs to focus on your ingested data volume, and we have guidance to help understand that [here](https://docs.microsoft.com/azure/azure-monitor/app/pricing#managing-your-data-volume).   \n+Sending data to Application Insights might incur data bandwidth charges. As described in the [Azure Bandwidth pricing page](https://azure.microsoft.com/pricing/details/bandwidth/), data transfer between Azure services located in two regions charged as outbound data transfer at the normal rate. Inbound data transfer is free. However, this charge is very small (few %) compared to the costs for Application Insights log data ingestion. Consequently controlling costs for Log Analytics needs to focus on your ingested data volume, and we have guidance to help understand that [here](https://docs.microsoft.com/azure/azure-monitor/app/pricing#managing-your-data-volume).\n \n ## Limits summary\n \n@@ -213,12 +221,12 @@ To disable the daily volume cap e-mails, under the **Configure** section of your\n \n ## Legacy Enterprise (Per Node) pricing tier\n \n-For early adopters of Azure Application Insights, there are still two possible pricing tiers: Basic and Enterprise. The Basic pricing tier is the same as described above and is the default tier. It includes all Enterprise tier features, at no additional cost. The Basic tier bills primarily on the volume of data that's ingested. \n+For early adopters of Azure Application Insights, there are still two possible pricing tiers: Basic and Enterprise. The Basic pricing tier is the same as described above and is the default tier. It includes all Enterprise tier features, at no additional cost. The Basic tier bills primarily on the volume of data that's ingested.\n \n > [!NOTE]\n > These legacy pricing tiers have been renamed. The Enterprise pricing tier is now called **Per Node** and the Basic pricing tier is now called **Per GB**. These new names are used below and in the Azure portal.  \n \n-The Per Node (formerly Enterprise) tier has a per-node charge, and each node receives a daily data allowance. In the Per Node pricing tier, you are charged for data ingested above the included allowance. If you are using Operations Management Suite, you should choose the Per Node tier. \n+The Per Node (formerly Enterprise) tier has a per-node charge, and each node receives a daily data allowance. In the Per Node pricing tier, you are charged for data ingested above the included allowance. If you are using Operations Management Suite, you should choose the Per Node tier.\n \n For current prices in your currency and region, see [Application Insights pricing](https://azure.microsoft.com/pricing/details/application-insights/).\n \n@@ -227,7 +235,7 @@ For current prices in your currency and region, see [Application Insights pricin\n \n ### Per Node tier and Operations Management Suite subscription entitlements\n \n-Customers who purchase Operations Management Suite E1 and E2 can get Application Insights Per Node as an additional component at no additional cost as [previously announced](https://blogs.technet.microsoft.com/msoms/2017/05/19/azure-application-insights-enterprise-as-part-of-operations-management-suite-subscription/). Specifically, each unit of Operations Management Suite E1 and E2 includes an entitlement to one node of the Application Insights Per Node tier. Each Application Insights node includes up to 200 MB of data ingested per day (separate from Log Analytics data ingestion), with 90-day data retention at no additional cost. The tier is described in more detailed later in the article. \n+Customers who purchase Operations Management Suite E1 and E2 can get Application Insights Per Node as an additional component at no additional cost as [previously announced](https://blogs.technet.microsoft.com/msoms/2017/05/19/azure-application-insights-enterprise-as-part-of-operations-management-suite-subscription/). Specifically, each unit of Operations Management Suite E1 and E2 includes an entitlement to one node of the Application Insights Per Node tier. Each Application Insights node includes up to 200 MB of data ingested per day (separate from Log Analytics data ingestion), with 90-day data retention at no additional cost. The tier is described in more detailed later in the article.\n \n Because this tier is applicable only to customers with an Operations Management Suite subscription, customers who don't have an Operations Management Suite subscription don't see an option to select this tier.\n \n@@ -246,29 +254,28 @@ Because this tier is applicable only to customers with an Operations Management\n * A data volume allocation of 200 MB per day is given for each node that's detected (with hourly granularity). Unused data allocation isn't carried over from one day to the next.\n   * If you choose the Per Node pricing tier, each subscription gets a daily allowance of data based on the number of nodes that send telemetry to the Application Insights resources in that subscription. So, if you have five nodes that send data all day, you'll have a pooled allowance of 1 GB applied to all Application Insights resources in that subscription. It doesn't matter if certain nodes send more data than other nodes because the included data is shared across all nodes. If on a given day, the Application Insights resources receive more data than is included in the daily data allocation for this subscription, the per-GB overage data charges apply. \n   * The daily data allowance is calculated as the number of hours in the day (using UTC) that each node sends telemetry divided by 24 multiplied by 200 MB. So, if you have four nodes that send telemetry during 15 of the 24 hours in the day, the included data for that day would be ((4 &#215; 15) / 24) &#215; 200 MB = 500 MB. At the price of 2.30 USD per GB for data overage, the charge would be 1.15 USD if the nodes send 1 GB of data that day.\n-  * The Per Node tier daily allowance isn't shared with applications for which you have chosen the Per GB tier. Unused allowance isn't carried over from day-to-day. \n+  * The Per Node tier daily allowance isn't shared with applications for which you have chosen the Per GB tier. Unused allowance isn't carried over from day-to-day.\n \n ### Examples of how to determine distinct node count\n \n | Scenario                               | Total daily node count |\n |:---------------------------------------|:----------------:|\n | 1 application using 3 Azure App Service instances and 1 virtual server | 4 |\n | 3 applications running on 2 VMs; the Application Insights resources for these applications are in the same subscription and in the Per Node tier | 2 | \n-| 4 applications whose Applications Insights resources are in the same subscription; each application running 2 instances during 16 off-peak hours, and 4 instances during 8 peak hours | 13.33 | \n+| 4 applications whose Applications Insights resources are in the same subscription; each application running 2 instances during 16 off-peak hours, and 4 instances during 8 peak hours | 13.33 |\n | Cloud services with 1 Worker Role and 1 Web Role, each running 2 instances | 4 | \n | A 5-node Azure Service Fabric cluster running 50 microservices; each microservice running 3 instances | 5|\n \n * The precise node counting depends on which Application Insights SDK your application is using. \n-  * In SDK versions 2.2 and later, both the Application Insights [Core SDK](https://www.nuget.org/packages/Microsoft.ApplicationInsights/) and the [Web SDK](https://www.nuget.org/packages/Microsoft.ApplicationInsights.Web/) report each application host as a node. Examples are the computer name for physical server and VM hosts or the instance name for cloud services.  The only exception is an application that uses only the [.NET Core](https://dotnet.github.io/) and the Application Insights Core SDK. In that case, only one node is reported for all hosts because the host name isn't available. \n-  * For earlier versions of the SDK, the [Web SDK](https://www.nuget.org/packages/Microsoft.ApplicationInsights.Web/) behaves like the newer SDK versions, but the [Core SDK](https://www.nuget.org/packages/Microsoft.ApplicationInsights/) reports only one node, regardless of the number of application hosts. \n-  * If your application uses the SDK to set **roleInstance** to a custom value, by default, that same value is used to determine node count. \n-  * If you're using a new SDK version with an app that runs from client machines or mobile devices, the node count might return a number that's large (because of the large number of client machines or mobile devices). \n+  * In SDK versions 2.2 and later, both the Application Insights [Core SDK](https://www.nuget.org/packages/Microsoft.ApplicationInsights/) and the [Web SDK](https://www.nuget.org/packages/Microsoft.ApplicationInsights.Web/) report each application host as a node. Examples are the computer name for physical server and VM hosts or the instance name for cloud services.  The only exception is an application that uses only the [.NET Core](https://dotnet.github.io/) and the Application Insights Core SDK. In that case, only one node is reported for all hosts because the host name isn't available.\n+  * For earlier versions of the SDK, the [Web SDK](https://www.nuget.org/packages/Microsoft.ApplicationInsights.Web/) behaves like the newer SDK versions, but the [Core SDK](https://www.nuget.org/packages/Microsoft.ApplicationInsights/) reports only one node, regardless of the number of application hosts.\n+  * If your application uses the SDK to set **roleInstance** to a custom value, by default, that same value is used to determine node count.\n+  * If you're using a new SDK version with an app that runs from client machines or mobile devices, the node count might return a number that's large (because of the large number of client machines or mobile devices).\n \n ## Automation\n \n You can write a script to set the pricing tier by using Azure Resource Management. [Learn how](powershell.md#price).\n \n-\n ## Next steps\n \n * [Sampling](../../azure-monitor/app/sampling.md)"
  },
  {
    "Number": 107004,
    "Title": "Remove \"preview\" from OpenCensus Python",
    "ClosedAt": "2020-03-09T19:59:51Z",
    "User": "lzchen",
    "FileName": "articles/azure-monitor/app/opencensus-python.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -9,7 +9,7 @@ ms.date: 10/11/2019\n ms.reviewer: mbullwin\n ---\n \n-# Set up Azure Monitor for your Python application (preview)\n+# Set up Azure Monitor for your Python application\n \n Azure Monitor supports distributed tracing, metric collection, and logging of Python applications through integration with [OpenCensus](https://opencensus.io). This article will walk you through the process of setting up OpenCensus for Python and sending your monitoring data to Azure Monitor.\n "
  },
  {
    "Number": 101581,
    "Title": "[Azure RBAC] Updates to monitor notactions",
    "ClosedAt": "2020-01-21T17:56:19Z",
    "User": "rolyon",
    "FileName": "articles/azure-monitor/platform/manage-access.md",
    "Addition": 5,
    "Delections": 9,
    "Changes": 14,
    "Patch": "@@ -237,13 +237,12 @@ See [Defining per-table access control](#table-level-rbac) below if you want to\n \n **Table level RBAC** allows you to define more granular control to data in a Log Analytics workspace in addition to the other permissions. This control allows you to define specific data types that are accessible only to a specific set of users.\n \n-You implement table access control with [Azure custom roles](../../role-based-access-control/custom-roles.md) to either grant or deny access to specific [tables](../log-query/logs-structure.md) in the workspace. These roles are applied to workspaces with either workspace-context or resource-context [access control modes](design-logs-deployment.md#access-control-mode) regardless of the user's [access mode](design-logs-deployment.md#access-mode).\n+You implement table access control with [Azure custom roles](../../role-based-access-control/custom-roles.md) to either grant access to specific [tables](../log-query/logs-structure.md) in the workspace. These roles are applied to workspaces with either workspace-context or resource-context [access control modes](design-logs-deployment.md#access-control-mode) regardless of the user's [access mode](design-logs-deployment.md#access-mode).\n \n Create a [custom role](../../role-based-access-control/custom-roles.md) with the following actions to define access to table access control.\n \n-* To grant access to a table, include it in the **Actions** section of the role definition.\n-* To deny access to a table, include it in the **NotActions** section of the role definition.\n-* Use * to specify all tables.\n+* To grant access to a table, include it in the **Actions** section of the role definition. To subtract access from the allowed **Actions**, include it in the **NotActions** section.\n+* Use Microsoft.OperationalInsights/workspaces/query/* to specify all tables.\n \n For example, to create a role with access to the _Heartbeat_ and _AzureActivity_ tables, create a custom role using the following actions:\n \n@@ -256,24 +255,21 @@ For example, to create a role with access to the _Heartbeat_ and _AzureActivity_\n   ],\n ```\n \n-To create a role with access to only _SecurityBaseline_ and no other tables, create a custom role using the following actions:\n+To create a role with access to only the _SecurityBaseline_ table, create a custom role using the following actions:\n \n ```\n \"Actions\":  [\n     \"Microsoft.OperationalInsights/workspaces/read\",\n     \"Microsoft.OperationalInsights/workspaces/query/read\",\n     \"Microsoft.OperationalInsights/workspaces/query/SecurityBaseline/read\"\n ],\n-\"NotActions\":  [\n-    \"Microsoft.OperationalInsights/workspaces/query/*/read\"\n-],\n ```\n \n ### Custom logs\n \n  Custom logs are created from data sources such as custom logs and HTTP Data Collector API. The easiest way to identify the type of log is by checking the tables listed under [Custom Logs in the log schema](../log-query/get-started-portal.md#understand-the-schema).\n \n- You can't currently grant or deny access to individual custom logs, but you can grant or deny access to all custom logs. To create a role with access to all custom logs, create a custom role using the following actions:\n+ You can't currently grant access to individual custom logs, but you can grant access to all custom logs. To create a role with access to all custom logs, create a custom role using the following actions:\n \n ```\n \"Actions\":  ["
  },
  {
    "Number": 106817,
    "Title": "Change to use campaign links",
    "ClosedAt": "2020-03-06T21:52:34Z",
    "User": "mairaw",
    "FileName": "articles/azure-monitor/learn/quick-monitor-portal.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -23,7 +23,7 @@ analyzing live statistics, which is just one of the various methods you can use\n ## Prerequisites\n To complete this quickstart:\n \n-- Install [Visual Studio 2019](https://www.visualstudio.com/downloads/) with the following workloads:\n+- Install [Visual Studio 2019](https://visualstudio.microsoft.com/downloads/?utm_medium=microsoft&utm_source=docs.microsoft.com&utm_campaign=inline+link&utm_content=download+vs2019) with the following workloads:\n \t- ASP.NET and web development\n \t- Azure development\n "
  },
  {
    "Number": 106800,
    "Title": "update links",
    "ClosedAt": "2020-03-06T19:29:37Z",
    "User": "mairaw",
    "FileName": "articles/azure-monitor/learn/dotnetcore-quick-start.md",
    "Addition": 2,
    "Delections": 2,
    "Changes": 4,
    "Patch": "@@ -20,10 +20,10 @@ This quickstart guides you through adding the Application Insights SDK to an exi\n \n To complete this quickstart:\n \n-- [Install Visual Studio 2019](https://www.visualstudio.com/downloads/) with the following workloads:\n+- [Install Visual Studio 2019](https://visualstudio.microsoft.com/downloads/?utm_medium=microsoft&utm_source=docs.microsoft.com&utm_campaign=inline+link&utm_content=download+vs2019) with the following workloads:\n   - ASP.NET and web development\n   - Azure development\n-- [Install .NET Core 2.0 SDK](https://www.microsoft.com/net/core)\n+- [Install .NET Core 2.0 SDK](https://dotnet.microsoft.com/download)\n - You will need an Azure subscription and an existing .NET Core web application.\n \n If you don't have an ASP.NET Core web application, you can use our step-by-step guide to [create an ASP.NET Core app and add Application Insights.](../../azure-monitor/app/asp-net-core.md)"
  },
  {
    "Number": 106660,
    "Title": "Azure Monitor what's new February 2020",
    "ClosedAt": "2020-03-06T04:26:07Z",
    "User": "bwren",
    "FileName": "articles/azure-monitor/whats-new.md",
    "Addition": 63,
    "Delections": 9,
    "Changes": 72,
    "Patch": "@@ -5,13 +5,67 @@ ms.subservice:\n ms.topic: overview\n author: bwren\n ms.author: bwren\n-ms.date: 02/05/2020\n+ms.date: 03/05/2020\n \n ---\n \n # What's new in Azure Monitor documentation?\n This article provides lists Azure Monitor articles that are either new or have been significantly updated. It will be refreshed the first week of each month to include article updates from the previous month.\n \n+## March 2020\n+\n+### Agents\n+Multiple updates as part of rewrite of diagnostics extension content.\n+\n+- [Overview of the Azure monitoring agents](platform/agents-overview.md) - Restructured tables to better clarify unique features of each agent.\n+- [Azure Diagnostics extension overview](platform/diagnostics-extension-overview.md) - Complete rewrite.\n+- [Use blob storage for IIS and table storage for events in Azure Monitor](platform/diagnostics-extension-logs.md) - General rewrite for update and clarity.\n+- [Install and configure Windows Azure diagnostics extension (WAD)](platform/diagnostics-extension-windows-install.md) - New article. \n+- [Windows diagnostics extension schema](platform/diagnostics-extension-schema-windows.md) - Reorganized.\n+- [Send data from Windows Azure diagnostics extension to Azure Event Hubs](platform/diagnostics-extension-stream-event-hubs.md) - Completely rewritten and updated.\n+- [Store and view diagnostic data in Azure Storage](platform/diagnostics-extension-to-storage.md) - Completely rewritten and updated.\n+- [Log Analytics virtual machine extension for Windows](../virtual-machines/extensions/oms-windows.md) - Better clarifies relationship with Log Analytics agent.\n+- [Azure Monitor virtual machine extension for Linux](../virtual-machines/extensions/oms-linux.md) - Better clarifies relationship with Log Analytics agent.\n+\n+\n+\n+\n+### Application Insights\n+- [Connection strings in Azure Application Insights](app/sdk-connection-string.md) - New article.\n+\n+### Insights and solutions\n+\n+#### Azure Monitor for Containers\n+- [Integrate Azure Active Directory with Azure Kubernetes Service](../aks/azure-ad-integration.md) - Added note for creating a client application to support RBAC-enabled cluster to support Azure Monitor for containers.\n+\n+#### Azure Monitor for VMs\n+- [Azure Monitor for VMs (GA) frequently asked questions](insights/vminsights-ga-release-faq.md) - Change to how performance data is stored.\n+\n+#### Office 365\n+- [Office 365 management solution in Azure](insights/solution-office-365.md) - Updated deprecation date.\n+\n+\n+### Logs\n+- [Optimize log queries in Azure Monitor](log-query/query-optimization.md) - New article.\n+- [Manage usage and costs for Azure Monitor Logs](platform/manage-cost-storage.md) - Improved sample queries to help understand your usage.\n+\n+### Metrics\n+- [Azure Monitor platform metrics exportable via Diagnostic Settings](platform/metrics-supported-export-diagnostic-settings.md) - Added section on change to behavior for nulls and zero values.\n+\n+\n+### Visualizations\n+Multiple new articles for view designer to workbooks conversion guide.\n+\n+- [Azure Monitor view designer to workbooks transition guide](platform/view-designer-conversion-overview.md) - New article.\n+- [Azure Monitor view designer to workbooks conversion options](platform/view-designer-conversion-options.md) - New article.\n+- [Azure Monitor view designer to workbooks tile conversions](platform/view-designer-conversion-tiles.md) - New article.\n+- [Azure Monitor view designer to workbooks conversion summary and access](platform/view-designer-conversion-access.md) - New article.\n+- [Azure Monitor view designer to workbooks conversion common tasks](platform/view-designer-conversion-tasks.md) - New article.\n+- [Azure Monitor view designer to workbooks conversion examples](platform/view-designer-conversion-examples.md) - New article.\n+\n+\n+\n+\n ## January 2020\n \n ### General\n@@ -39,14 +93,14 @@ This article provides lists Azure Monitor articles that are either new or have b\n ### Insights and solutions\n \n #### Azure Monitor for Containers\n-- [Configure Azure Monitor for containers agent data collection](https://docs.microsoft.com/azure/azure-monitor/insights/container-insights-agent-config) - Added details for upgrading agent on Azure Red Hat OpenShift, and added additional information to distinguish the methods for upgrading agent.\n-- [Create performance alerts for Azure Monitor for containers](https://docs.microsoft.com/azure/azure-monitor/insights/container-insights-alerts) - Revised information and updated steps for creating an alert on performance data stored in workspace using workspace-context alerts.\n-- [Kubernetes monitoring with Azure Monitor for containers](https://docs.microsoft.com/azure/azure-monitor/insights/container-insights-analyze) - Updated both the overview article and the analyze article regarding support of Windows Kubernetes clusters.\n-- [Configure Azure Red Hat OpenShift clusters with Azure Monitor for containers](https://docs.microsoft.com/azure/azure-monitor/insights/container-insights-azure-redhat-setup) - Added details for upgrading agent on Azure Red Hat OpenShift, and added additional information to distinguish the methods for upgrading agent.\n-- [Configure Hybrid Kubernetes clusters with Azure Monitor for containers](https://docs.microsoft.com/azure/azure-monitor/insights/container-insights-hybrid-setup) - Updated to reflect added support for secure port:10250 with the Kubelet's cAdvisor.\n-- [How to manage the Azure Monitor for containers agent](https://docs.microsoft.com/azure/azure-monitor/insights/container-insights-manage-agent) - Updated details related to behavior and config of metric scraping with Azure Red Hat OpenShift compared to other types of Kubernetes clusters.\n-- [Configure Azure Monitor for containers Prometheus Integration](https://docs.microsoft.com/azure/azure-monitor/insights/container-insights-prometheus-integration) - Updated details related to behavior and config of metric scraping with Azure Red Hat OpenShift compared to other types of Kubernetes clusters.\n-- [How to update Azure Monitor for containers for metrics](https://docs.microsoft.com/azure/azure-monitor/insights/container-insights-update-metrics) - Updated details related to behavior and config of metric scraping with Azure Red Hat OpenShift compared to other types of Kubernetes clusters.\n+- [Configure Azure Monitor for containers agent data collection](insights/container-insights-agent-config.md) - Added details for upgrading agent on Azure Red Hat OpenShift, and added additional information to distinguish the methods for upgrading agent.\n+- [Create performance alerts for Azure Monitor for containers](insights/container-insights-alerts.md) - Revised information and updated steps for creating an alert on performance data stored in workspace using workspace-context alerts.\n+- [Kubernetes monitoring with Azure Monitor for containers](insights/container-insights-analyze.md) - Updated both the overview article and the analyze article regarding support of Windows Kubernetes clusters.\n+- [Configure Azure Red Hat OpenShift clusters with Azure Monitor for containers](insights/container-insights-azure-redhat-setup.md) - Added details for upgrading agent on Azure Red Hat OpenShift, and added additional information to distinguish the methods for upgrading agent.\n+- [Configure Hybrid Kubernetes clusters with Azure Monitor for containers](insights/container-insights-hybrid-setup.md) - Updated to reflect added support for secure port:10250 with the Kubelet's cAdvisor.\n+- [How to manage the Azure Monitor for containers agent](insights/container-insights-manage-agent.md) - Updated details related to behavior and config of metric scraping with Azure Red Hat OpenShift compared to other types of Kubernetes clusters.\n+- [Configure Azure Monitor for containers Prometheus Integration](insights/container-insights-prometheus-integration.md) - Updated details related to behavior and config of metric scraping with Azure Red Hat OpenShift compared to other types of Kubernetes clusters.\n+- [How to update Azure Monitor for containers for metrics](insights/container-insights-update-metrics.md) - Updated details related to behavior and config of metric scraping with Azure Red Hat OpenShift compared to other types of Kubernetes clusters.\n \n \n #### Azure Monitor for VMs"
  },
  {
    "Number": 106725,
    "Title": "Fix up OpenCensus Python Links",
    "ClosedAt": "2020-03-06T01:41:30Z",
    "User": "lzchen",
    "FileName": "articles/azure-monitor/app/opencensus-python.md",
    "Addition": 5,
    "Delections": 4,
    "Changes": 9,
    "Patch": "@@ -422,13 +422,14 @@ For more detailed information about how to use queries and logs, see [Logs in Az\n \n * [OpenCensus Python on GitHub](https://github.com/census-instrumentation/opencensus-python)\n * [Customization](https://github.com/census-instrumentation/opencensus-python/blob/master/README.rst#customization)\n-* [Flask integration](https://github.com/census-instrumentation/opencensus-python/tree/master/contrib/opencensus-ext-flask)\n-* [Django integration](https://github.com/census-instrumentation/opencensus-python/tree/master/contrib/opencensus-ext-django)\n-* [MySQL integration](https://github.com/census-instrumentation/opencensus-python/tree/master/contrib/opencensus-ext-mysql)\n-* [PostgreSQL](https://github.com/census-instrumentation/opencensus-python/tree/master/contrib/opencensus-ext-postgresql)\n+* [Azure Monitor Exporters on GitHub](https://github.com/census-instrumentation/opencensus-python/tree/master/contrib/opencensus-ext-azure)\n+* [OpenCensus Integrations](https://github.com/census-instrumentation/opencensus-python#extensions)\n+* [Azure Monitor Sample Applications](https://github.com/Azure-Samples/azure-monitor-opencensus-python)\n \n ## Next steps\n \n+* [Tracking Incoming requests](./../../azure-monitor/app/opencensus-python-dependency.md)\n+* [Tracking Out-going requests](./../../azure-monitor/app/opencensus-python-request.md)\n * [Application map](./../../azure-monitor/app/app-map.md)\n * [End-to-end performance monitoring](./../../azure-monitor/learn/tutorial-performance.md)\n "
  },
  {
    "Number": 106612,
    "Title": "Typo fixes",
    "ClosedAt": "2020-03-05T16:24:53Z",
    "User": "yossi-y",
    "FileName": "articles/azure-monitor/platform/customer-managed-keys.md",
    "Addition": 2,
    "Delections": 3,
    "Changes": 5,
    "Patch": "@@ -329,18 +329,17 @@ Content-type: application/json\n \n {\n   \"properties\": {\n-    \"WriteAccessResourceId\": \"subscriptions/<subscription-id>/resourcegroups/<resource-group-name>/providers/microsoft.operationalinsights/clusters/<cluster-name>\"\n+    \"WriteAccessResourceId\": \"/subscriptions/<subscription-id>/resourcegroups/<resource-group-name>/providers/microsoft.operationalinsights/clusters/<cluster-name>\"\n     }\n }\n ```\n-The *clusterDefinitionId* is the *clusterId* value provided in the response from the previous step.\n \n **Response**\n \n ```json\n {\n   \"properties\": {\n-    \"WriteAccessResourceId\": \"subscriptions/<subscription-id>/resourcegroups/<resource-group-name>/providers/microsoft.operationalinsights/clusters/<cluster-name>\"\n+    \"WriteAccessResourceId\": \"/subscriptions/<subscription-id>/resourcegroups/<resource-group-name>/providers/microsoft.operationalinsights/clusters/<cluster-name>\"\n     },\n   \"id\": \"/subscriptions/subscription-id/resourcegroups/resource-group-name/providers/microsoft.operationalinsights/workspaces/workspace-name/linkedservices/cluster\",\n   \"name\": \"workspace-name/cluster\","
  },
  {
    "Number": 106610,
    "Title": "Add App Service Environments",
    "ClosedAt": "2020-03-05T15:34:49Z",
    "User": "harelbr",
    "FileName": "articles/azure-monitor/platform/alerts-metric-near-real-time.md",
    "Addition": 2,
    "Delections": 0,
    "Changes": 2,
    "Patch": "@@ -78,6 +78,8 @@ Here's the full list of Azure monitor metric sources supported by the newer aler\n |Microsoft.Storage/storageAccounts/services | Yes| No | [Blob Services](../../azure-monitor/platform/metrics-supported.md#microsoftstoragestorageaccountsblobservices), [File Services](../../azure-monitor/platform/metrics-supported.md#microsoftstoragestorageaccountsfileservices), [Queue Services](../../azure-monitor/platform/metrics-supported.md#microsoftstoragestorageaccountsqueueservices) and [Table Services](../../azure-monitor/platform/metrics-supported.md#microsoftstoragestorageaccountstableservices)|\n |Microsoft.StreamAnalytics/streamingjobs |N/A| No | [Stream Analytics](../../azure-monitor/platform/metrics-supported.md#microsoftstreamanalyticsstreamingjobs)|\n |Microsoft.Microsoft.VMWareCloudSimple/virtualMachines |Yes|No |[CloudSimple Virtual Machines](../../azure-monitor/platform/metrics-supported.md#microsoftvmwarecloudsimplevirtualmachines)|\n+|Microsoft.Web/hostingEnvironments/multiRolePools | Yes | No | [App Service Environment Multi-Role Pools](../../azure-monitor/platform/metrics-supported.md#microsoftwebhostingenvironmentsmultirolepools)|\n+|Microsoft.Web/hostingEnvironments/workerPools | Yes | No | [App Service Environment Worker Pools](../../azure-monitor/platform/metrics-supported.md#microsoftwebhostingenvironmentsworkerpools)|\n |Microsoft.Web/serverfarms | Yes | No | [App Service Plans](../../azure-monitor/platform/metrics-supported.md#microsoftwebserverfarms)|\n |Microsoft.Web/sites | Yes | No | [App Services](../../azure-monitor/platform/metrics-supported.md#microsoftwebsites-excluding-functions) and [Functions](../../azure-monitor/platform/metrics-supported.md#microsoftwebsites-functions)|\n |Microsoft.Web/sites/slots | Yes | No | [App Service slots](../../azure-monitor/platform/metrics-supported.md#microsoftwebsitesslots)|"
  },
  {
    "Number": 106606,
    "Title": "Update customer-managed-keys.md",
    "ClosedAt": "2020-03-05T14:55:33Z",
    "User": "yossi-y",
    "FileName": "articles/azure-monitor/platform/customer-managed-keys.md",
    "Addition": 6,
    "Delections": 4,
    "Changes": 10,
    "Patch": "@@ -318,8 +318,9 @@ Content-type: application/json\n For Application Insights CMK configuration, follow the Appendix content for this step.\n \n You need to have ‘write’ permissions on both your workspace and *Cluster* resource to perform this operation, which include these actions:\n-In workspace: Microsoft.OperationalInsights/workspaces/write\n-In *Cluster* resource: Microsoft.OperationalInsights/clusters/write\n+\n+- In workspace: Microsoft.OperationalInsights/workspaces/write\n+- In *Cluster* resource: Microsoft.OperationalInsights/clusters/write\n \n ```rst\n PUT https://management.azure.com/subscriptions/<subscription-id>/resourcegroups/<resource-group-name>/providers/microsoft.operationalinsights/workspaces/<workspace-name>/linkedservices/cluster?api-version=2019-08-01-preview \n@@ -606,8 +607,9 @@ Identity is assigned to the *Cluster* resource at creation time.\n ### Associate a component to a *Cluster* resource using [Components - Create Or Update](https://docs.microsoft.com/rest/api/application-insights/components/createorupdate) API\n \n You need to have ‘write’ permissions on both your component and *Cluster* resource to perform this operation, which include these actions:\n-In component: Microsoft.Insights/component/write\n-In *Cluster* resource: Microsoft.OperationalInsights/clusters/write\n+\n+- In component: Microsoft.Insights/component/write\n+- In *Cluster* resource: Microsoft.OperationalInsights/clusters/write\n \n ```rst\n PUT https://management.azure.com/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.Insights/components/<component-name>?api-version=2015-05-01"
  },
  {
    "Number": 106605,
    "Title": "Add Azure NetApp Files",
    "ClosedAt": "2020-03-05T14:53:14Z",
    "User": "harelbr",
    "FileName": "articles/azure-monitor/platform/alerts-metric-near-real-time.md",
    "Addition": 3,
    "Delections": 1,
    "Changes": 4,
    "Patch": "@@ -5,7 +5,7 @@ author: harelbr\n ms.author: harelbr\n services: monitoring\n ms.topic: conceptual\n-ms.date: 12/17/2019\n+ms.date: 3/5/2020\n ms.subservice: alerts\n ---\n \n@@ -55,6 +55,8 @@ Here's the full list of Azure monitor metric sources supported by the newer aler\n |Microsoft.KeyVault/vaults| No |No |[Vaults](../../azure-monitor/platform/metrics-supported.md#microsoftkeyvaultvaults)|\n |Microsoft.Logic/workflows |N/A | No |[Logic Apps](../../azure-monitor/platform/metrics-supported.md#microsoftlogicworkflows) |\n |Microsoft.MachineLearningServices/workspaces|Yes| No | [Machine Learning](../../azure-monitor/platform/metrics-supported.md#microsoftmachinelearningservicesworkspaces) |\n+|Microsoft.NetApp/netAppAccounts/capacityPools |Yes| No | [Azure NetApp Capacity Pools](../../azure-monitor/platform/metrics-supported.md#microsoftnetappnetappaccountscapacitypools) |\n+|Microsoft.NetApp/netAppAccounts/capacityPools/volumes |Yes| No | [Azure NetApp Volumes](../../azure-monitor/platform/metrics-supported.md#microsoftnetappnetappaccountscapacitypoolsvolumes) |\n |Microsoft.Network/applicationGateways|N/A| No |  |\n |Microsoft.Network/dnsZones | N/A| No | [DNS Zones](../../azure-monitor/platform/metrics-supported.md#microsoftnetworkdnszones) |\n |Microsoft.Network/expressRouteCircuits | N/A | No |[Express Route Circuits](../../azure-monitor/platform/metrics-supported.md#microsoftnetworkexpressroutecircuits) |"
  },
  {
    "Number": 106598,
    "Title": "Added permissions information",
    "ClosedAt": "2020-03-05T11:53:33Z",
    "User": "yossi-y",
    "FileName": "articles/azure-monitor/platform/customer-managed-keys.md",
    "Addition": 8,
    "Delections": 1,
    "Changes": 9,
    "Patch": "@@ -317,6 +317,10 @@ Content-type: application/json\n \n For Application Insights CMK configuration, follow the Appendix content for this step.\n \n+You need to have ‘write’ permissions on both your workspace and *Cluster* resource to perform this operation, which include these actions:\n+In workspace: Microsoft.OperationalInsights/workspaces/write\n+In *Cluster* resource: Microsoft.OperationalInsights/clusters/write\n+\n ```rst\n PUT https://management.azure.com/subscriptions/<subscription-id>/resourcegroups/<resource-group-name>/providers/microsoft.operationalinsights/workspaces/<workspace-name>/linkedservices/cluster?api-version=2019-08-01-preview \n Authorization: Bearer <token>\n@@ -544,7 +548,6 @@ apply to your Application Insights data.\n The configuration of Application Insights CMK is identical to the process illustrated in this article, including constraints and troubleshooting except these steps:\n \n - Create a *Cluster* resource\n-\n - Associate a component to a *Cluster* resource\n \n When configuring CMK for Application Insights, use these steps instead\n@@ -602,6 +605,10 @@ Identity is assigned to the *Cluster* resource at creation time.\n \n ### Associate a component to a *Cluster* resource using [Components - Create Or Update](https://docs.microsoft.com/rest/api/application-insights/components/createorupdate) API\n \n+You need to have ‘write’ permissions on both your component and *Cluster* resource to perform this operation, which include these actions:\n+In component: Microsoft.Insights/component/write\n+In *Cluster* resource: Microsoft.OperationalInsights/clusters/write\n+\n ```rst\n PUT https://management.azure.com/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.Insights/components/<component-name>?api-version=2015-05-01\n Authorization: Bearer <token>"
  },
  {
    "Number": 106586,
    "Title": "Update metric alert rules quota limit",
    "ClosedAt": "2020-03-05T09:28:12Z",
    "User": "harelbr",
    "FileName": "includes/azure-monitor-limits-alerts.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -13,7 +13,7 @@ ms.custom: \"include file\"\n | Resource | Default limit | Maximum limit |\n | --- | --- | --- |\n | Metric alerts (classic) |100 active alert rules per subscription. | Call support. |\n-| Metric alerts |1000 active alert rules per subscription in Azure public, Azure China 21Vianet and Azure Government clouds. | Call support. |\n+| Metric alerts |2,000 active alert rules per subscription in Azure public, Azure China 21Vianet and Azure Government clouds. | Call support. |\n | Activity log alerts | 100 active alert rules per subscription. | Same as default. |\n | Log alerts | 512 | Call support. |\n | Action groups |2,000 action groups per subscription. | Call support. |"
  },
  {
    "Number": 106578,
    "Title": "Azure Monitor added note to Using custom metrics",
    "ClosedAt": "2020-03-05T06:02:23Z",
    "User": "lgayhardt",
    "FileName": "articles/azure-monitor/platform/metrics-custom-overview.md",
    "Addition": 11,
    "Delections": 7,
    "Changes": 18,
    "Patch": "@@ -147,17 +147,21 @@ In the following example, you create a custom metric called **Memory Bytes in Us\n There's no need to predefine a custom metric in Azure Monitor before it's emitted. Each metric data point published contains namespace, name, and dimension information. So the first time a custom metric is emitted to Azure Monitor, a metric definition is automatically created. This metric definition is then discoverable on any resource the metric is emitted against via the metric definitions.\n \n > [!NOTE]  \n-> Azure Monitor doesn’t yet support defining **Units** for a custom metric.\n+> Azure Monitor doesn't yet support defining **Units** for a custom metric.\n \n ## Using custom metrics\n After custom metrics are submitted to Azure Monitor, you can browse them via the Azure portal and query them via the Azure Monitor REST APIs. You can also create alerts on them to notify you when certain conditions are met.\n+\n+> [!NOTE]\n+> You need to be a reader or contributor role to view custom metrics.\n+\n ### Browse your custom metrics via the Azure portal\n-1.\tGo to the [Azure portal](https://portal.azure.com).\n-2.\tSelect the **Monitor** pane.\n-3.\tSelect **Metrics**.\n-4.\tSelect a resource you've emitted custom metrics against.\n-5.\tSelect the metrics namespace for your custom metric.\n-6.\tSelect the custom metric.\n+1.    Go to the [Azure portal](https://portal.azure.com).\n+2.    Select the **Monitor** pane.\n+3.    Select **Metrics**.\n+4.    Select a resource you've emitted custom metrics against.\n+5.    Select the metrics namespace for your custom metric.\n+6.    Select the custom metric.\n \n ## Supported regions\n During the public preview, the ability to publish custom metrics is available only in a subset of Azure regions. This restriction means that metrics can be published only for resources in one of the supported regions. The following table lists the set of supported Azure regions for custom metrics. It also lists the corresponding endpoints that metrics for resources in those regions should be published to:"
  },
  {
    "Number": 106477,
    "Title": "Update log-analytics-agent.md",
    "ClosedAt": "2020-03-04T17:25:14Z",
    "User": "johnkemnetz",
    "FileName": "articles/azure-monitor/platform/log-analytics-agent.md",
    "Addition": 1,
    "Delections": 1,
    "Changes": 2,
    "Patch": "@@ -97,7 +97,7 @@ This section provides details about the supported Linux distributions.\n Starting with versions released after August 2018, we are making the following changes to our support model:  \n \n * Only the server versions are supported, not client.  \n-* New versions of [Azure Linux Endorsed distros](../../virtual-machines/linux/endorsed-distros.md) are always supported.  \n+* Focus support on any of the [Azure Linux Endorsed distros](../../virtual-machines/linux/endorsed-distros.md). Note that there may be some delay between a new distro/version being Azure Linux Endorsed and it being supported for the Log Analytics Linux agent.\n * All minor releases are supported for each major version listed.\n * Versions that have passed their manufacturer's end-of-support date are not supported.  \n * New versions of AMI are not supported.  "
  },
  {
    "Number": 106412,
    "Title": "Update asp-net-core.md",
    "ClosedAt": "2020-03-03T23:39:17Z",
    "User": "TimothyMothra",
    "FileName": "articles/azure-monitor/app/asp-net-core.md",
    "Addition": 8,
    "Delections": 0,
    "Changes": 8,
    "Patch": "@@ -155,6 +155,14 @@ The preceding steps are enough to help you start collecting server-side telemetr\n         @Html.Raw(JavaScriptSnippet.FullScript)\n         </head>\n     ```\n+    \n+Alternatively to using the `FullScript` the `ScriptBody` is available starting in SDK v2.14. Use this if you need to control the `<script>` tag to set a Content Security Policy:\n+\n+    ```cshtml\n+        <script> // apply custom changes to this script tag.\n+            @Html.Raw(JavaScriptSnippet.ScriptBody)\n+        </script>\n+    ```\n \n The `.cshtml` file names referenced earlier are from a default MVC application template. Ultimately, if you want to properly enable client-side monitoring for your application, the JavaScript snippet must appear in the `<head>` section of each page of your application that you want to monitor. You can accomplish this goal for this application template by adding the JavaScript snippet to `_Layout.cshtml`. \n "
  },
  {
    "Number": 106369,
    "Title": "point-in-time value and no preview",
    "ClosedAt": "2020-03-03T20:49:57Z",
    "User": "spelluru",
    "FileName": "articles/service-bus-relay/relay-metrics-azure-monitor.md",
    "Addition": 16,
    "Delections": 16,
    "Changes": 32,
    "Patch": "@@ -1,5 +1,5 @@\n ---\n-title: Azure Relay metrics in Azure Monitor (preview) | Microsoft Docs\n+title: Azure Relay metrics in Azure Monitor  | Microsoft Docs\n description: This article provides information on how you can use Azure Monitor to monitor to state of Azure Relay. \n services: service-bus-relay\n documentationcenter: .NET\n@@ -17,7 +17,7 @@ ms.date: 01/21/2020\n ms.author: spelluru\n \n ---\n-# Azure Relay metrics in Azure Monitor (preview)\n+# Azure Relay metrics in Azure Monitor \n Azure Relay metrics give you the state of resources in your Azure subscription. With a rich set of metrics data, you can assess the overall health of your Relay resources, not only at the namespace level, but also at the entity level. These statistics can be important as they help you to monitor the state of Azure Relay. Metrics can also help troubleshoot root-cause issues without needing to contact Azure support.\n \n Azure Monitor provides unified user interfaces for monitoring across various Azure services. For more information, see [Monitoring in Microsoft Azure](../monitoring-and-diagnostics/monitoring-overview.md) and the [Retrieve Azure Monitor metrics with .NET](https://github.com/Azure-Samples/monitor-dotnet-metrics-api) sample on GitHub.\n@@ -37,7 +37,7 @@ You can monitor metrics over time in the [Azure portal](https://portal.azure.com\n \n ![][1]\n \n-You can also access metrics directly via the namespace. To do so, select your namespace and then click **Metrics (Preview)**. \n+You can also access metrics directly via the namespace. To do so, select your namespace and then click **Metrics **. \n \n For metrics supporting dimensions, you must filter with the desired dimension value.\n \n@@ -56,24 +56,24 @@ All metrics values are sent to Azure Monitor every minute. The time granularity\n \n | Metric Name | Description |\n | ------------------- | ----------------- |\n-| ListenerConnections-Success (preview) | The number of successful listener connections made to Azure Relay over a specified period. <br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n-|ListenerConnections-ClientError (preview)|The number of client errors on listener connections over a specified period.<br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n-|ListenerConnections-ServerError (preview)|The number of server errors on the listener connections over a specified period.<br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n-|SenderConnections-Success (preview)|The number of successful sender connections made over a specified period.<br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n-|SenderConnections-ClientError (preview)|The number of client errors on sender connections over a specified period.<br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n-|SenderConnections-ServerError (preview)|The number of server errors on sender connections over a specified period.<br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n-|ListenerConnections-TotalRequests (preview)|The total number of listener connections over a specified period.<br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n-|SenderConnections-TotalRequests (preview)|The connection requests made by the senders over a specified period.<br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n-|ActiveConnections (preview)|The number of active connections over a specified period.<br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n-|ActiveListeners (preview)|The number of active listeners over a specified period.<br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n-|ListenerDisconnects (preview)|The number of disconnected listeners over a specified period.<br/><br/> Unit: Bytes <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n-|SenderDisconnects (preview)|The number of disconnected senders over a specified period.<br/><br/> Unit: Bytes <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n+| ListenerConnections-Success  | The number of successful listener connections made to Azure Relay over a specified period. <br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n+|ListenerConnections-ClientError |The number of client errors on listener connections over a specified period.<br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n+|ListenerConnections-ServerError |The number of server errors on the listener connections over a specified period.<br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n+|SenderConnections-Success |The number of successful sender connections made over a specified period.<br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n+|SenderConnections-ClientError |The number of client errors on sender connections over a specified period.<br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n+|SenderConnections-ServerError |The number of server errors on sender connections over a specified period.<br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n+|ListenerConnections-TotalRequests |The total number of listener connections over a specified period.<br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n+|SenderConnections-TotalRequests |The connection requests made by the senders over a specified period.<br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n+|ActiveConnections |The number of active connections. This value is a point-in-time value.<br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n+|ActiveListeners |The number of active listeners. This value is a point-in-time value.<br/><br/> Unit: Count <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n+|ListenerDisconnects |The number of disconnected listeners over a specified period.<br/><br/> Unit: Bytes <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n+|SenderDisconnects |The number of disconnected senders over a specified period.<br/><br/> Unit: Bytes <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n \n ## Memory usage metrics\n \n | Metric Name | Description |\n | ------------------- | ----------------- |\n-|BytesTransferred (preview)|The number of bytes transferred over a specified period.<br/><br/> Unit: Bytes <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n+|BytesTransferred |The number of bytes transferred over a specified period.<br/><br/> Unit: Bytes <br/> Aggregation Type: Total <br/> Dimension: EntityName|\n \n ## Metrics dimensions\n "
  },
  {
    "Number": 106112,
    "Title": "App Insights Powershell update from Dale",
    "ClosedAt": "2020-03-03T03:24:09Z",
    "User": "lgayhardt",
    "FileName": "articles/azure-monitor/app/powershell.md",
    "Addition": 37,
    "Delections": 6,
    "Changes": 43,
    "Patch": "@@ -124,7 +124,7 @@ Create a new .json file - let's call it `template1.json` in this example. Copy t\n             },\n             \"dailyQuotaResetTime\": {\n                 \"type\": \"int\",\n-                \"defaultValue\": 24,\n+                \"defaultValue\": 0,\n                 \"metadata\": {\n                     \"description\": \"Enter daily quota reset hour in UTC (0 to 23). Values outside the range will get a random reset hour.\"\n                 }\n@@ -316,16 +316,30 @@ To get the daily cap properties, use the [Set-AzApplicationInsightsPricingPlan](\n Set-AzApplicationInsightsDailyCap -ResourceGroupName <resource group> -Name <resource name> | Format-List\n ```\n \n-To set the daily cap properties, use same cmdlet. For instance, to set the cap to 300 GB/day, \n+To set the daily cap properties, use same cmdlet. For instance, to set the cap to 300 GB/day,\n \n ```PS\n Set-AzApplicationInsightsDailyCap -ResourceGroupName <resource group> -Name <resource name> -DailyCapGB 300\n ```\n \n+You can also use [ARMClient](https://github.com/projectkudu/ARMClient) to get and set daily cap parameters.  To get the current values, use:\n+\n+```PS\n+armclient GET /subscriptions/00000000-0000-0000-0000-00000000000/resourceGroups/MyResourceGroupName/providers/microsoft.insights/components/MyResourceName/CurrentBillingFeatures?api-version=2018-05-01-preview\n+```\n+\n+## Set the daily cap reset time\n+\n+To set the daily cap reset time, you can use [ARMClient](https://github.com/projectkudu/ARMClient). Here's an example using `ARMClient`, to set the reset time to a new hour (in this example 12:00 UTC):\n+\n+```PS\n+armclient PUT /subscriptions/00000000-0000-0000-0000-00000000000/resourceGroups/MyResourceGroupName/providers/microsoft.insights/components/MyResourceName/CurrentBillingFeatures?api-version=2018-05-01-preview \"{'CurrentBillingFeatures':['Basic'],'DataVolumeCap':{'ResetTime':12}}\"\n+```\n+\n <a id=\"price\"></a>\n ## Set the pricing plan \n \n-To get current pricing plan, use the [Set-AzApplicationInsightsPricingPlan](https://docs.microsoft.com/powershell/module/az.applicationinsights/Set-AzApplicationInsightsPricingPlan) cmdlet: \n+To get current pricing plan, use the [Set-AzApplicationInsightsPricingPlan](https://docs.microsoft.com/powershell/module/az.applicationinsights/Set-AzApplicationInsightsPricingPlan) cmdlet:\n \n ```PS\n Set-AzApplicationInsightsPricingPlan -ResourceGroupName <resource group> -Name <resource name> | Format-List\n@@ -346,19 +360,36 @@ You can also set the pricing plan on an existing Application Insights resource u\n                -appName myApp\n ```\n \n+The `priceCode` is defined as:\n+\n |priceCode|plan|\n |---|---|\n |1|Per GB (formerly named the Basic plan)|\n |2|Per Node (formerly name the Enterprise plan)|\n \n+Finally, you can use [ARMClient](https://github.com/projectkudu/ARMClient) to get and set pricing plans and daily cap parameters.  To get the current values, use:\n+\n+```PS\n+armclient GET /subscriptions/00000000-0000-0000-0000-00000000000/resourceGroups/MyResourceGroupName/providers/microsoft.insights/components/MyResourceName/CurrentBillingFeatures?api-version=2018-05-01-preview\n+```\n+\n+And you can set all of these parameters using:\n+\n+```PS\n+armclient PUT /subscriptions/00000000-0000-0000-0000-00000000000/resourceGroups/MyResourceGroupName/providers/microsoft.insights/components/MyResourceName/CurrentBillingFeatures?api-version=2018-05-01-preview\n+\"{'CurrentBillingFeatures':['Basic'],'DataVolumeCap':{'Cap':200,'ResetTime':12,'StopSendNotificationWhenHitCap':true,'WarningThreshold':90,'StopSendNotificationWhenHitThreshold':true}}\"\n+```\n+\n+This will set the daily cap to 200 GB/day, configure the daily cap reset time to 12:00 UTC, send emails both when the cap is hit and the warning level is met, and set the warning threshold to 90% of the cap.  \n+\n ## Add a metric alert\n \n-To automate the creation of metric alerts consult the [metric alerts template article](https://docs.microsoft.com/azure/azure-monitor/platform/alerts-metric-create-templates#template-for-a-simple-static-threshold-metric-alert)\n+To automate the creation of metric alerts, consult the [metric alerts template article](https://docs.microsoft.com/azure/azure-monitor/platform/alerts-metric-create-templates#template-for-a-simple-static-threshold-metric-alert)\n \n \n ## Add an availability test\n \n-To automate availability tests consult the [metric alerts template article](https://docs.microsoft.com/azure/azure-monitor/platform/alerts-metric-create-templates#template-for-an-availability-test-along-with-a-metric-alert).\n+To automate availability tests, consult the [metric alerts template article](https://docs.microsoft.com/azure/azure-monitor/platform/alerts-metric-create-templates#template-for-an-availability-test-along-with-a-metric-alert).\n \n ## Add more resources\n \n@@ -423,4 +454,4 @@ Other automation articles:\n * [Create web tests](https://azure.microsoft.com/blog/creating-a-web-test-alert-programmatically-with-application-insights/)\n * [Send Azure Diagnostics to Application Insights](powershell-azure-diagnostics.md)\n * [Deploy to Azure from GitHub](https://blogs.msdn.com/b/webdev/archive/2015/09/16/deploy-to-azure-from-github-with-application-insights.aspx)\n-* [Create release annotations](https://github.com/Microsoft/ApplicationInsights-Home/blob/master/API/CreateReleaseAnnotation.ps1)\n+* [Create release annotations](https://github.com/Microsoft/ApplicationInsights-Home/blob/master/API/CreateReleaseAnnotation.ps1)\n\\ No newline at end of file"
  },
  {
    "Number": 106205,
    "Title": "Add standard metrics in OpenCensus Python docs",
    "ClosedAt": "2020-03-02T19:53:02Z",
    "User": "lzchen",
    "FileName": "articles/azure-monitor/app/opencensus-python.md",
    "Addition": 48,
    "Delections": 4,
    "Changes": 52,
    "Patch": "@@ -129,11 +129,20 @@ Here are the exporters that OpenCensus provides mapped to the types of telemetry\n         main()\n     ```\n \n-4. Now when you run the Python script, you should still be prompted to enter values, but only the value is being printed in the shell. The created `SpanData` will be sent to Azure Monitor. You can find the emitted span data under `dependencies`.\n+4. Now when you run the Python script, you should still be prompted to enter values, but only the value is being printed in the shell. The created `SpanData` will be sent to Azure Monitor. You can find the emitted span data under `dependencies`. For more details on outgoing requests, see OpenCensus Python [dependencies](https://docs.microsoft.com/azure/azure-monitor/app/opencensus-python-dependency).\n+For more details on incoming requests, see OpenCensus Python [requests](https://docs.microsoft.com/azure/azure-monitor/app/opencensus-python-request).\n \n-5. For information on sampling in OpenCensus, take a look at [sampling in OpenCensus](sampling.md#configuring-fixed-rate-sampling-for-opencensus-python-applications).\n+#### Sampling\n \n-6. For details on telemetry correlation in your trace data, take a look at OpenCensus [telemetry correlation](https://docs.microsoft.com/azure/azure-monitor/app/correlation#telemetry-correlation-in-opencensus-python).\n+For information on sampling in OpenCensus, take a look at [sampling in OpenCensus](sampling.md#configuring-fixed-rate-sampling-for-opencensus-python-applications).\n+\n+#### Trace correlation\n+\n+For details on telemetry correlation in your trace data, take a look at OpenCensus Python [telemetry correlation](https://docs.microsoft.com/azure/azure-monitor/app/correlation#telemetry-correlation-in-opencensus-python).\n+\n+#### Modify telemetry\n+\n+For details on how to modify tracked telemetry before it is sent to Azure Monitor, see OpenCensus Python [telemetry processors](https://docs.microsoft.com/azure/azure-monitor/app/api-filtering-sampling#opencensus-python-telemetry-processors).\n \n ### Metrics\n \n@@ -238,6 +247,32 @@ Here are the exporters that OpenCensus provides mapped to the types of telemetry\n \n 4. The exporter will send metric data to Azure Monitor at a fixed interval. The default is every 15 seconds. We're tracking a single metric, so this metric data, with whatever value and time stamp it contains, will be sent every interval. You can find the data under `customMetrics`.\n \n+#### Standard metrics\n+\n+By default, the metrics exporter will send a set of standard metrics to Azure Monitor. You can disable this by setting the `enable_standard_metrics` flag to `False` in the constructor of the metrics exporter.\n+\n+    ```python\n+    ...\n+    exporter = metrics_exporter.new_metrics_exporter(\n+      enable_standard_metrics=False,\n+      connection_string='InstrumentationKey=<your-instrumentation-key-here>')\n+    ...\n+    ```\n+Below is a list of standard metrics that are currently sent:\n+\n+- Available Memory (bytes)\n+- CPU Processor Time (percentage)\n+- Incoming Request Rate (per second)\n+- Incoming Request Average Execution Time (milliseconds)\n+- Outgoing Request Rate (per second)\n+- Process CPU Usage (percentage)\n+- Process Private Bytes (bytes)\n+\n+You should be able to see these metrics in `performanceCounters`. Incoming request rate would be under `customMetrics`.\n+#### Modify telemetry\n+\n+For details on how to modify tracked telemetry before it is sent to Azure Monitor, see OpenCensus Python [telemetry processors](https://docs.microsoft.com/azure/azure-monitor/app/api-filtering-sampling#opencensus-python-telemetry-processors).\n+\n ### Logs\n \n 1. First, let's generate some local log data.\n@@ -357,8 +392,17 @@ Here are the exporters that OpenCensus provides mapped to the types of telemetry\n     except Exception:\n     logger.exception('Captured an exception.', extra=properties)\n     ```\n+#### Sampling\n+\n+For information on sampling in OpenCensus, take a look at [sampling in OpenCensus](sampling.md#configuring-fixed-rate-sampling-for-opencensus-python-applications).\n+\n+#### Log correlation\n+\n+For details on how to enrich your logs with trace context data, see OpenCensus Python [logs integration](https://docs.microsoft.com/azure/azure-monitor/app/correlation#log-correlation).\n+\n+#### Modify telemetry\n \n-7. For details on how to enrich your logs with trace context data, see OpenCensus Python [logs integration](https://docs.microsoft.com/azure/azure-monitor/app/correlation#log-correlation).\n+For details on how to modify tracked telemetry before it is sent to Azure Monitor, see OpenCensus Python [telemetry processors](https://docs.microsoft.com/azure/azure-monitor/app/api-filtering-sampling#opencensus-python-telemetry-processors).\n \n ## View your data with queries\n "
  },
  {
    "Number": 106129,
    "Title": "Corrected typo about principle-id",
    "ClosedAt": "2020-03-01T15:50:31Z",
    "User": "yossi-y",
    "FileName": "articles/azure-monitor/platform/customer-managed-keys.md",
    "Addition": 6,
    "Delections": 6,
    "Changes": 12,
    "Patch": "@@ -213,7 +213,7 @@ Authorization: Bearer <token>\n   \"identity\": {\n     \"type\": \"SystemAssigned\",\n     \"tenantId\": \"tenant-id\",\n-    \"principalId\": \"principal-Id\"\n+    \"principalId\": \"principal-id\"\n     },\n   \"properties\": {\n     \"provisioningState\": \"Succeeded\",\n@@ -227,10 +227,10 @@ Authorization: Bearer <token>\n   }\n ```\n \n-\"principalId\" is a GUID generated by the managed identity service for the *Cluster* resource.\n+\"principal-id\" is a GUID generated by the managed identity service for the *Cluster* resource.\n \n > [!IMPORTANT]\n-> Copy and keep the \"cluster-id\" value since you will need it in next steps.\n+> Copy and keep the \"principal-id\" value since you will need it in next steps.\n \n \n ### Grant Key Vault permissions\n@@ -242,7 +242,7 @@ Update your Key Vault with a new access policy that grant permissions to your *C\n Open your Key Vault in Azure portal and click \"Access Policies\" then \"+ Add Access Policy\" to create a new policy with these settings:\n \n - Key permissions: select 'Get', 'Wrap Key' and 'Unwrap Key' permissions.\n-- Select principal: enter the cluster-id value that returned in the response in the previous step.\n+- Select principal: enter the principal-id value that returned in the response in the previous step.\n \n ![grant Key Vault permissions](media/customer-managed-keys/grant-key-vault-permissions.png)\n \n@@ -595,10 +595,10 @@ Identity is assigned to the *Cluster* resource at creation time.\n   \"location\": \"region-name\"\n }\n ```\n-\"principalId\" is a GUID that was generated by the managed identity service.\n+\"principle-id\" is a GUID that was generated by the managed identity service.\n \n > [!IMPORTANT]\n-> Copy and keep the \"cluster-id\" value since you will need it in next steps.\n+> Copy and keep the \"principle-id\" value since you will need it in next steps.\n \n ### Associate a component to a *Cluster* resource using [Components - Create Or Update](https://docs.microsoft.com/rest/api/application-insights/components/createorupdate) API\n "
  }
]
